#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æƒé‡èåˆæ¨¡å‹ - æ”¯æŒå¤§è§„æ¨¡è®­ç»ƒçš„æƒé‡èåˆæ¶æ„
åŸºäºTransformerå’ŒCNNçš„æ··åˆæ¨¡å‹ï¼Œå®ç°RF:NNæƒé‡èåˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class WeightedFusionTransformer(nn.Module):
    """æƒé‡èåˆTransformeræ¨¡å‹ - ä¸“ä¸ºè°±é¢ç”Ÿæˆä¼˜åŒ–"""
    
    def __init__(self, input_dim=15, d_model=256, num_heads=8, num_layers=6, 
                 dropout=0.1, rf_weight=0.2, nn_weight=0.8, 
                 learnable_weights=True):
        super().__init__()
        
        # æƒé‡å‚æ•°p
        if learnable_weights:
            self.rf_weight = nn.Parameter(torch.tensor(rf_weight, dtype=torch.float32))
            self.nn_weight = nn.Parameter(torch.tensor(nn_weight, dtype=torch.float32))
        else:
            self.register_buffer('rf_weight', torch.tensor(rf_weight, dtype=torch.float32))
            self.register_buffer('nn_weight', torch.tensor(nn_weight, dtype=torch.float32))
        
        self.learnable_weights = learnable_weights
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # RFåˆ†æ”¯ - æ¨¡æ‹Ÿä¼ ç»Ÿå†³ç­–æ ‘/éšæœºæ£®æ—é€»è¾‘
        self.rf_branch = RFBranch(d_model, dropout)
        
        # NNåˆ†æ”¯ - æ·±åº¦Transformerç½‘ç»œ
        self.nn_branch = NNBranch(d_model, num_heads, num_layers, dropout)
        
        # è¾“å‡ºå±‚
        self.output_notes = nn.Linear(d_model, 4)     # 4è½¨é“éŸ³ç¬¦
        self.output_events = nn.Linear(d_model, 3)    # 3ç§äº‹ä»¶ç±»å‹
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
    
    def forward(self, x):
        # è¾“å…¥æŠ•å½±
        x_projected = self.input_projection(x)  # [batch, d_model]
        
        # æƒé‡å½’ä¸€åŒ–
        total_weight = self.rf_weight + self.nn_weight
        normalized_rf = self.rf_weight / (total_weight + 1e-8)
        normalized_nn = self.nn_weight / (total_weight + 1e-8)
        
        # RFåˆ†æ”¯å¤„ç†
        rf_features = self.rf_branch(x_projected)
        
        # NNåˆ†æ”¯å¤„ç†
        nn_features = self.nn_branch(x_projected)
        
        # æƒé‡èåˆ
        fused_features = normalized_rf * rf_features + normalized_nn * nn_features
        
        # è¾“å‡ºé¢„æµ‹
        note_output = torch.sigmoid(self.output_notes(fused_features))
        event_output = torch.sigmoid(self.output_events(fused_features))
        
        # è¿”å›åˆ†ç¦»çš„è¾“å‡ºä»¥åŒ¹é…åŸºç¡€ç³»ç»ŸæœŸæœ›çš„æ ¼å¼
        return note_output, event_output
    
    def get_branch_predictions(self, x):
        """è·å–å„åˆ†æ”¯çš„å•ç‹¬é¢„æµ‹ï¼Œç”¨äºåˆ†æ"""
        with torch.no_grad():
            x_projected = self.input_projection(x)
            
            # å„åˆ†æ”¯ç‰¹å¾
            rf_features = self.rf_branch(x_projected)
            nn_features = self.nn_branch(x_projected)
            
            # å„åˆ†æ”¯è¾“å‡º
            rf_notes = torch.sigmoid(self.output_notes(rf_features))
            rf_events = torch.sigmoid(self.output_events(rf_features))
            rf_pred = torch.cat([rf_notes, rf_events], dim=1)
            
            nn_notes = torch.sigmoid(self.output_notes(nn_features))
            nn_events = torch.sigmoid(self.output_events(nn_features))
            nn_pred = torch.cat([nn_notes, nn_events], dim=1)
            
            # èåˆé¢„æµ‹
            fused_note_pred, fused_event_pred = self.forward(x)
            fused_pred = torch.cat([fused_note_pred, fused_event_pred], dim=1)
            
            # å½“å‰æƒé‡
            total_weight = self.rf_weight + self.nn_weight
            
            return {
                'rf_pred': rf_pred,
                'nn_pred': nn_pred,
                'fused_pred': fused_pred,
                'rf_weight': (self.rf_weight / total_weight).item(),
                'nn_weight': (self.nn_weight / total_weight).item(),
                'rf_features': rf_features,
                'nn_features': nn_features
            }
    
    def set_weights(self, rf_weight, nn_weight):
        """è®¾ç½®æƒé‡æ¯”ä¾‹"""
        if self.learnable_weights:
            with torch.no_grad():
                self.rf_weight.data = torch.tensor(rf_weight, dtype=torch.float32)
                self.nn_weight.data = torch.tensor(nn_weight, dtype=torch.float32)
        else:
            self.rf_weight.data = torch.tensor(rf_weight, dtype=torch.float32)
            self.nn_weight.data = torch.tensor(nn_weight, dtype=torch.float32)
    
    def get_weights(self):
        """è·å–å½“å‰æƒé‡"""
        total = self.rf_weight + self.nn_weight
        return {
            'rf_weight': (self.rf_weight / total).item(),
            'nn_weight': (self.nn_weight / total).item(),
            'raw_rf': self.rf_weight.item(),
            'raw_nn': self.nn_weight.item()
        }

class RFBranch(nn.Module):
    """RFåˆ†æ”¯ - æ¨¡æ‹Ÿéšæœºæ£®æ—çš„å†³ç­–é€»è¾‘"""
    
    def __init__(self, d_model, dropout=0.1):
        super().__init__()
        
        # å¤šä¸ªå†³ç­–è·¯å¾„ï¼Œæ¨¡æ‹Ÿéšæœºæ£®æ—çš„å¤šæ£µæ ‘
        self.decision_paths = nn.ModuleList([
            self._create_decision_path(d_model, dropout) for _ in range(8)
        ])
        
        # è·¯å¾„èåˆ
        self.path_fusion = nn.Sequential(
            nn.Linear(d_model * 8, d_model * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 2, d_model),
            nn.ReLU()
        )
        
    def _create_decision_path(self, d_model, dropout):
        """åˆ›å»ºå•ä¸ªå†³ç­–è·¯å¾„ï¼ˆæ¨¡æ‹Ÿä¸€æ£µå†³ç­–æ ‘ï¼‰"""
        return nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),  # RFåˆ†æ”¯ä½¿ç”¨è¾ƒå°‘çš„dropout
            nn.Linear(d_model // 2, d_model // 4),
            nn.ReLU(),
            nn.Linear(d_model // 4, d_model)
        )
    
    def forward(self, x):
        # å¤šè·¯å¾„å¤„ç†
        path_outputs = []
        for path in self.decision_paths:
            path_out = path(x)
            path_outputs.append(path_out)
        
        # è¿æ¥æ‰€æœ‰è·¯å¾„è¾“å‡º
        concatenated = torch.cat(path_outputs, dim=1)
        
        # èåˆå¤šè·¯å¾„ç‰¹å¾
        fused = self.path_fusion(concatenated)
        
        return fused

class NNBranch(nn.Module):
    """NNåˆ†æ”¯ - æ·±åº¦ç¥ç»ç½‘ç»œç‰¹å¾æå–"""
    
    def __init__(self, d_model, num_heads=8, num_layers=6, dropout=0.1):
        super().__init__()
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚
        self.attention_layers = nn.ModuleList([
            nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
            for _ in range(num_layers)
        ])
        
        # å‰é¦ˆç½‘ç»œå±‚
        self.ffn_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model * 4, d_model),
                nn.Dropout(dropout)
            ) for _ in range(num_layers)
        ])
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norms1 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(num_layers)])
        self.layer_norms2 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(num_layers)])
        
        # æœ€ç»ˆç‰¹å¾æå–
        self.final_projection = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 2, d_model)
        )
        
    def forward(self, x):
        # x: [batch, d_model]
        # æ‰©å±•ç»´åº¦ä»¥é€‚åº”å¤šå¤´æ³¨æ„åŠ› [batch, 1, d_model]
        x = x.unsqueeze(1)
        
        # å¤šå±‚Transformerå¤„ç†
        for i, (attn, ffn, ln1, ln2) in enumerate(zip(
            self.attention_layers, self.ffn_layers, 
            self.layer_norms1, self.layer_norms2
        )):
            # è‡ªæ³¨æ„åŠ›
            attn_out, _ = attn(x, x, x)
            x = ln1(x + attn_out)
            
            # å‰é¦ˆç½‘ç»œ
            ffn_out = ffn(x)
            x = ln2(x + ffn_out)
        
        # ç§»é™¤åºåˆ—ç»´åº¦ [batch, d_model]
        x = x.squeeze(1)
        
        # æœ€ç»ˆç‰¹å¾æŠ•å½±
        x = self.final_projection(x)
        
        return x

class WeightedFusionCNN(nn.Module):
    """åŸºäºCNNçš„æƒé‡èåˆæ¨¡å‹ - é€‚ç”¨äºåºåˆ—æ•°æ®"""
    
    def __init__(self, input_dim=15, rf_weight=0.2, nn_weight=0.8, 
                 learnable_weights=True):
        super().__init__()
        
        # æƒé‡å‚æ•°
        if learnable_weights:
            self.rf_weight = nn.Parameter(torch.tensor(rf_weight, dtype=torch.float32))
            self.nn_weight = nn.Parameter(torch.tensor(nn_weight, dtype=torch.float32))
        else:
            self.register_buffer('rf_weight', torch.tensor(rf_weight, dtype=torch.float32))
            self.register_buffer('nn_weight', torch.tensor(nn_weight, dtype=torch.float32))
        
        # RFåˆ†æ”¯ - æ¨¡æ‹Ÿä¼ ç»Ÿç‰¹å¾æå–
        self.rf_branch = nn.Sequential(
            nn.Conv1d(input_dim, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 7),
            nn.Sigmoid()
        )
        
        # NNåˆ†æ”¯ - æ·±åº¦å·ç§¯ç½‘ç»œ
        self.nn_branch = nn.Sequential(
            nn.Conv1d(input_dim, 64, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 7),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œéœ€è¦è½¬æ¢ä¸º3Dè¿›è¡Œå·ç§¯
        if x.dim() == 2:
            x = x.unsqueeze(-1)  # [batch, features, 1]
            x = x.transpose(1, 2)  # [batch, 1, features]
            x = x.expand(-1, x.size(-1), -1)  # [batch, features, features]
        
        # æƒé‡å½’ä¸€åŒ–
        total_weight = self.rf_weight + self.nn_weight
        normalized_rf = self.rf_weight / (total_weight + 1e-8)
        normalized_nn = self.nn_weight / (total_weight + 1e-8)
        
        # ä¸¤ä¸ªåˆ†æ”¯çš„é¢„æµ‹
        rf_pred = self.rf_branch(x)
        nn_pred = self.nn_branch(x)
        
        # æƒé‡èåˆ
        fused_pred = normalized_rf * rf_pred + normalized_nn * nn_pred
        
        return fused_pred
    
    def get_branch_predictions(self, x):
        """è·å–å„åˆ†æ”¯é¢„æµ‹"""
        with torch.no_grad():
            if x.dim() == 2:
                x = x.unsqueeze(-1).transpose(1, 2).expand(-1, x.size(-1), -1)
            
            rf_pred = self.rf_branch(x)
            nn_pred = self.nn_branch(x)
            fused_pred = self.forward(x)
            
            total_weight = self.rf_weight + self.nn_weight
            return {
                'rf_pred': rf_pred,
                'nn_pred': nn_pred,
                'fused_pred': fused_pred,
                'rf_weight': (self.rf_weight / total_weight).item(),
                'nn_weight': (self.nn_weight / total_weight).item()
            }

def create_weighted_fusion_model(model_type='transformer', **kwargs):
    """æ¨¡å‹å·¥å‚å‡½æ•°"""
    if model_type == 'transformer':
        return WeightedFusionTransformer(**kwargs)
    elif model_type == 'cnn':
        return WeightedFusionCNN(**kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'WeightedFusionTransformer',
    'WeightedFusionCNN', 
    'RFBranch',
    'NNBranch',
    'create_weighted_fusion_model'
]

if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    print("ğŸ§ª æƒé‡èåˆæ¨¡å‹æµ‹è¯•")
    
    # æµ‹è¯•Transformeræ¨¡å‹
    model = WeightedFusionTransformer(
        input_dim=15, 
        rf_weight=0.2, 
        nn_weight=0.8
    )
    
    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 32
    x = torch.randn(batch_size, 15)
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # åˆ†æ”¯åˆ†æ
    branch_results = model.get_branch_predictions(x)
    print(f"RFæƒé‡: {branch_results['rf_weight']:.3f}")
    print(f"NNæƒé‡: {branch_results['nn_weight']:.3f}")
    
    print("âœ… æƒé‡èåˆæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
