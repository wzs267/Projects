#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦å­¦ä¹ è°±é¢ç”Ÿæˆæ¨ç†ç³»ç»Ÿ
åŸºäºè®­ç»ƒå¥½çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡è°±é¢
"""

import os
import numpy as np
import torch
import librosa
import json
from typing import List, Dict, Any, Tuple
from deep_learning_beatmap_system import DeepBeatmapLearningSystem, TransformerBeatmapGenerator
import matplotlib.pyplot as plt

class DeepBeatmapGenerator:
    """åŸºäºæ·±åº¦å­¦ä¹ çš„è°±é¢ç”Ÿæˆå™¨"""
    
    def __init__(self, model_path: str = None):
        """
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.scaler = None
        self.sequence_length = 64
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
    
    def load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        self.model = TransformerBeatmapGenerator(
            input_dim=15,  # æ ‡å‡†éŸ³é¢‘ç‰¹å¾ç»´åº¦
            d_model=256,
            num_heads=8,
            num_layers=6,
            dropout=0.1
        ).to(self.device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        # åŠ è½½æ ‡å‡†åŒ–å™¨
        self.scaler = checkpoint['scaler']
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"   ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"   ğŸ† è®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'unknown')}")
        print(f"   ğŸ“ˆ éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 'unknown'):.4f}")
    
    def extract_audio_features(self, audio_file: str) -> np.ndarray:
        """
        æå–éŸ³é¢‘ç‰¹å¾åºåˆ—
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            audio_features: [æ—¶é—´æ­¥, 15ç»´ç‰¹å¾]
        """
        print(f"ğŸµ åˆ†æéŸ³é¢‘: {os.path.basename(audio_file)}")
        
        # åŠ è½½éŸ³é¢‘
        try:
            y, sr = librosa.load(audio_file, sr=22050)
        except Exception as e:
            print(f"âŒ éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
            return None
        
        print(f"   â±ï¸ éŸ³é¢‘æ—¶é•¿: {len(y)/sr:.1f}ç§’")
        print(f"   ğŸ¼ é‡‡æ ·ç‡: {sr} Hz")
        
        # è®¡ç®—æ—¶é—´ç½‘æ ¼ï¼ˆ50msåˆ†è¾¨ç‡ï¼‰
        time_resolution = 0.05
        frame_length = int(sr * time_resolution)
        num_frames = len(y) // frame_length
        
        audio_features = []
        
        for i in range(num_frames):
            start_sample = i * frame_length
            end_sample = start_sample + frame_length
            frame = y[start_sample:end_sample]
            
            if len(frame) < frame_length:
                frame = np.pad(frame, (0, frame_length - len(frame)))
            
            # æå–15ç»´éŸ³é¢‘ç‰¹å¾
            features = self._extract_frame_features(frame, sr)
            audio_features.append(features)
        
        audio_features = np.array(audio_features)
        print(f"   ğŸ“Š æå–ç‰¹å¾: {audio_features.shape[0]} ä¸ªæ—¶é—´æ­¥ Ã— {audio_features.shape[1]} ç»´ç‰¹å¾")
        
        return audio_features
    
    def _extract_frame_features(self, frame: np.ndarray, sr: int) -> np.ndarray:
        """æå–å•å¸§éŸ³é¢‘ç‰¹å¾"""
        features = []
        
        # 1. RMSèƒ½é‡(dB)
        rms = librosa.feature.rms(y=frame)[0, 0]
        rms_db = 20 * np.log10(max(rms, 1e-8))
        features.append(rms_db)
        
        # 2. éŸ³ç¬¦èµ·å§‹å¼ºåº¦
        onset_strength = librosa.onset.onset_strength(y=frame, sr=sr)[0] if len(frame) > 512 else 0
        features.append(onset_strength)
        
        # 3-7. MFCCç‰¹å¾ (å‰5ä¸ª)
        try:
            mfccs = librosa.feature.mfcc(y=frame, sr=sr, n_mfcc=5)
            features.extend(mfccs[:, 0])
        except:
            features.extend([0.0] * 5)
        
        # 8. é¢‘è°±è´¨å¿ƒ
        try:
            spectral_centroid = librosa.feature.spectral_centroid(y=frame, sr=sr)[0, 0]
            features.append(spectral_centroid)
        except:
            features.append(0.0)
        
        # 9. è‰²åº¦ç‰¹å¾å‡å€¼
        try:
            chroma = librosa.feature.chroma_stft(y=frame, sr=sr)
            chroma_mean = np.mean(chroma)
            features.append(chroma_mean)
        except:
            features.append(0.0)
        
        # 10. è¿‡é›¶ç‡
        zcr = librosa.feature.zero_crossing_rate(frame)[0, 0]
        features.append(zcr)
        
        # 11. é¢‘è°±å¯¹æ¯”åº¦
        try:
            contrast = librosa.feature.spectral_contrast(y=frame, sr=sr)[0, 0]
            features.append(contrast)
        except:
            features.append(0.0)
        
        # 12. BPMï¼ˆä½¿ç”¨å›ºå®šå€¼ï¼Œå› ä¸ºå•å¸§æ— æ³•å‡†ç¡®ä¼°è®¡ï¼‰
        features.append(120.0)  # é»˜è®¤BPM
        
        # 13-14. éš¾åº¦å‚æ•°ï¼ˆåœ¨ç”Ÿæˆæ—¶è®¾ç½®ï¼‰
        features.extend([0.5, 0.5])  # é»˜è®¤ä¸­ç­‰éš¾åº¦
        
        return np.array(features, dtype=np.float32)
    
    def generate_beatmap_deep(self, audio_file: str, difficulty: str = 'Normal', 
                            note_threshold: float = 0.5) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ç”Ÿæˆè°±é¢
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            difficulty: éš¾åº¦çº§åˆ«
            note_threshold: éŸ³ç¬¦æ”¾ç½®é˜ˆå€¼
            
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        print(f"ğŸ® ä½¿ç”¨æ·±åº¦å­¦ä¹ ç”Ÿæˆ {difficulty} éš¾åº¦è°±é¢")
        
        # æå–éŸ³é¢‘ç‰¹å¾
        audio_features = self.extract_audio_features(audio_file)
        if audio_features is None:
            return None
        
        # è®¾ç½®éš¾åº¦å‚æ•°
        difficulty_params = self._get_difficulty_params(difficulty)
        
        # æ›´æ–°éŸ³é¢‘ç‰¹å¾ä¸­çš„éš¾åº¦å‚æ•°
        audio_features[:, -2] = difficulty_params['note_density']
        audio_features[:, -1] = difficulty_params['note_threshold']
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if self.scaler:
            audio_features = self.scaler.transform(audio_features)
        
        # ç”Ÿæˆè°±é¢äº‹ä»¶
        generated_events = self._predict_beatmap_sequence(audio_features, note_threshold)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        note_events = [e for e in generated_events if e['type'] == 'note']
        long_events = [e for e in generated_events if e['type'] == 'long_start']
        
        result = {
            'audio_file': audio_file,
            'difficulty': difficulty,
            'audio_duration': len(audio_features) * 0.05,
            'generated_events': generated_events,
            'statistics': {
                'total_events': len(generated_events),
                'note_count': len(note_events),
                'long_note_count': len(long_events),
                'note_density': len(generated_events) / (len(audio_features) * 0.05),
                'difficulty_params': difficulty_params
            }
        }
        
        print(f"âœ… æ·±åº¦å­¦ä¹ ç”Ÿæˆå®Œæˆ!")
        print(f"   ğŸµ ç”ŸæˆéŸ³ç¬¦: {len(note_events)} ä¸ªæ™®é€šéŸ³ç¬¦ + {len(long_events)} ä¸ªé•¿æ¡")
        print(f"   ğŸ“Š éŸ³ç¬¦å¯†åº¦: {result['statistics']['note_density']:.2f} éŸ³ç¬¦/ç§’")
        
        return result
    
    def _get_difficulty_params(self, difficulty: str) -> Dict[str, float]:
        """è·å–éš¾åº¦å‚æ•°"""
        params = {
            'Easy': {'note_density': 0.3, 'note_threshold': 0.7},
            'Normal': {'note_density': 0.5, 'note_threshold': 0.6},
            'Hard': {'note_density': 0.7, 'note_threshold': 0.5},
            'Expert': {'note_density': 0.9, 'note_threshold': 0.4}
        }
        return params.get(difficulty, params['Normal'])
    
    def _predict_beatmap_sequence(self, audio_features: np.ndarray, 
                                note_threshold: float) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹è°±é¢åºåˆ—"""
        generated_events = []
        
        # æ»‘åŠ¨çª—å£é¢„æµ‹
        for i in range(self.sequence_length, len(audio_features)):
            # æå–åºåˆ—
            start_idx = i - self.sequence_length
            audio_seq = audio_features[start_idx:i]
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            audio_tensor = torch.FloatTensor(audio_seq).unsqueeze(0).to(self.device)
            
            # æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                note_probs, event_probs = self.model(audio_tensor)
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                note_probs = note_probs.cpu().numpy()[0]  # [4]
                event_probs = event_probs.cpu().numpy()[0]  # [3]
            
            # å†³å®šæ˜¯å¦æ”¾ç½®éŸ³ç¬¦
            current_time = i * 0.05
            
            # æ£€æŸ¥æ¯ä¸ªè½¨é“
            for column in range(4):
                if note_probs[column] > note_threshold:
                    # ç¡®å®šäº‹ä»¶ç±»å‹
                    event_type_idx = np.argmax(event_probs)
                    event_types = ['note', 'long_start', 'long_end']
                    event_type = event_types[event_type_idx]
                    
                    # åˆ›å»ºäº‹ä»¶
                    event = {
                        'time': current_time,
                        'column': column,
                        'type': event_type,
                        'confidence': float(note_probs[column]),
                        'event_confidence': float(event_probs[event_type_idx])
                    }
                    
                    # ä¸ºé•¿æ¡éŸ³ç¬¦æ·»åŠ æŒç»­æ—¶é—´
                    if event_type == 'long_start':
                        event['duration'] = self._estimate_long_note_duration(
                            audio_features, i, note_probs[column]
                        )
                    
                    generated_events.append(event)
        
        return generated_events
    
    def _estimate_long_note_duration(self, audio_features: np.ndarray, 
                                   start_idx: int, start_confidence: float) -> float:
        """ä¼°è®¡é•¿æ¡éŸ³ç¬¦æŒç»­æ—¶é—´"""
        min_duration = 0.2  # æœ€å°200ms
        max_duration = 2.0  # æœ€å¤§2ç§’
        
        # åŸºäºèµ·å§‹ç½®ä¿¡åº¦ä¼°è®¡åŸºç¡€æŒç»­æ—¶é—´
        base_duration = 0.3 + (start_confidence - 0.5) * 0.4
        
        # æ£€æŸ¥åç»­éŸ³é¢‘ç‰¹å¾çš„æŒç»­æ€§
        duration = base_duration
        for i in range(start_idx + 1, min(start_idx + 40, len(audio_features))):  # æ£€æŸ¥åç»­2ç§’
            frame_energy = audio_features[i, 0]  # RMSèƒ½é‡
            if frame_energy < audio_features[start_idx, 0] - 10:  # èƒ½é‡æ˜¾è‘—ä¸‹é™
                break
            duration += 0.05
        
        return max(min_duration, min(duration, max_duration))
    
    def visualize_generated_beatmap(self, result: Dict[str, Any], save_path: str = None):
        """å¯è§†åŒ–ç”Ÿæˆçš„è°±é¢"""
        events = result['generated_events']
        duration = result['audio_duration']
        
        # åˆ›å»ºæ—¶é—´è½´
        time_points = [e['time'] for e in events]
        columns = [e['column'] for e in events]
        colors = []
        
        for e in events:
            if e['type'] == 'note':
                colors.append('blue')
            elif e['type'] == 'long_start':
                colors.append('red')
            else:
                colors.append('orange')
        
        # ç»˜åˆ¶è°±é¢å›¾
        plt.figure(figsize=(15, 8))
        
        # ä¸»è°±é¢å›¾
        plt.subplot(2, 1, 1)
        plt.scatter(time_points, columns, c=colors, alpha=0.7, s=50)
        plt.ylim(-0.5, 3.5)
        plt.yticks([0, 1, 2, 3], ['è½¨é“1', 'è½¨é“2', 'è½¨é“3', 'è½¨é“4'])
        plt.xlabel('æ—¶é—´ (ç§’)')
        plt.ylabel('è½¨é“')
        plt.title(f'{result["difficulty"]} éš¾åº¦è°±é¢ç”Ÿæˆç»“æœ')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹
        import matplotlib.patches as mpatches
        blue_patch = mpatches.Patch(color='blue', label='æ™®é€šéŸ³ç¬¦')
        red_patch = mpatches.Patch(color='red', label='é•¿æ¡å¼€å§‹')
        orange_patch = mpatches.Patch(color='orange', label='é•¿æ¡ç»“æŸ')
        plt.legend(handles=[blue_patch, red_patch, orange_patch])
        
        # éŸ³ç¬¦å¯†åº¦å›¾
        plt.subplot(2, 1, 2)
        time_bins = np.arange(0, duration, 1.0)  # 1ç§’ä¸ºå•ä½
        density, _ = np.histogram(time_points, bins=time_bins)
        plt.bar(time_bins[:-1], density, width=0.8, alpha=0.7, color='green')
        plt.xlabel('æ—¶é—´ (ç§’)')
        plt.ylabel('éŸ³ç¬¦æ•°é‡')
        plt.title('éŸ³ç¬¦å¯†åº¦åˆ†å¸ƒ (æ¯ç§’)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è°±é¢å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        
        plt.show()


def demo_deep_generation():
    """æ¼”ç¤ºæ·±åº¦å­¦ä¹ è°±é¢ç”Ÿæˆ"""
    print("ğŸ® æ·±åº¦å­¦ä¹ è°±é¢ç”Ÿæˆæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = DeepBeatmapGenerator()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    model_files = [
        'large_scale_beatmap_model.pth',
        'best_deep_beatmap_model.pth',
        'test_model.pth'
    ]
    
    model_path = None
    for model_file in model_files:
        if os.path.exists(model_file):
            model_path = model_file
            break
    
    if model_path:
        # åŠ è½½æ¨¡å‹
        generator.load_model(model_path)
        
        # æŸ¥æ‰¾æµ‹è¯•éŸ³é¢‘
        audio_files = []
        if os.path.exists('extracted_audio'):
            audio_files = [f for f in os.listdir('extracted_audio') if f.endswith('.ogg')]
        
        if audio_files:
            test_audio = os.path.join('extracted_audio', audio_files[0])
            print(f"\nğŸµ ä½¿ç”¨æµ‹è¯•éŸ³é¢‘: {test_audio}")
            
            # ç”Ÿæˆä¸åŒéš¾åº¦çš„è°±é¢
            difficulties = ['Easy', 'Normal', 'Hard', 'Expert']
            
            for difficulty in difficulties:
                print(f"\nğŸ¯ ç”Ÿæˆ {difficulty} éš¾åº¦è°±é¢...")
                result = generator.generate_beatmap_deep(test_audio, difficulty)
                
                if result:
                    stats = result['statistics']
                    print(f"   ğŸ“Š ç”Ÿæˆç»“æœ:")
                    print(f"      â€¢ éŸ³ç¬¦æ•°é‡: {stats['note_count']}")
                    print(f"      â€¢ é•¿æ¡æ•°é‡: {stats['long_note_count']}")
                    print(f"      â€¢ éŸ³ç¬¦å¯†åº¦: {stats['note_density']:.2f} éŸ³ç¬¦/ç§’")
                    
                    # å¯è§†åŒ–ç¬¬ä¸€ä¸ªéš¾åº¦çš„ç»“æœ
                    if difficulty == 'Normal':
                        generator.visualize_generated_beatmap(
                            result, f'deep_generated_{difficulty.lower()}_beatmap.png'
                        )
        else:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
    else:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œå¤§è§„æ¨¡è®­ç»ƒæˆ–æµ‹è¯•è®­ç»ƒ")


if __name__ == "__main__":
    demo_deep_generation()
