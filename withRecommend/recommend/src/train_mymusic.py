#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºmymusicæ•°æ®åº“çš„éŸ³ä¹æ¨èæ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Flatten, Lambda
from tensorflow.keras.models import Model
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.utils.mymusic_data_loader import load_mymusic_interactions, generate_mymusic_training_data
except ImportError:
    print("âš ï¸  æ— æ³•å¯¼å…¥mymusic_data_loaderï¼Œå°†ä½¿ç”¨æœ¬åœ°æ•°æ®åŠ è½½æ–¹æ³•")

class MyMusicConfig:
    """mymusicæ•°æ®åº“é…ç½®"""
    # æ¨¡å‹å‚æ•°
    EMBEDDING_DIM = 64
    LSTM_UNITS = 128
    DENSE_UNITS = 64
    USER_TOWER_OUTPUT_DIM = 64
    SONG_TOWER_OUTPUT_DIM = 64
    
    # è®­ç»ƒå‚æ•°
    EPOCHS = 10
    BATCH_SIZE = 32
    
    # è·¯å¾„é…ç½®
    MODEL_SAVE_PATH = "models/mymusic_twin_tower.keras"

def dot_product_func(inputs):
    """ç‚¹ç§¯å‡½æ•°ï¼Œå¿…é¡»ä¸predictè„šæœ¬ä¸­çš„å®šä¹‰å®Œå…¨ä¸€è‡´"""
    user_vec, song_vec = inputs
    return tf.reduce_sum(user_vec * song_vec, axis=1, keepdims=True)

def build_mymusic_twin_tower_model(num_users: int, num_songs: int):
    """
    æ„å»ºé€‚é…mymusicæ•°æ®çš„åŒå¡”æ¨¡å‹
    
    Args:
        num_users: ç”¨æˆ·æ•°é‡
        num_songs: æ­Œæ›²æ•°é‡
        
    Returns:
        tensorflow.keras.Model: ç¼–è¯‘åçš„æ¨¡å‹
    """
    config = MyMusicConfig()
    
    # ç”¨æˆ·å¡”
    user_input = Input(shape=(1,), name='user_input')
    user_embed = Embedding(num_users, config.EMBEDDING_DIM, name='user_embedding')(user_input)
    user_lstm = LSTM(config.LSTM_UNITS, name='user_lstm')(user_embed)
    user_vec = Dense(config.USER_TOWER_OUTPUT_DIM, activation='relu', name='user_dense')(user_lstm)
    
    # æ­Œæ›²å¡”
    song_input = Input(shape=(1,), name='song_input')
    song_embed = Embedding(num_songs, config.EMBEDDING_DIM, name='song_embedding')(song_input)
    song_vec = Dense(config.SONG_TOWER_OUTPUT_DIM, activation='relu', name='song_dense')(song_embed)
    song_vec = Flatten(name='song_flatten')(song_vec)
    
    # äº¤äº’å±‚ï¼šä½¿ç”¨ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦
    dot_product = Lambda(
        dot_product_func,
        name="dot_product",
        output_shape=(1,)
    )([user_vec, song_vec])
    
    # æ„å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    model = Model(inputs=[user_input, song_input], outputs=dot_product, name='mymusic_twin_tower')
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall']
    )
    
    return model

def load_mymusic_data_fallback():
    """
    å¤‡ç”¨æ•°æ®åŠ è½½æ–¹æ³•ï¼ˆå½“æ— æ³•ä½¿ç”¨æ•°æ®åº“æ—¶ï¼‰
    """
    import json
    import numpy as np
    
    # å°è¯•ä»JSONæ–‡ä»¶åŠ è½½
    json_file = Path("data/mymusic_generated/user_plays.json")
    if not json_file.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {json_file}")
    
    print("ğŸ“ ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®...")
    with open(json_file, 'r', encoding='utf-8') as f:
        plays = json.load(f)
    
    # å¤„ç†æ•°æ®
    interactions = []
    user_ids = set()
    song_ids = set()
    
    for play in plays:
        user_id = f"user_{play['user_id']}"
        song_id = f"song_{play['song_id']}"
        
        completion_rate = play['play_duration'] / play['full_duration'] if play['full_duration'] > 0 else 0
        label = 1 if (completion_rate > 0.6 or play.get('is_liked', 0) == 1) else 0
        
        interactions.append({
            'user_id': user_id,
            'song_id': song_id,
            'label': label
        })
        
        user_ids.add(user_id)
        song_ids.add(song_id)
    
    # åˆ›å»ºæ˜ å°„
    user_to_idx = {user_id: idx for idx, user_id in enumerate(sorted(user_ids))}
    song_to_idx = {song_id: idx for idx, song_id in enumerate(sorted(song_ids))}
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    positive_samples = [(user_to_idx[item['user_id']], song_to_idx[item['song_id']], item['label']) 
                       for item in interactions]
    
    # ç”Ÿæˆè´Ÿæ ·æœ¬
    positive_pairs = set((item[0], item[1]) for item in positive_samples)
    negative_samples = []
    
    while len(negative_samples) < len(positive_samples):
        user_idx = np.random.randint(0, len(user_to_idx))
        song_idx = np.random.randint(0, len(song_to_idx))
        
        if (user_idx, song_idx) not in positive_pairs:
            negative_samples.append((user_idx, song_idx, 0))
    
    # åˆå¹¶å¹¶æ‰“ä¹±
    all_samples = positive_samples + negative_samples
    np.random.shuffle(all_samples)
    
    users = np.array([sample[0] for sample in all_samples])
    songs = np.array([sample[1] for sample in all_samples])
    labels = np.array([sample[2] for sample in all_samples])
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(user_to_idx)} ç”¨æˆ·, {len(song_to_idx)} æ­Œæ›², {len(all_samples)} æ ·æœ¬")
    
    return interactions, user_to_idx, song_to_idx, users, songs, labels

def train_mymusic_model():
    """è®­ç»ƒmymusicæ¨èæ¨¡å‹"""
    print("ğŸµ å¼€å§‹è®­ç»ƒmymusicéŸ³ä¹æ¨èæ¨¡å‹...")
    print("=" * 60)
    
    try:
        # å°è¯•ä½¿ç”¨æ•°æ®åº“åŠ è½½å™¨
        interactions, user_to_idx, song_to_idx = load_mymusic_interactions()
        users, songs, labels = generate_mymusic_training_data(interactions, user_to_idx, song_to_idx)
    except:
        # ä½¿ç”¨å¤‡ç”¨åŠ è½½æ–¹æ³•
        print("âš ï¸  ä½¿ç”¨å¤‡ç”¨æ•°æ®åŠ è½½æ–¹æ³•...")
        interactions, user_to_idx, song_to_idx, users, songs, labels = load_mymusic_data_fallback()
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   ç”¨æˆ·æ•°é‡: {len(user_to_idx)}")
    print(f"   æ­Œæ›²æ•°é‡: {len(song_to_idx)}")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(users)}")
    print(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {labels.mean():.2%}")
    
    # æ„å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸  æ„å»ºåŒå¡”æ¨¡å‹...")
    model = build_mymusic_twin_tower_model(len(user_to_idx), len(song_to_idx))
    
    # æ˜¾ç¤ºæ¨¡å‹ç»“æ„
    model.summary()
    
    # è®­ç»ƒæ¨¡å‹
    print(f"\nğŸ§  å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    config = MyMusicConfig()
    
    history = model.fit(
        [users, songs], 
        labels,
        batch_size=config.BATCH_SIZE,
        epochs=config.EPOCHS,
        validation_split=0.2,
        verbose=1
    )
    
    # ä¿å­˜æ¨¡å‹
    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)
    
    model.save(config.MODEL_SAVE_PATH)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {config.MODEL_SAVE_PATH}")
    
    # ä¿å­˜æ˜ å°„å…³ç³»ï¼ˆç”¨äºé¢„æµ‹ï¼‰
    mappings_file = Path("data/mymusic_processed/mappings.npy")
    mappings_file.parent.mkdir(exist_ok=True)
    
    import numpy as np
    mappings = {
        'user_to_idx': user_to_idx,
        'song_to_idx': song_to_idx
    }
    np.save(mappings_file, mappings)
    print(f"âœ… æ˜ å°„å…³ç³»å·²ä¿å­˜åˆ°: {mappings_file}")
    
    # æ˜¾ç¤ºè®­ç»ƒç»“æœ
    final_loss = history.history['loss'][-1]
    final_accuracy = history.history['accuracy'][-1]
    val_loss = history.history['val_loss'][-1]
    val_accuracy = history.history['val_accuracy'][-1]
    
    print(f"\nğŸ“ˆ è®­ç»ƒç»“æœ:")
    print(f"   è®­ç»ƒæŸå¤±: {final_loss:.4f}")
    print(f"   è®­ç»ƒå‡†ç¡®ç‡: {final_accuracy:.4f}")
    print(f"   éªŒè¯æŸå¤±: {val_loss:.4f}")
    print(f"   éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
    
    print(f"\nğŸ‰ mymusicæ¨¡å‹è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    train_mymusic_model()
