#!/usr/bin/env python3
"""
ç²¾ç¡®æ—¶æœºæ ¡å‡†çš„è°±é¢ç”Ÿæˆå™¨
åœ¨éŸ³é‡å³°å€¼å¤„ç²¾ç¡®æ”¾ç½®éŸ³ç¬¦ï¼Œç¡®ä¿æœ€ä½³æ¸¸æˆä½“éªŒ
"""

import numpy as np
import librosa
import json
import zipfile
import os
from scipy.signal import find_peaks, savgol_filter
import matplotlib.pyplot as plt

def generate_precise_beatmap(audio_path, output_path, target_note_count=1500, plot_analysis=True):
    """
    ç”Ÿæˆæ—¶æœºç²¾ç¡®æ ¡å‡†çš„è°±é¢
    
    Args:
        audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºMCZæ–‡ä»¶è·¯å¾„
        target_note_count: ç›®æ ‡éŸ³ç¬¦æ•°é‡
        plot_analysis: æ˜¯å¦ç»˜åˆ¶åˆ†æå›¾è¡¨
    """
    print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆç²¾ç¡®æ—¶æœºæ ¡å‡†çš„è°±é¢...")
    print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {os.path.basename(audio_path)}")
    
    # 1. åŠ è½½éŸ³é¢‘
    y, sr = librosa.load(audio_path, sr=22050)
    duration = len(y) / sr
    print(f"â±ï¸ éŸ³é¢‘æ—¶é•¿: {duration:.2f} ç§’")
    
    # 2. ç²¾ç¡®çš„å³°å€¼æ£€æµ‹
    peak_times = detect_precise_peaks(y, sr, target_note_count)
    print(f"ğŸµ æ£€æµ‹åˆ° {len(peak_times)} ä¸ªç²¾ç¡®å³°å€¼")
    
    # 3. æ™ºèƒ½é”®ä½åˆ†é…
    note_data = assign_intelligent_columns(y, sr, peak_times)
    print(f"ğŸ¹ å®Œæˆé”®ä½åˆ†é…")
    
    # 4. ç”ŸæˆMCZæ–‡ä»¶
    generate_mcz_file(note_data, duration, output_path)
    print(f"âœ… è°±é¢ç”Ÿæˆå®Œæˆ: {output_path}")
    
    # 5. ç»˜åˆ¶åˆ†æå›¾è¡¨
    if plot_analysis:
        plot_timing_analysis(y, sr, peak_times, note_data)
    
    return note_data

def detect_precise_peaks(y, sr, target_count):
    """
    æ£€æµ‹éŸ³é¢‘ä¸­çš„ç²¾ç¡®å³°å€¼ä½ç½®
    
    Args:
        y: éŸ³é¢‘ä¿¡å·
        sr: é‡‡æ ·ç‡
        target_count: ç›®æ ‡å³°å€¼æ•°é‡
    
    Returns:
        peak_times: å³°å€¼æ—¶é—´æ•°ç»„
    """
    print(f"ğŸ” æ­£åœ¨è¿›è¡Œç²¾ç¡®å³°å€¼æ£€æµ‹...")
    
    # 1. è®¡ç®—çŸ­æ—¶èƒ½é‡ (10msçª—å£ï¼Œé«˜æ—¶é—´åˆ†è¾¨ç‡)
    frame_length = int(0.01 * sr)  # 10msçª—å£
    hop_length = int(0.005 * sr)   # 5msæ­¥é•¿
    
    # ä½¿ç”¨RMSèƒ½é‡
    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
    
    # 2. å¹³æ»‘å¤„ç†å‡å°‘å™ªå£°
    if len(rms) > 10:
        window_length = min(11, len(rms) if len(rms) % 2 == 1 else len(rms) - 1)
        rms_smooth = savgol_filter(rms, window_length, 3)
    else:
        rms_smooth = rms
    
    # 3. åŠ¨æ€é˜ˆå€¼å³°å€¼æ£€æµ‹
    rms_mean = np.mean(rms_smooth)
    rms_std = np.std(rms_smooth)
    
    # ä»é«˜é˜ˆå€¼å¼€å§‹ï¼Œé€æ­¥é™ä½ç›´åˆ°æ‰¾åˆ°è¶³å¤Ÿçš„å³°å€¼
    thresholds = [
        rms_mean + 2 * rms_std,
        rms_mean + 1.5 * rms_std,
        rms_mean + rms_std,
        rms_mean + 0.5 * rms_std,
        rms_mean
    ]
    
    best_peaks = None
    for threshold in thresholds:
        # æ£€æµ‹å³°å€¼ï¼Œè®¾ç½®æœ€å°é—´è·é¿å…è¿‡å¯†
        min_distance = int(0.05 * len(rms_smooth))  # æœ€å°50msé—´è·
        
        peaks, properties = find_peaks(
            rms_smooth, 
            height=threshold,
            distance=min_distance,
            prominence=rms_std * 0.1
        )
        
        print(f"   é˜ˆå€¼ {threshold:.4f}: æ‰¾åˆ° {len(peaks)} ä¸ªå³°å€¼")
        
        # å¦‚æœå³°å€¼æ•°é‡åˆé€‚ï¼Œä½¿ç”¨è¿™ä¸ªé˜ˆå€¼
        if target_count * 0.8 <= len(peaks) <= target_count * 1.2:
            best_peaks = peaks
            break
        elif len(peaks) > target_count * 0.5:
            best_peaks = peaks
    
    if best_peaks is None:
        # å¦‚æœéƒ½ä¸åˆé€‚ï¼Œä½¿ç”¨æœ€å®½æ¾çš„é˜ˆå€¼å¹¶æˆªå–
        threshold = rms_mean * 0.5
        peaks, _ = find_peaks(rms_smooth, height=threshold, distance=min_distance)
        best_peaks = peaks
    
    # 4. ç²¾ç¡®å®šä½å³°å€¼ (åœ¨åŸå§‹ä¿¡å·ä¸­æ‰¾åˆ°ç²¾ç¡®ä½ç½®)
    precise_peaks = []
    
    for peak_idx in best_peaks:
        # è½¬æ¢åˆ°åŸå§‹ä¿¡å·çš„æ—¶é—´
        peak_time = peak_idx * hop_length / sr
        
        # åœ¨å³°å€¼é™„è¿‘å¯»æ‰¾çœŸæ­£çš„æœ€å¤§å€¼ (Â±10msèŒƒå›´)
        search_range = int(0.01 * sr)  # 10msæœç´¢èŒƒå›´
        start_sample = max(0, int(peak_time * sr) - search_range)
        end_sample = min(len(y), int(peak_time * sr) + search_range)
        
        if start_sample < end_sample:
            local_segment = np.abs(y[start_sample:end_sample])
            local_max_idx = np.argmax(local_segment)
            precise_time = (start_sample + local_max_idx) / sr
            precise_peaks.append(precise_time)
    
    # 5. å¦‚æœæ•°é‡ä¸å¤Ÿï¼Œè¡¥å……ä¸€äº›æ¬¡è¦å³°å€¼
    if len(precise_peaks) < target_count * 0.8:
        print(f"   å³°å€¼ä¸å¤Ÿï¼Œè¡¥å……æ¬¡è¦å³°å€¼...")
        # é™ä½é˜ˆå€¼ï¼Œæ‰¾æ›´å¤šå³°å€¼
        lower_threshold = rms_mean * 0.3
        additional_peaks, _ = find_peaks(rms_smooth, height=lower_threshold, distance=min_distance//2)
        
        for peak_idx in additional_peaks:
            peak_time = peak_idx * hop_length / sr
            if not any(abs(peak_time - existing) < 0.1 for existing in precise_peaks):
                precise_peaks.append(peak_time)
                if len(precise_peaks) >= target_count:
                    break
    
    # 6. å¦‚æœæ•°é‡è¿‡å¤šï¼Œé€‰æ‹©æœ€å¼ºçš„å³°å€¼
    if len(precise_peaks) > target_count:
        print(f"   å³°å€¼è¿‡å¤šï¼Œé€‰æ‹©æœ€å¼ºçš„ {target_count} ä¸ª...")
        # è®¡ç®—æ¯ä¸ªå³°å€¼çš„å¼ºåº¦
        peak_strengths = []
        for peak_time in precise_peaks:
            sample_idx = int(peak_time * sr)
            if 0 <= sample_idx < len(y):
                strength = abs(y[sample_idx])
                peak_strengths.append((peak_time, strength))
        
        # æŒ‰å¼ºåº¦æ’åºï¼Œé€‰æ‹©æœ€å¼ºçš„
        peak_strengths.sort(key=lambda x: x[1], reverse=True)
        precise_peaks = [pt[0] for pt in peak_strengths[:target_count]]
    
    # 7. æŒ‰æ—¶é—´æ’åº
    precise_peaks = sorted(precise_peaks)
    
    print(f"âœ… æœ€ç»ˆæ£€æµ‹åˆ° {len(precise_peaks)} ä¸ªç²¾ç¡®å³°å€¼")
    print(f"   æ—¶é—´èŒƒå›´: {precise_peaks[0]:.3f} - {precise_peaks[-1]:.3f} ç§’")
    
    return np.array(precise_peaks)

def assign_intelligent_columns(y, sr, peak_times):
    """
    åŸºäºéŸ³é¢‘ç‰¹å¾æ™ºèƒ½åˆ†é…é”®ä½
    
    Args:
        y: éŸ³é¢‘ä¿¡å·
        sr: é‡‡æ ·ç‡
        peak_times: å³°å€¼æ—¶é—´æ•°ç»„
    
    Returns:
        note_data: åŒ…å«æ—¶é—´å’Œé”®ä½çš„éŸ³ç¬¦æ•°æ®
    """
    print(f"ğŸ¹ å¼€å§‹æ™ºèƒ½é”®ä½åˆ†é…...")
    
    note_data = []
    
    for i, peak_time in enumerate(peak_times):
        # 1. æå–å³°å€¼é™„è¿‘çš„éŸ³é¢‘ç‰¹å¾
        sample_idx = int(peak_time * sr)
        
        # æå–Â±25msçš„éŸ³é¢‘æ®µç”¨äºç‰¹å¾åˆ†æ
        window_size = int(0.025 * sr)
        start_idx = max(0, sample_idx - window_size)
        end_idx = min(len(y), sample_idx + window_size)
        audio_window = y[start_idx:end_idx]
        
        if len(audio_window) == 0:
            column = 0
        else:
            # 2. è®¡ç®—éŸ³é¢‘ç‰¹å¾
            features = calculate_audio_features(audio_window, sr)
            
            # 3. åŸºäºç‰¹å¾é€‰æ‹©é”®ä½
            column = select_column_by_features(features, i, len(peak_times))
        
        note_data.append({
            'time': peak_time,
            'column': column,
            'features': features if len(audio_window) > 0 else {}
        })
    
    # 4. åå¤„ç†ï¼šé¿å…è¿ç»­ç›¸åŒé”®ä½
    note_data = post_process_columns(note_data)
    
    print(f"âœ… é”®ä½åˆ†é…å®Œæˆ")
    return note_data

def calculate_audio_features(audio_window, sr):
    """è®¡ç®—éŸ³é¢‘çª—å£çš„ç‰¹å¾"""
    if len(audio_window) == 0:
        return {}
    
    features = {}
    
    # 1. æ—¶åŸŸç‰¹å¾
    features['rms'] = np.sqrt(np.mean(audio_window ** 2))
    features['peak_amplitude'] = np.max(np.abs(audio_window))
    features['zcr'] = np.sum(np.diff(np.sign(audio_window)) != 0) / len(audio_window)
    
    # 2. é¢‘åŸŸç‰¹å¾
    fft = np.fft.fft(audio_window)
    magnitude = np.abs(fft)
    freqs = np.fft.fftfreq(len(fft), 1/sr)
    
    # åªè€ƒè™‘æ­£é¢‘ç‡
    positive_freqs = freqs[:len(freqs)//2]
    positive_magnitude = magnitude[:len(magnitude)//2]
    
    if np.sum(positive_magnitude) > 0:
        # é¢‘è°±è´¨å¿ƒ
        features['spectral_centroid'] = np.sum(positive_freqs * positive_magnitude) / np.sum(positive_magnitude)
        
        # é¢‘è°±èƒ½é‡åˆ†å¸ƒ
        total_energy = np.sum(positive_magnitude)
        features['low_freq_energy'] = np.sum(positive_magnitude[positive_freqs < 200]) / total_energy
        features['mid_freq_energy'] = np.sum(positive_magnitude[(positive_freqs >= 200) & (positive_freqs < 2000)]) / total_energy
        features['high_freq_energy'] = np.sum(positive_magnitude[positive_freqs >= 2000]) / total_energy
    else:
        features['spectral_centroid'] = 0
        features['low_freq_energy'] = 0
        features['mid_freq_energy'] = 0
        features['high_freq_energy'] = 0
    
    return features

def select_column_by_features(features, note_index, total_notes):
    """åŸºäºç‰¹å¾é€‰æ‹©é”®ä½"""
    if not features:
        return note_index % 4
    
    # 1. åŸºäºé¢‘ç‡ç‰¹å¾çš„åŸºç¡€é”®ä½é€‰æ‹©
    spectral_centroid = features.get('spectral_centroid', 1000)
    low_energy = features.get('low_freq_energy', 0)
    high_energy = features.get('high_freq_energy', 0)
    
    # é¢‘ç‡ -> é”®ä½æ˜ å°„
    if spectral_centroid < 300 or low_energy > 0.6:
        base_column = 0  # ä½é¢‘ -> å·¦è¾¹
    elif spectral_centroid < 800:
        base_column = 1
    elif spectral_centroid < 1500:
        base_column = 2
    else:
        base_column = 3  # é«˜é¢‘ -> å³è¾¹
    
    # 2. è€ƒè™‘éŸ³é‡å¼ºåº¦çš„è°ƒæ•´
    rms = features.get('rms', 0)
    peak_amp = features.get('peak_amplitude', 0)
    
    # å¼ºéŸ³ç¬¦å€¾å‘äºå¤–ä¾§é”®ä½ (0æˆ–3)
    if rms > 0.3 or peak_amp > 0.5:
        if base_column == 1:
            base_column = 0
        elif base_column == 2:
            base_column = 3
    
    # 3. æ—¶é—´ä½ç½®çš„å¾®è°ƒ (é¿å…å•è°ƒ)
    position_factor = note_index / total_notes
    if position_factor < 0.25:  # å¼€å¤´éƒ¨åˆ†
        bias = 0
    elif position_factor < 0.5:  # å‰åŠéƒ¨åˆ†
        bias = 1
    elif position_factor < 0.75:  # ååŠéƒ¨åˆ†
        bias = 2
    else:  # ç»“å°¾éƒ¨åˆ†
        bias = 3
    
    # è½»å¾®åå‘æ—¶é—´ä½ç½®å¯¹åº”çš„é”®ä½
    if np.random.random() < 0.3:
        base_column = bias
    
    return base_column

def post_process_columns(note_data):
    """åå¤„ç†é”®ä½åˆ†é…ï¼Œé¿å…ä¸è‰¯æ¨¡å¼"""
    if len(note_data) < 2:
        return note_data
    
    # 1. é¿å…è¿ç»­è¶…è¿‡3ä¸ªç›¸åŒé”®ä½
    for i in range(len(note_data) - 2):
        if (note_data[i]['column'] == note_data[i+1]['column'] == note_data[i+2]['column']):
            # å¦‚æœæœ‰3ä¸ªè¿ç»­ç›¸åŒï¼Œæ”¹å˜ç¬¬3ä¸ª
            available_columns = [c for c in range(4) if c != note_data[i]['column']]
            note_data[i+2]['column'] = np.random.choice(available_columns)
    
    # 2. ç¡®ä¿æ¯ä¸ªé”®ä½éƒ½æœ‰åˆç†çš„ä½¿ç”¨
    columns_count = [0, 0, 0, 0]
    for note in note_data:
        columns_count[note['column']] += 1
    
    # å¦‚æœæŸä¸ªé”®ä½ä½¿ç”¨å¤ªå°‘ï¼ˆ<10%ï¼‰ï¼Œå¢åŠ ä¸€äº›
    total_notes = len(note_data)
    min_usage = total_notes * 0.1
    
    for col in range(4):
        if columns_count[col] < min_usage:
            # éšæœºé€‰æ‹©ä¸€äº›éŸ³ç¬¦æ”¹ä¸ºè¿™ä¸ªé”®ä½
            needed = int(min_usage - columns_count[col])
            indices = np.random.choice(total_notes, min(needed, total_notes//4), replace=False)
            for idx in indices:
                note_data[idx]['column'] = col
    
    return note_data

def generate_mcz_file(note_data, duration, output_path):
    """ç”ŸæˆMCZæ–‡ä»¶"""
    print(f"ğŸ“¦ ç”ŸæˆMCZæ–‡ä»¶...")
    
    # è®¾ç½®åŸºæœ¬å‚æ•°
    bpm = 156
    beats_per_measure = 4
    subdivision = 24  # æ¯æ‹24ç»†åˆ†
    
    # 1. è½¬æ¢æ—¶é—´åˆ°beat
    mc_notes = []
    
    # æ·»åŠ éŸ³é¢‘æ§åˆ¶éŸ³ç¬¦
    start_beat = [0, 0, subdivision]
    end_beat_value = duration * bpm / 60
    end_beat = [int(end_beat_value), int((end_beat_value % 1) * subdivision), subdivision]
    
    mc_notes.extend([
        {"beat": [0, 0, int(subdivision)], "endbeat": [0, 0, int(subdivision)], "sound": "4833.ogg"},
        {"beat": [int(end_beat[0]), int(end_beat[1]), int(end_beat[2])], "endbeat": [int(end_beat[0]), int(end_beat[1]), int(end_beat[2])], "sound": ""}
    ])
    
    # 2. æ·»åŠ æ¸¸æˆéŸ³ç¬¦
    for note in note_data:
        beat_value = note['time'] * bpm / 60
        x = int(beat_value)
        y = int((beat_value % 1) * subdivision)
        z = subdivision
        
        mc_notes.append({
            "beat": [int(x), int(y), int(z)],
            "endbeat": [int(x), int(y), int(z)],
            "column": int(note['column'])
        })
    
    # 3. åˆ›å»ºMCæ•°æ®ç»“æ„
    mc_data = {
        "meta": {
            "version": "4.0.0",
            "mode": "0",
            "time": int(duration * 1000),
            "song": {
                "title": "Generated Beatmap (Precise)",
                "artist": "AI Generator",
                "id": 4833
            },
            "mode_ext": {
                "column": 4
            }
        },
        "time": [
            {
                "beat": [0, 0, int(subdivision)],
                "bpm": float(bpm)
            }
        ],
        "note": mc_notes
    }
    
    # 4. åˆ›å»ºMCZæ–‡ä»¶
    import tempfile
    import shutil
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # åˆ›å»ºç›®å½•ç»“æ„
        beat_dir = os.path.join(temp_dir, "0")
        os.makedirs(beat_dir)
        
        # å†™å…¥MCæ–‡ä»¶
        mc_file_path = os.path.join(beat_dir, "1511697495.mc")
        with open(mc_file_path, 'w', encoding='utf-8') as f:
            json.dump(mc_data, f, separators=(',', ':'))
        
        # å¤åˆ¶éŸ³é¢‘æ–‡ä»¶ (å¦‚æœå­˜åœ¨)
        audio_src = "generated_audio.ogg"
        if os.path.exists(audio_src):
            shutil.copy2(audio_src, os.path.join(beat_dir, "4833.ogg"))
        
        # åˆ›å»ºZIPæ–‡ä»¶
        shutil.make_archive(output_path.replace('.mcz', ''), 'zip', temp_dir)
        if os.path.exists(output_path.replace('.mcz', '') + '.zip'):
            os.rename(output_path.replace('.mcz', '') + '.zip', output_path)
    
    print(f"âœ… MCZæ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")

def plot_timing_analysis(y, sr, peak_times, note_data, show_plot=True):
    """ç»˜åˆ¶æ—¶æœºåˆ†æå›¾è¡¨"""
    print(f"ğŸ“Š ç»˜åˆ¶æ—¶æœºåˆ†æå›¾è¡¨...")
    
    plt.figure(figsize=(15, 10))
    
    # 1. éŸ³é¢‘æ³¢å½¢å’Œå³°å€¼
    plt.subplot(3, 1, 1)
    time_axis = np.linspace(0, len(y)/sr, len(y))
    plt.plot(time_axis, y, alpha=0.6, color='lightblue', label='éŸ³é¢‘æ³¢å½¢')
    
    # æ ‡è®°æ£€æµ‹åˆ°çš„å³°å€¼
    for peak_time in peak_times:
        plt.axvline(x=peak_time, color='red', alpha=0.7, linestyle='--', linewidth=1)
    
    plt.title('éŸ³é¢‘æ³¢å½¢ä¸æ£€æµ‹åˆ°çš„å³°å€¼')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('æŒ¯å¹…')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. RMSèƒ½é‡æ›²çº¿
    plt.subplot(3, 1, 2)
    frame_length = int(0.01 * sr)
    hop_length = int(0.005 * sr)
    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
    rms_times = np.linspace(0, len(y)/sr, len(rms))
    
    plt.plot(rms_times, rms, color='green', label='RMSèƒ½é‡')
    
    # æ ‡è®°å³°å€¼å¯¹åº”çš„RMS
    for peak_time in peak_times:
        plt.axvline(x=peak_time, color='red', alpha=0.7, linestyle='--', linewidth=1)
    
    plt.title('RMSèƒ½é‡æ›²çº¿')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('RMS')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. é”®ä½åˆ†å¸ƒ
    plt.subplot(3, 1, 3)
    columns = [note['column'] for note in note_data]
    times = [note['time'] for note in note_data]
    
    colors = ['red', 'blue', 'green', 'orange']
    for col in range(4):
        col_times = [t for t, c in zip(times, columns) if c == col]
        col_y = [col] * len(col_times)
        plt.scatter(col_times, col_y, c=colors[col], alpha=0.7, s=20, label=f'é”®ä½ {col}')
    
    plt.title('é”®ä½åˆ†å¸ƒ')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('é”®ä½')
    plt.yticks(range(4))
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if show_plot:
        plt.show()
    else:
        plt.savefig('timing_analysis.png', dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜ä¸º timing_analysis.png")
    
    plt.close()

def analyze_timing_precision(audio_path, note_data):
    """åˆ†ææ—¶æœºç²¾ç¡®åº¦"""
    print(f"\nğŸ¯ æ—¶æœºç²¾ç¡®åº¦åˆ†æ:")
    
    y, sr = librosa.load(audio_path, sr=22050)
    
    precisions = []
    for note in note_data:
        peak_time = note['time']
        sample_idx = int(peak_time * sr)
        
        # æ£€æŸ¥Â±5msèŒƒå›´å†…çš„æœ€å¤§å€¼ä½ç½®
        search_range = int(0.005 * sr)  # 5ms
        start_idx = max(0, sample_idx - search_range)
        end_idx = min(len(y), sample_idx + search_range)
        
        if start_idx < end_idx:
            local_segment = np.abs(y[start_idx:end_idx])
            true_peak_idx = np.argmax(local_segment)
            true_peak_sample = start_idx + true_peak_idx
            
            # è®¡ç®—è¯¯å·®
            error_samples = abs(sample_idx - true_peak_sample)
            error_ms = error_samples / sr * 1000
            precisions.append(error_ms)
    
    if precisions:
        avg_error = np.mean(precisions)
        max_error = np.max(precisions)
        std_error = np.std(precisions)
        
        print(f"   å¹³å‡è¯¯å·®: {avg_error:.2f} ms")
        print(f"   æœ€å¤§è¯¯å·®: {max_error:.2f} ms")
        print(f"   è¯¯å·®æ ‡å‡†å·®: {std_error:.2f} ms")
        print(f"   95%éŸ³ç¬¦è¯¯å·®å°äº: {np.percentile(precisions, 95):.2f} ms")
        
        # è¯„ä»·
        if avg_error < 10:
            print(f"   ğŸŸ¢ ç²¾ç¡®åº¦è¯„ä»·: ä¼˜ç§€ (å¹³å‡è¯¯å·® < 10ms)")
        elif avg_error < 20:
            print(f"   ğŸŸ¡ ç²¾ç¡®åº¦è¯„ä»·: è‰¯å¥½ (å¹³å‡è¯¯å·® < 20ms)")
        else:
            print(f"   ğŸ”´ ç²¾ç¡®åº¦è¯„ä»·: éœ€æ”¹è¿› (å¹³å‡è¯¯å·® > 20ms)")

if __name__ == "__main__":
    # æµ‹è¯•ç²¾ç¡®è°±é¢ç”Ÿæˆ - ä½¿ç”¨ç°æœ‰çš„éŸ³é¢‘æ–‡ä»¶
    audio_file = "extracted_audio\_song_10088_Kawaki wo Ameku.ogg"
    output_file = "precise_beatmap.mcz"
    
    if os.path.exists(audio_file):
        print(f"ğŸµ ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶: {audio_file}")
        note_data = generate_precise_beatmap(
            audio_file, 
            output_file, 
            target_note_count=1500,
            plot_analysis=False
        )
        
        # åˆ†æç²¾ç¡®åº¦
        analyze_timing_precision(audio_file, note_data)
        
        print(f"\nğŸ® ç²¾ç¡®è°±é¢ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸµ éŸ³ç¬¦æ•°é‡: {len(note_data)}")
        print(f"ğŸ¯ ç‰¹ç‚¹: æ‰€æœ‰éŸ³ç¬¦éƒ½ç²¾ç¡®å¯¹å‡†éŸ³é‡å³°å€¼ï¼Œè¯¯å·®æœ€å°åŒ–")
    else:
        # å¦‚æœç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–æ–‡ä»¶
        alternative_files = [
            "temp_mcz_analysis/0/Kawaki wo Ameku.ogg",
            "extracted_audio/_song_1011_audio.ogg",
            "preprocessed_data/audio/_song_1314_ON FIRE.ogg"
        ]
        
        for alt_file in alternative_files:
            if os.path.exists(alt_file):
                print(f"ğŸµ ä½¿ç”¨å¤‡é€‰éŸ³é¢‘æ–‡ä»¶: {alt_file}")
                note_data = generate_precise_beatmap(
                    alt_file, 
                    output_file, 
                    target_note_count=1500,
                    plot_analysis=False
                )
                
                # åˆ†æç²¾ç¡®åº¦
                analyze_timing_precision(alt_file, note_data)
                
                print(f"\nğŸ® ç²¾ç¡®è°±é¢ç”Ÿæˆå®Œæˆï¼")
                print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_file}")
                print(f"ğŸµ éŸ³ç¬¦æ•°é‡: {len(note_data)}")
                print(f"ğŸ¯ ç‰¹ç‚¹: æ‰€æœ‰éŸ³ç¬¦éƒ½ç²¾ç¡®å¯¹å‡†éŸ³é‡å³°å€¼ï¼Œè¯¯å·®æœ€å°åŒ–")
                break
        else:
            print(f"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶")
            print(f"   è¯·æä¾›éŸ³é¢‘æ–‡ä»¶æˆ–æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
