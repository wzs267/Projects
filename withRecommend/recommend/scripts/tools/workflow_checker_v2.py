#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
éŸ³ä¹æ¨èç³»ç»Ÿæµç¨‹æ£€æŸ¥å™¨ (æ–°ç‰ˆæœ¬ - é€‚åº”é‡ç»„åçš„é¡¹ç›®ç»“æ„)
æ£€æŸ¥æ¯ä¸ªæ­¥éª¤çš„æ‰§è¡ŒçŠ¶æ€å’Œæ•°æ®å®Œæ•´æ€§
"""

import os
import json
import numpy as np
from datetime import datetime

class WorkflowChecker:
    def __init__(self):
        self.status = {}
        self.errors = []
        
    def print_header(self, title):
        print(f"\n{'='*50}")
        print(f"ğŸ” {title}")
        print(f"{'='*50}")
    
    def check_step(self, step_name, condition, details=""):
        status = "âœ… é€šè¿‡" if condition else "âŒ å¤±è´¥"
        self.status[step_name] = condition
        print(f"{status} {step_name}")
        if details:
            print(f"   {details}")
        if not condition:
            self.errors.append(step_name)
    
    def check_environment(self):
        """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
        self.print_header("ç¯å¢ƒæ£€æŸ¥")
        
        # æ£€æŸ¥Pythonæ¨¡å—
        try:
            import numpy
            numpy_version = numpy.__version__
            self.check_step("NumPy", True, f"ç‰ˆæœ¬: {numpy_version}")
        except ImportError:
            self.check_step("NumPy", False, "æœªå®‰è£…numpy")
        
        try:
            import tensorflow as tf
            tf_version = tf.__version__
            self.check_step("TensorFlow", True, f"ç‰ˆæœ¬: {tf_version}")
        except ImportError:
            self.check_step("TensorFlow", False, "æœªå®‰è£…tensorflow")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_exists = os.path.exists('src/config.py')
        self.check_step("é…ç½®æ–‡ä»¶", config_exists, "src/config.py")
        
        if config_exists:
            try:
                import sys
                sys.path.append('src')
                from config import Config
                cfg = Config()
                self.check_step("é…ç½®æ–‡ä»¶æ ¼å¼", True, f"ç”¨æˆ·æ•°: {cfg.NUM_USERS}, æ­Œæ›²æ•°: {cfg.NUM_SONGS}")
            except Exception as e:
                self.check_step("é…ç½®æ–‡ä»¶æ ¼å¼", False, f"å¯¼å…¥é”™è¯¯: {str(e)}")
    
    def check_data_generation(self):
        """æ£€æŸ¥æ•°æ®ç”Ÿæˆæ­¥éª¤"""
        self.print_header("æ•°æ®ç”Ÿæˆæ£€æŸ¥")
        
        # æ£€æŸ¥åŸå§‹æ•°æ®
        raw_data_exists = os.path.exists('data/raw/user_plays.json')
        self.check_step("åŸå§‹æ•°æ®æ–‡ä»¶", raw_data_exists, "data/raw/user_plays.json")
        
        if raw_data_exists:
            try:
                with open('data/raw/user_plays.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.check_step("åŸå§‹æ•°æ®æ ¼å¼", True, f"åŒ…å« {len(data)} æ¡è®°å½•")
                    
                    # æ£€æŸ¥æ•°æ®å­—æ®µ
                    if data and len(data) > 0:
                        first_record = data[0]
                        expected_fields = ['user_id', 'song_id', 'duration']
                        has_all_fields = all(field in first_record for field in expected_fields)
                        self.check_step("æ•°æ®å­—æ®µå®Œæ•´", has_all_fields, f"å­—æ®µ: {list(first_record.keys())}")
            except Exception as e:
                self.check_step("åŸå§‹æ•°æ®æ ¼å¼", False, f"è¯»å–é”™è¯¯: {str(e)}")
        
        # æ£€æŸ¥æµ‹è¯•æ•°æ®
        test_files = ['data/test/users.json', 'data/test/songs.json', 'data/test/user_plays.json']
        for test_file in test_files:
            exists = os.path.exists(test_file)
            name = os.path.basename(test_file)
            self.check_step(f"æµ‹è¯•æ•°æ® {name}", exists, test_file)
        
        # æ£€æŸ¥SQLè„šæœ¬ç›®å½•
        sql_dir = 'sql/data'
        sql_dir_exists = os.path.exists(sql_dir)
        self.check_step("SQLè„šæœ¬ç›®å½•", sql_dir_exists, sql_dir)
        
        if sql_dir_exists:
            sql_files = [f for f in os.listdir(sql_dir) if f.endswith('.sql')]
            self.check_step("SQLæ–‡ä»¶æ•°é‡", len(sql_files) > 0, f"æ‰¾åˆ° {len(sql_files)} ä¸ªSQLæ–‡ä»¶")
    
    def check_preprocessing(self):
        """æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤"""
        self.print_header("æ•°æ®é¢„å¤„ç†æ£€æŸ¥")
        
        # æ£€æŸ¥å¤„ç†åçš„æ•°æ®ç›®å½•
        processed_dir = 'data/processed'
        processed_exists = os.path.exists(processed_dir)
        self.check_step("å¤„ç†æ•°æ®ç›®å½•", processed_exists, processed_dir)
        
        # æ£€æŸ¥äº¤äº’çŸ©é˜µ
        interactions_file = 'data/processed/interactions.npy'
        interactions_exists = os.path.exists(interactions_file)
        self.check_step("äº¤äº’çŸ©é˜µæ–‡ä»¶", interactions_exists, interactions_file)
        
        if interactions_exists:
            try:
                interactions = np.load(interactions_file)
                self.check_step("äº¤äº’çŸ©é˜µæ ¼å¼", True, f"çŸ©é˜µå½¢çŠ¶: {interactions.shape}")
                
                # æ£€æŸ¥å€¼åŸŸ
                min_val, max_val = interactions.min(), interactions.max()
                self.check_step("äº¤äº’çŸ©é˜µå€¼åŸŸ", True, f"å€¼åŸŸ: [{min_val:.3f}, {max_val:.3f}]")
            except Exception as e:
                self.check_step("äº¤äº’çŸ©é˜µæ ¼å¼", False, f"åŠ è½½é”™è¯¯: {str(e)}")
        
        # æ£€æŸ¥æ˜ å°„æ–‡ä»¶
        mappings_file = 'data/processed/mappings.npy'
        mappings_exists = os.path.exists(mappings_file)
        self.check_step("æ˜ å°„æ–‡ä»¶", mappings_exists, mappings_file)
        
        if mappings_exists:
            try:
                mappings = np.load(mappings_file, allow_pickle=True)
                if isinstance(mappings, np.ndarray) and mappings.ndim == 0:
                    # å•ä¸ªæ˜ å°„å­—å…¸
                    mapping_dict = mappings.item()
                    self.check_step("ç”¨æˆ·æ˜ å°„", len(mapping_dict) > 0, f"ç”¨æˆ·æ•°: {len(mapping_dict)}")
                    self.check_step("æ­Œæ›²æ˜ å°„", True, "æ•°æ®å­˜å‚¨åœ¨äº¤äº’çŸ©é˜µä¸­")
                else:
                    # å¤šä¸ªæ˜ å°„çš„å­—å…¸æ ¼å¼
                    user_mapping = mappings.get('user_to_idx', {})
                    song_mapping = mappings.get('song_to_idx', {})
                    self.check_step("ç”¨æˆ·æ˜ å°„", len(user_mapping) > 0, f"ç”¨æˆ·æ•°: {len(user_mapping)}")
                    self.check_step("æ­Œæ›²æ˜ å°„", len(song_mapping) > 0, f"æ­Œæ›²æ•°: {len(song_mapping)}")
            except Exception as e:
                self.check_step("æ˜ å°„æ–‡ä»¶æ ¼å¼", False, f"åŠ è½½é”™è¯¯: {str(e)}")
    
    def check_training(self):
        """æ£€æŸ¥æ¨¡å‹è®­ç»ƒæ­¥éª¤"""
        self.print_header("æ¨¡å‹è®­ç»ƒæ£€æŸ¥")
        
        # æ£€æŸ¥æ¨¡å‹ç›®å½•
        models_dir = 'models'
        models_exists = os.path.exists(models_dir)
        self.check_step("æ¨¡å‹ç›®å½•", models_exists, models_dir)
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_file = 'models/twin_tower.keras'
        model_exists = os.path.exists(model_file)
        self.check_step("æ¨¡å‹æ–‡ä»¶", model_exists, model_file)
        
        if model_exists:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(model_file) / (1024 * 1024)  # MB
            self.check_step("æ¨¡å‹æ–‡ä»¶å¤§å°", file_size > 0.1, f"{file_size:.2f} MB")
            
            # æ£€æŸ¥åˆ›å»ºæ—¶é—´
            create_time = datetime.fromtimestamp(os.path.getctime(model_file))
            self.check_step("æ¨¡å‹åˆ›å»ºæ—¶é—´", True, create_time.strftime("%Y-%m-%d %H:%M:%S"))
        
        # æ£€æŸ¥è®­ç»ƒè„šæœ¬
        train_script = 'src/train.py'
        train_exists = os.path.exists(train_script)
        self.check_step("è®­ç»ƒè„šæœ¬", train_exists, train_script)
    
    def check_prediction(self):
        """æ£€æŸ¥é¢„æµ‹åŠŸèƒ½"""
        self.print_header("é¢„æµ‹åŠŸèƒ½æ£€æŸ¥")
        
        # æ£€æŸ¥é¢„æµ‹è„šæœ¬
        predict_script = 'src/predict.py'
        predict_exists = os.path.exists(predict_script)
        self.check_step("é¢„æµ‹è„šæœ¬", predict_exists, predict_script)
        
        # æ£€æŸ¥å·¥å…·ç›®å½•
        utils_dir = 'src/utils'
        utils_exists = os.path.exists(utils_dir)
        self.check_step("å·¥å…·ç›®å½•", utils_exists, utils_dir)
        
        # å°è¯•å¯¼å…¥é¢„æµ‹å‡½æ•°
        if predict_exists:
            try:
                import sys
                sys.path.append('src')
                from predict import predict_for_user
                self.check_step("é¢„æµ‹å‡½æ•°å¯¼å…¥", True, "æˆåŠŸå¯¼å…¥ predict_for_user")
            except ImportError as e:
                self.check_step("é¢„æµ‹å‡½æ•°å¯¼å…¥", False, f"å¯¼å…¥é”™è¯¯: {str(e)}")
    
    def check_deployment(self):
        """æ£€æŸ¥éƒ¨ç½²å°±ç»ªçŠ¶æ€"""
        self.print_header("éƒ¨ç½²å°±ç»ªæ£€æŸ¥")
        
        # æ ¸å¿ƒæ–‡ä»¶
        required_files = [
            'src/config.py', 'src/train.py', 'src/predict.py', 'requirements.txt',
            'scripts/data_generation/data_generator.py', 'sql/schema/database_schema.sql'
        ]
        
        for file_path in required_files:
            exists = os.path.exists(file_path)
            filename = os.path.basename(file_path)
            self.check_step(f"å¿…éœ€æ–‡ä»¶ {filename}", exists, file_path)
        
        # æ–‡æ¡£æ–‡ä»¶
        doc_files = ['docs/DEPLOYMENT_GUIDE.md', 'docs/DATABASE_INIT_GUIDE.md', 'docs/DETAILED_WORKFLOW.md']
        for doc_file in doc_files:
            exists = os.path.exists(doc_file)
            filename = os.path.basename(doc_file)
            self.check_step(f"æ–‡æ¡£æ–‡ä»¶ {filename}", exists, doc_file)
        
        # è‡ªåŠ¨åŒ–è„šæœ¬
        auto_scripts = ['scripts/database/init_database.bat', 'scripts/database/run_all.bat']
        for script_file in auto_scripts:
            exists = os.path.exists(script_file)
            filename = os.path.basename(script_file)
            self.check_step(f"è‡ªåŠ¨åŒ–è„šæœ¬ {filename}", exists, script_file)
    
    def print_summary(self):
        """æ‰“å°æ£€æŸ¥æ€»ç»“"""
        self.print_header("æ£€æŸ¥æ€»ç»“")
        
        total_checks = len(self.status)
        passed_checks = sum(self.status.values())
        failed_checks = total_checks - passed_checks
        success_rate = (passed_checks / total_checks) * 100 if total_checks > 0 else 0
        
        print(f"ğŸ“Š æ€»æ£€æŸ¥é¡¹: {total_checks}")
        print(f"âœ… é€šè¿‡é¡¹ç›®: {passed_checks}")
        print(f"âŒ å¤±è´¥é¡¹ç›®: {failed_checks}")
        print(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.1f}%")
        
        if self.errors:
            print("\nâš ï¸  éœ€è¦å…³æ³¨çš„é—®é¢˜:")
            for error in self.errors:
                print(f"   - {error}")
        
        print("\nğŸ’¡ å»ºè®®:")
        if failed_checks > 0:
            print("   - åˆå§‹åŒ–æ•°æ®åº“: è¿è¡Œ scripts/database/init_database.bat æˆ–ç›¸åº”SQLè„šæœ¬")
            print("   - ç”Ÿæˆæµ‹è¯•æ•°æ®: è¿è¡Œ python scripts/data_generation/data_generator.py")
            print("   - å®Œæ•´è¿è¡Œæµç¨‹è¯·å‚è€ƒ: docs/DETAILED_WORKFLOW.md")
        else:
            print("   - ğŸ‰ ç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œæ‰€æœ‰æ£€æŸ¥é¡¹ç›®éƒ½é€šè¿‡ï¼")
    
    def run_all_checks(self):
        """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
        print("ğŸµ éŸ³ä¹æ¨èç³»ç»Ÿæµç¨‹æ£€æŸ¥å™¨")
        print(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.check_environment()
        self.check_data_generation()
        self.check_preprocessing()
        self.check_training()
        self.check_prediction()
        self.check_deployment()
        self.print_summary()

if __name__ == "__main__":
    checker = WorkflowChecker()
    checker.run_all_checks()
