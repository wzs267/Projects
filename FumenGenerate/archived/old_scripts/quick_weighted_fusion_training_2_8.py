#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æƒé‡èåˆå¿«é€Ÿè®­ç»ƒè„šæœ¬ (RF:NN = 2:8)
ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡Œé«˜æ•ˆè®­ç»ƒ
"""

import os
import sys
import time
im    def create_weighted_fusion_model(self, input_dim: int = 15):
        """åˆ›å»ºæƒé‡èåˆæ¨¡å‹ - ä¸å®Œæ•´ç‰ˆæœ¬å¯¹é½"""
        print("ğŸ—ï¸ åˆ›å»ºæƒé‡èåˆæ¨¡å‹...")
        
        self.model = WeightedFusionTransformer(
            input_dim=input_dim,
            d_model=256,          # ä¸å®Œæ•´ç‰ˆæœ¬å¯¹é½ï¼šæ›´å¤§çš„ç‰¹å¾ç»´åº¦
            num_heads=8,          # ä¸å®Œæ•´ç‰ˆæœ¬å¯¹é½ï¼šæ›´å¤šæ³¨æ„åŠ›å¤´
            num_layers=6,         # ä¸å®Œæ•´ç‰ˆæœ¬å¯¹é½ï¼šæ›´æ·±çš„ç½‘ç»œ
            dropout=0.1,
            rf_weight=self.rf_weight,
            nn_weight=self.nn_weight,
            learnable_weights=True
        ).to(self.device)
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=0.01      # æ·»åŠ æƒé‡è¡°å‡
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä¸å®Œæ•´ç‰ˆæœ¬å¯¹é½
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )ch
import torch.nn as nn
import numpy as np
import json
from models.deep_learning_beatmap_system import DeepBeatmapLearningSystem
from models.weighted_fusion_model import WeightedFusionTransformer

class WeightedFusionQuickSystem(DeepBeatmapLearningSystem):
    """æƒé‡èåˆå¿«é€Ÿè®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, rf_weight=0.3, nn_weight=0.7, **kwargs):
        super().__init__(**kwargs)
        self.rf_weight = rf_weight
        self.nn_weight = nn_weight
        print(f"ğŸ¤ æƒé‡èåˆç³»ç»Ÿåˆå§‹åŒ–:")
        print(f"   ğŸŒ² RFæƒé‡: {rf_weight}")
        print(f"   ğŸ§  NNæƒé‡: {nn_weight}")
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch - ä¿®æ­£è¾“å…¥ç»´åº¦é—®é¢˜"""
        self.model.eval()
        total_loss = 0
        total_note_accuracy = 0
        total_event_accuracy = 0
        num_batches = 0
        
        with torch.no_grad():
            for audio_seq, beatmap_target in val_loader:
                audio_seq = audio_seq.to(self.device)
                beatmap_target = beatmap_target.to(self.device)
                
                # ä¿®æ­£ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œä¸è®­ç»ƒä¿æŒä¸€è‡´
                note_pred, event_pred = self.model(audio_seq[:, -1, :])
                
                # è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡
                loss, loss_dict = self.compute_loss(note_pred, event_pred, beatmap_target)
                accuracy_dict = self.compute_accuracy(note_pred, event_pred, beatmap_target)
                
                total_loss += loss_dict['total']
                total_note_accuracy += accuracy_dict['note_accuracy']
                total_event_accuracy += accuracy_dict['event_accuracy']
                num_batches += 1
        
        return {
            'loss': total_loss / num_batches,
            'note_accuracy': total_note_accuracy / num_batches,
            'event_accuracy': total_event_accuracy / num_batches
        }
    
    def load_preprocessed_data(self):
        """åŠ è½½é¢„å¤„ç†æ•°æ®"""
        print("ğŸ“‚ åŠ è½½é¢„å¤„ç†æ•°æ®...")
        
        # åŠ è½½è°±é¢æ•°æ®
        beatmap_file = "preprocessed_data/all_4k_beatmaps.json"
        if not os.path.exists(beatmap_file):
            print("âŒ æœªæ‰¾åˆ°é¢„å¤„ç†è°±é¢æ•°æ®")
            return None, None
        
        with open(beatmap_file, 'r', encoding='utf-8') as f:
            beatmap_data = json.load(f)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(beatmap_data)} ä¸ªé¢„å¤„ç†è°±é¢")
        
        # æ¨¡æ‹ŸéŸ³é¢‘ç‰¹å¾ (åœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥æ˜¯çœŸå®çš„éŸ³é¢‘ç‰¹å¾)
        # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºåˆç†çš„éŸ³é¢‘ç‰¹å¾ä»¥è¿›è¡Œè®­ç»ƒæ¼”ç¤º
        all_audio_features = []
        all_beatmap_labels = []
        
        for i, beatmap in enumerate(beatmap_data):
            if i >= 100:  # é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ä»¥åŠ å¿«æ¼”ç¤º
                break
                
            # ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘ç‰¹å¾
            # 15ä¸ªç‰¹å¾: [èƒ½é‡, é¢‘è°±è´¨å¿ƒ, é›¶äº¤å‰ç‡, æ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°(12ä¸ª)]
            num_frames = len(beatmap.get('notes', []))
            if num_frames == 0:
                continue
                
            # ç¡®ä¿è‡³å°‘æœ‰åºåˆ—é•¿åº¦çš„å¸§æ•°
            if num_frames < self.sequence_length:
                continue
            
            # éŸ³é¢‘ç‰¹å¾çŸ©é˜µ [num_frames, 15]
            audio_features = np.random.randn(num_frames, 15) * 0.5 + 0.5
            
            # è°±é¢æ ‡ç­¾ [num_frames, 7] (4è½¨é“ + 3äº‹ä»¶ç±»å‹)
            beatmap_labels = np.zeros((num_frames, 7))
            
            # ä»è°±é¢æ•°æ®ä¸­æå–æ ‡ç­¾
            notes = beatmap.get('notes', [])
            for j, note in enumerate(notes):
                if j >= num_frames:
                    break
                
                # éŸ³ç¬¦ä¿¡æ¯ (4è½¨é“)
                if isinstance(note, dict):
                    track = note.get('track', 0)
                    if 0 <= track <= 3:
                        beatmap_labels[j, track] = 1
                    
                    # äº‹ä»¶ç±»å‹ (å¯ä»¥æ ¹æ®éŸ³ç¬¦å¯†åº¦ç­‰ä¿¡æ¯æ¨æ–­)
                    if j % 8 == 0:  # å¼ºæ‹
                        beatmap_labels[j, 4] = 1
                    elif j % 4 == 0:  # ä¸­æ‹
                        beatmap_labels[j, 5] = 1
                    else:  # å¼±æ‹
                        beatmap_labels[j, 6] = 1
            
            all_audio_features.append(audio_features)
            all_beatmap_labels.append(beatmap_labels)
            
            if i % 20 == 0:
                print(f"   ğŸ“ å¤„ç†è¿›åº¦: {i+1}/{min(100, len(beatmap_data))}")
        
        if not all_audio_features:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
            return None, None
        
        # åˆå¹¶æ•°æ®
        audio_features = np.vstack(all_audio_features)
        beatmap_labels = np.vstack(all_beatmap_labels)
        
        print(f"âœ… é¢„å¤„ç†æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   ğŸ“Š éŸ³é¢‘ç‰¹å¾: {audio_features.shape}")
        print(f"   ğŸ® è°±é¢æ ‡ç­¾: {beatmap_labels.shape}")
        
        return audio_features, beatmap_labels
    
    def create_weighted_fusion_model(self, input_dim: int = 15):
        """åˆ›å»ºæƒé‡èåˆæ¨¡å‹"""
        print("ğŸ—ï¸ åˆ›å»ºæƒé‡èåˆæ¨¡å‹...")
        
        self.model = WeightedFusionTransformer(
            input_dim=input_dim,
            d_model=128,          # è¾ƒå°çš„æ¨¡å‹ä»¥åŠ å¿«è®­ç»ƒ
            num_heads=4,
            num_layers=3,
            dropout=0.1,
            rf_weight=self.rf_weight,
            nn_weight=self.nn_weight,
            learnable_weights=True
        ).to(self.device)
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=0.01
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepochï¼Œå¢åŠ æƒé‡ç›‘æ§"""
        self.model.train()
        total_loss = 0
        total_note_accuracy = 0
        total_event_accuracy = 0
        num_batches = 0
        
        # æƒé‡è¿½è¸ª
        rf_weights = []
        nn_weights = []
        
        for batch_idx, (audio_seq, beatmap_target) in enumerate(train_loader):
            audio_seq = audio_seq.to(self.device)
            beatmap_target = beatmap_target.to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            note_pred, event_pred = self.model(audio_seq[:, -1, :])  # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            
            # è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡
            loss, loss_dict = self.compute_loss(note_pred, event_pred, beatmap_target)
            accuracy_dict = self.compute_accuracy(note_pred, event_pred, beatmap_target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # ç´¯è®¡ç»Ÿè®¡
            total_loss += loss_dict['total']
            total_note_accuracy += accuracy_dict['note_accuracy']
            total_event_accuracy += accuracy_dict['event_accuracy']
            num_batches += 1
            
            # è®°å½•æƒé‡å˜åŒ–
            if hasattr(self.model, 'get_weights'):
                weights_info = self.model.get_weights()
                rf_weights.append(weights_info['rf_weight'])
                nn_weights.append(weights_info['nn_weight'])
            
            # æ¯10ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡æƒé‡ä¿¡æ¯
            if batch_idx % 10 == 0 and len(rf_weights) > 0:
                current_rf = rf_weights[-1]
                current_nn = nn_weights[-1]
                print(f"    æ‰¹æ¬¡ {batch_idx}: Loss={loss_dict['total']:.4f}, "
                      f"RFæƒé‡={current_rf:.3f}, NNæƒé‡={current_nn:.3f}")
        
        # æ·»åŠ æƒé‡ä¿¡æ¯åˆ°å†å²è®°å½•
        if rf_weights:
            self.training_history.setdefault('rf_weights', []).append(np.mean(rf_weights))
            self.training_history.setdefault('nn_weights', []).append(np.mean(nn_weights))
        
        return {
            'loss': total_loss / num_batches,
            'note_accuracy': total_note_accuracy / num_batches,
            'event_accuracy': total_event_accuracy / num_batches,
            'avg_rf_weight': np.mean(rf_weights) if rf_weights else self.rf_weight,
            'avg_nn_weight': np.mean(nn_weights) if nn_weights else self.nn_weight
        }
    
    def analyze_weight_evolution(self):
        """åˆ†ææƒé‡æ¼”åŒ–"""
        if 'rf_weights' not in self.training_history:
            return
        
        rf_weights = self.training_history['rf_weights']
        nn_weights = self.training_history['nn_weights']
        
        print(f"\nâš–ï¸ æƒé‡æ¼”åŒ–åˆ†æ:")
        print(f"   åˆå§‹æƒé‡: RF={rf_weights[0]:.3f}, NN={nn_weights[0]:.3f}")
        print(f"   æœ€ç»ˆæƒé‡: RF={rf_weights[-1]:.3f}, NN={nn_weights[-1]:.3f}")
        print(f"   RFå˜åŒ–: {rf_weights[-1] - rf_weights[0]:+.3f}")
        print(f"   NNå˜åŒ–: {nn_weights[-1] - nn_weights[0]:+.3f}")

def weighted_fusion_quick_training():
    """æƒé‡èåˆå¿«é€Ÿè®­ç»ƒ"""
    print("ğŸ® æƒé‡èåˆå¿«é€Ÿè®­ç»ƒ (RF:NN = 3:7)")
    print("=" * 50)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    system = WeightedFusionQuickSystem(
        rf_weight=0.3,           # 30%
        nn_weight=0.7,           # 70%
        sequence_length=32,      # è¾ƒçŸ­åºåˆ—ä»¥åŠ å¿«è®­ç»ƒ
        batch_size=32,
        learning_rate=0.001
    )
    
    start_time = time.time()
    
    try:
        # é˜¶æ®µ1: åŠ è½½é¢„å¤„ç†æ•°æ®
        print("\nğŸ” é˜¶æ®µ1: åŠ è½½é¢„å¤„ç†æ•°æ®...")
        audio_features, beatmap_labels = system.load_preprocessed_data()
        
        if audio_features is None:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return None
        
        # é˜¶æ®µ2: åˆ›å»ºæƒé‡èåˆæ¨¡å‹
        print("\nğŸ—ï¸ é˜¶æ®µ2: åˆ›å»ºæƒé‡èåˆæ¨¡å‹...")
        system.create_weighted_fusion_model(input_dim=audio_features.shape[1])
        
        # é˜¶æ®µ3: å‡†å¤‡è®­ç»ƒæ•°æ®
        print("\nğŸ”§ é˜¶æ®µ3: å‡†å¤‡è®­ç»ƒæ•°æ®...")
        train_loader, val_loader = system.prepare_training_data(
            audio_features, beatmap_labels, train_ratio=0.8
        )
        
        # é˜¶æ®µ4: æƒé‡èåˆè®­ç»ƒ
        print("\nğŸš€ é˜¶æ®µ4: å¼€å§‹æƒé‡èåˆè®­ç»ƒ...")
        print("ğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"   ğŸ¤ æƒé‡æ¯”ä¾‹: RF={system.rf_weight:.1f} : NN={system.nn_weight:.1f}")
        print(f"   ğŸ“Š è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
        print(f"   ğŸ“Š éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
        
        # å¼€å§‹è®­ç»ƒ
        system.train(
            train_loader, val_loader,
            num_epochs=30,  # è¾ƒå°‘çš„è½®æ•°ä»¥å¿«é€Ÿæ¼”ç¤º
            save_path='quick_weighted_fusion_model_2_8.pth'
        )
        
        # é˜¶æ®µ5: æƒé‡æ¼”åŒ–åˆ†æ
        print("\nğŸ“Š é˜¶æ®µ5: æƒé‡æ¼”åŒ–åˆ†æ...")
        system.analyze_weight_evolution()
        
        # åˆ†æ”¯æ€§èƒ½åˆ†æ
        if hasattr(system.model, 'get_branch_predictions'):
            print("\nğŸ”¬ åˆ†æ”¯æ€§èƒ½åˆ†æ...")
            system.model.eval()
            sample_batch = next(iter(val_loader))
            with torch.no_grad():
                audio_seq = sample_batch[0].to(system.device)
                branch_results = system.model.get_branch_predictions(audio_seq[:, -1, :])
                
                print(f"   ğŸŒ² RFåˆ†æ”¯æƒé‡: {branch_results['rf_weight']:.3f}")
                print(f"   ğŸ§  NNåˆ†æ”¯æƒé‡: {branch_results['nn_weight']:.3f}")
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        system.plot_training_history()
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        results_data = {
            'training_config': {
                'rf_weight': system.rf_weight,
                'nn_weight': system.nn_weight,
                'sequence_length': system.sequence_length,
                'batch_size': system.batch_size,
                'learning_rate': system.learning_rate
            },
            'training_history': system.training_history,
            'final_performance': {
                'final_train_loss': system.training_history['train_loss'][-1],
                'final_val_loss': system.training_history['val_loss'][-1],
                'best_val_loss': min(system.training_history['val_loss'])
            }
        }
        
        with open('quick_weighted_fusion_results_2_8.json', 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        total_time = time.time() - start_time
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        
        print("\nğŸ‰ æƒé‡èåˆå¿«é€Ÿè®­ç»ƒå®Œæˆï¼")
        print("=" * 50)
        print("ğŸ“ˆ æœ€ç»ˆç»“æœ:")
        print(f"   â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {minutes:02d}:{seconds:02d}")
        print(f"   ğŸ¤ æƒé‡æ¯”ä¾‹: RF={system.rf_weight:.1f} : NN={system.nn_weight:.1f}")
        
        if hasattr(system.training_history, 'rf_weights') and system.training_history.get('rf_weights'):
            final_rf = system.training_history['rf_weights'][-1]
            final_nn = system.training_history['nn_weights'][-1]
            print(f"   âš–ï¸ å­¦ä¹ åæƒé‡: RF={final_rf:.3f} : NN={final_nn:.3f}")
        
        print(f"   ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {system.training_history['val_loss'][-1]:.4f}")
        print(f"   ğŸ¯ æœ€ä½³éªŒè¯æŸå¤±: {min(system.training_history['val_loss']):.4f}")
        
        print("\nğŸ’¾ ä¿å­˜æ–‡ä»¶:")
        print("   â€¢ quick_weighted_fusion_model_2_8.pth - æƒé‡èåˆæ¨¡å‹")
        print("   â€¢ quick_weighted_fusion_results_2_8.json - è®­ç»ƒå†å²")
        print("   â€¢ deep_learning_training_history.png - è®­ç»ƒå›¾è¡¨")
        
        return system
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    print("ğŸ”§ å‡†å¤‡æƒé‡èåˆå¿«é€Ÿè®­ç»ƒç¯å¢ƒ...")
    
    # å¼€å§‹æƒé‡èåˆå¿«é€Ÿè®­ç»ƒ
    trained_system = weighted_fusion_quick_training()
    
    if trained_system:
        print("\nğŸŠ æƒé‡èåˆå¿«é€Ÿè®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸš€ ç³»ç»Ÿç°åœ¨ä½¿ç”¨ RF:NN = 2:8 æƒé‡æ¯”ä¾‹è¿›è¡Œè°±é¢ç”Ÿæˆï¼")
        print("ğŸ“Š é€šè¿‡æƒé‡å­¦ä¹ ä¼˜åŒ–äº†è€å¸ˆå‚…å’Œå­¦ç”Ÿçš„è´¡çŒ®æ¯”ä¾‹")
    else:
        print("\nğŸ’¥ è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯")
