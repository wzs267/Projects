#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ··åˆæ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼šç»“åˆéšæœºæ£®æ—ç‰¹å¾å·¥ç¨‹ + Transformeråºåˆ—å»ºæ¨¡

æ ¸å¿ƒåˆ›æ–°ï¼š
1. ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§æŒ‡å¯¼ç‰¹å¾é€‰æ‹©
2. é›†æˆéšæœºæ£®æ—é¢„æµ‹ä½œä¸ºTransformerçš„é¢å¤–è¾“å…¥
3. å¤šå°ºåº¦ç‰¹å¾æå–ï¼šçŸ­æœŸï¼ˆå•å¸§ï¼‰+ é•¿æœŸï¼ˆåºåˆ—ï¼‰
4. ä¸“å®¶ç³»ç»Ÿèåˆï¼šä¼ ç»ŸML + æ·±åº¦å­¦ä¹ 
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Any, Tuple, Optional
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from models.beatmap_learning_system import BeatmapLearningSystem
import warnings
warnings.filterwarnings('ignore')

class EnhancedFeatureExtractor:
    """å¢å¼ºç‰¹å¾æå–å™¨ï¼šç»“åˆéŸ³é¢‘åˆ†æå’Œéšæœºæ£®æ—ç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        self.rf_system = BeatmapLearningSystem()
        self.feature_scaler = StandardScaler()
        self.rf_models_trained = False
    
    def train_rf_feature_extractors(self, training_data_path: str):
        """è®­ç»ƒéšæœºæ£®æ—ç‰¹å¾æå–å™¨"""
        print("ğŸŒ³ è®­ç»ƒéšæœºæ£®æ—ç‰¹å¾æå–å™¨...")
        
        # ä½¿ç”¨ç°æœ‰çš„è®­ç»ƒæ•°æ®
        aligned_datasets = self.rf_system.collect_training_data(
            training_data_path, 'extracted_audio'
        )
        
        if not aligned_datasets:
            print("âŒ æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
            return False
        
        # å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®
        X, y_note, y_column, y_long = self.rf_system.prepare_machine_learning_data(aligned_datasets)
        
        # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        self.rf_system.train_models(X, y_note, y_column, y_long)
        
        # æ‹Ÿåˆç‰¹å¾æ ‡å‡†åŒ–å™¨
        self.feature_scaler.fit(X)
        
        self.rf_models_trained = True
        print("âœ… éšæœºæ£®æ—ç‰¹å¾æå–å™¨è®­ç»ƒå®Œæˆ")
        return True
    
    def extract_enhanced_features(self, audio_features: np.ndarray, 
                                difficulty_params: Dict[str, float]) -> np.ndarray:
        """
        æå–å¢å¼ºç‰¹å¾ï¼šåŸå§‹éŸ³é¢‘ç‰¹å¾ + éšæœºæ£®æ—é¢„æµ‹ + ç‰¹å¾å·¥ç¨‹
        
        Args:
            audio_features: åŸå§‹15ç»´éŸ³é¢‘ç‰¹å¾ [N, 15]
            difficulty_params: éš¾åº¦å‚æ•°
            
        Returns:
            enhanced_features: å¢å¼ºç‰¹å¾ [N, enhanced_dim]
        """
        if not self.rf_models_trained:
            print("âš ï¸ éšæœºæ£®æ—æ¨¡å‹æœªè®­ç»ƒï¼Œè¿”å›åŸå§‹ç‰¹å¾")
            return audio_features
        
        # 1. æ ‡å‡†åŒ–åŸå§‹ç‰¹å¾
        normalized_features = self.feature_scaler.transform(audio_features)
        
        # 2. éšæœºæ£®æ—é¢„æµ‹ä½œä¸ºç‰¹å¾
        rf_note_probs = self.rf_system.note_placement_model.predict_proba(normalized_features)
        rf_note_features = rf_note_probs[:, 1:2]  # éŸ³ç¬¦æ”¾ç½®æ¦‚ç‡
        
        # åªå¯¹æœ‰éŸ³ç¬¦çš„ä½ç½®é¢„æµ‹è½¨é“
        rf_column_features = np.zeros((len(audio_features), 4))
        has_note_mask = rf_note_features.flatten() > 0.5
        
        if np.sum(has_note_mask) > 0:
            rf_column_probs = self.rf_system.column_selection_model.predict_proba(
                normalized_features[has_note_mask]
            )
            rf_column_features[has_note_mask] = rf_column_probs
        
        rf_long_probs = self.rf_system.long_note_model.predict_proba(normalized_features)
        rf_long_features = rf_long_probs[:, 1:2]  # é•¿æ¡éŸ³ç¬¦æ¦‚ç‡
        
        # 3. æ—¶åºç‰¹å¾å·¥ç¨‹
        temporal_features = self._extract_temporal_features(normalized_features)
        
        # 4. éŸ³ä¹ç†è®ºç‰¹å¾
        music_theory_features = self._extract_music_theory_features(normalized_features)
        
        # 5. ç»Ÿè®¡ç‰¹å¾
        statistical_features = self._extract_statistical_features(normalized_features)
        
        # 6. éš¾åº¦ç›¸å…³ç‰¹å¾
        difficulty_features = self._extract_difficulty_features(
            normalized_features, difficulty_params
        )
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        enhanced_features = np.concatenate([
            normalized_features,      # 15ç»´ï¼šåŸå§‹éŸ³é¢‘ç‰¹å¾
            rf_note_features,        # 1ç»´ï¼šRFéŸ³ç¬¦é¢„æµ‹
            rf_column_features,      # 4ç»´ï¼šRFè½¨é“é¢„æµ‹
            rf_long_features,        # 1ç»´ï¼šRFé•¿æ¡é¢„æµ‹
            temporal_features,       # 10ç»´ï¼šæ—¶åºç‰¹å¾
            music_theory_features,   # 8ç»´ï¼šéŸ³ä¹ç†è®ºç‰¹å¾
            statistical_features,    # 6ç»´ï¼šç»Ÿè®¡ç‰¹å¾
            difficulty_features      # 5ç»´ï¼šéš¾åº¦ç‰¹å¾
        ], axis=1)
        
        return enhanced_features
    
    def _extract_temporal_features(self, features: np.ndarray) -> np.ndarray:
        """æå–æ—¶åºç‰¹å¾"""
        N = len(features)
        temporal_features = np.zeros((N, 10))
        
        # ä½¿ç”¨RMSèƒ½é‡ï¼ˆç¬¬ä¸€ä¸ªç‰¹å¾ï¼‰è¿›è¡Œæ—¶åºåˆ†æ
        rms_energy = features[:, 0]
        
        for i in range(N):
            # å½“å‰å¸§çš„æ—¶åºç‰¹å¾
            window_size = min(10, i + 1, N - i)
            start_idx = max(0, i - window_size // 2)
            end_idx = min(N, i + window_size // 2 + 1)
            window_energy = rms_energy[start_idx:end_idx]
            
            # 1-3. èƒ½é‡ç»Ÿè®¡
            temporal_features[i, 0] = np.mean(window_energy)      # å±€éƒ¨å¹³å‡èƒ½é‡
            temporal_features[i, 1] = np.std(window_energy)       # å±€éƒ¨èƒ½é‡æ–¹å·®
            temporal_features[i, 2] = np.max(window_energy) - np.min(window_energy)  # èƒ½é‡èŒƒå›´
            
            # 4-5. èƒ½é‡å˜åŒ–
            if i > 0:
                temporal_features[i, 3] = rms_energy[i] - rms_energy[i-1]  # ä¸€é˜¶å·®åˆ†
            if i > 1:
                temporal_features[i, 4] = temporal_features[i, 3] - temporal_features[i-1, 3]  # äºŒé˜¶å·®åˆ†
            
            # 6-7. è¶‹åŠ¿ç‰¹å¾
            if len(window_energy) > 3:
                # çº¿æ€§å›å½’æ–œç‡
                x = np.arange(len(window_energy))
                slope = np.polyfit(x, window_energy, 1)[0]
                temporal_features[i, 5] = slope
                
                # èƒ½é‡å³°å€¼æ£€æµ‹
                peaks = (window_energy[1:-1] > window_energy[:-2]) & (window_energy[1:-1] > window_energy[2:])
                temporal_features[i, 6] = np.sum(peaks) / len(window_energy)
            
            # 8-10. èŠ‚æ‹ç›¸å…³
            beat_phase = (i * 0.05) % 1.0  # å‡è®¾1ç§’å‘¨æœŸ
            temporal_features[i, 7] = np.sin(2 * np.pi * beat_phase)     # èŠ‚æ‹ç›¸ä½sin
            temporal_features[i, 8] = np.cos(2 * np.pi * beat_phase)     # èŠ‚æ‹ç›¸ä½cos
            temporal_features[i, 9] = beat_phase                         # èŠ‚æ‹ç›¸ä½çº¿æ€§
        
        return temporal_features
    
    def _extract_music_theory_features(self, features: np.ndarray) -> np.ndarray:
        """æå–éŸ³ä¹ç†è®ºç‰¹å¾"""
        N = len(features)
        music_features = np.zeros((N, 8))
        
        # ä½¿ç”¨MFCCç‰¹å¾è¿›è¡ŒéŸ³ä¹åˆ†æ
        mfcc_features = features[:, 4:9]  # MFCC 1-5
        spectral_centroid = features[:, 6]  # é¢‘è°±è´¨å¿ƒ
        
        for i in range(N):
            # 1-2. å’Œå£°ç‰¹å¾
            if i >= 4:
                # å’Œå¼¦ç¨³å®šæ€§ï¼ˆMFCCå˜åŒ–å°è¡¨ç¤ºå’Œå¼¦ç¨³å®šï¼‰
                mfcc_window = mfcc_features[i-4:i+1]
                music_features[i, 0] = 1.0 / (1.0 + np.std(mfcc_window))
                
                # éŸ³è‰²ä¸€è‡´æ€§
                centroid_window = spectral_centroid[i-4:i+1]
                music_features[i, 1] = 1.0 / (1.0 + np.std(centroid_window))
            
            # 3-4. æ—‹å¾‹ç‰¹å¾
            if i > 0:
                # æ—‹å¾‹æ–¹å‘
                centroid_change = spectral_centroid[i] - spectral_centroid[i-1]
                music_features[i, 2] = np.tanh(centroid_change / 1000)  # æ ‡å‡†åŒ–
                
                # MFCC1å˜åŒ–ï¼ˆéŸ³è‰²å˜åŒ–ï¼‰
                mfcc1_change = mfcc_features[i, 0] - mfcc_features[i-1, 0]
                music_features[i, 3] = np.tanh(mfcc1_change)
            
            # 5-6. èŠ‚å¥ç‰¹å¾
            # å¼ºæ‹ä½ç½®ï¼ˆåŸºäºèƒ½é‡ï¼‰
            beat_position = (i * 0.05) % 1.0
            music_features[i, 4] = 1.0 if beat_position < 0.1 else 0.0  # å¼ºæ‹
            music_features[i, 5] = 1.0 if 0.4 < beat_position < 0.6 else 0.0  # å¼±æ‹
            
            # 7-8. å¤æ‚åº¦ç‰¹å¾
            # MFCCå¤æ‚åº¦ï¼ˆæ‰€æœ‰MFCCçš„æ ‡å‡†å·®ï¼‰
            music_features[i, 6] = np.std(mfcc_features[i])
            
            # é¢‘è°±å¤æ‚åº¦ï¼ˆåŸºäºé¢‘è°±è´¨å¿ƒçš„ç›¸å¯¹ä½ç½®ï¼‰
            music_features[i, 7] = spectral_centroid[i] / 4000.0  # æ ‡å‡†åŒ–åˆ°[0,1]
        
        return music_features
    
    def _extract_statistical_features(self, features: np.ndarray) -> np.ndarray:
        """æå–ç»Ÿè®¡ç‰¹å¾"""
        N = len(features)
        stat_features = np.zeros((N, 6))
        
        window_size = 20  # 1ç§’çª—å£
        
        for i in range(N):
            start_idx = max(0, i - window_size // 2)
            end_idx = min(N, i + window_size // 2 + 1)
            window_features = features[start_idx:end_idx]
            
            if len(window_features) > 1:
                # 1. èƒ½é‡ç™¾åˆ†ä½æ•°
                energies = window_features[:, 0]
                stat_features[i, 0] = np.percentile(energies, 75) - np.percentile(energies, 25)
                
                # 2. ç‰¹å¾ç›¸å…³æ€§ï¼ˆRMSä¸é¢‘è°±è´¨å¿ƒï¼‰
                if len(window_features) > 3:
                    correlation = np.corrcoef(window_features[:, 0], window_features[:, 6])[0, 1]
                    stat_features[i, 1] = correlation if not np.isnan(correlation) else 0.0
                
                # 3-4. å¤šç»´ç‰¹å¾åˆ†å¸ƒ
                # ä½¿ç”¨PCAçš„ç¬¬ä¸€ä¸»æˆåˆ†æ–¹å·®ï¼ˆå¤æ‚åº¦æŒ‡æ ‡ï¼‰
                centered_features = window_features - np.mean(window_features, axis=0)
                if centered_features.shape[0] > centered_features.shape[1]:
                    cov_matrix = np.cov(centered_features.T)
                    eigenvals = np.linalg.eigvals(cov_matrix)
                    stat_features[i, 2] = np.max(eigenvals) / np.sum(eigenvals)  # ä¸»æˆåˆ†è´¡çŒ®ç‡
                    stat_features[i, 3] = np.sum(eigenvals > 0.1)  # æœ‰æ•ˆç»´åº¦æ•°
                
                # 5-6. æ—¶åºç¨³å®šæ€§
                if len(window_features) > 5:
                    # ç‰¹å¾å˜åŒ–ç‡
                    feature_changes = np.diff(window_features, axis=0)
                    stat_features[i, 4] = np.mean(np.std(feature_changes, axis=0))
                    
                    # å‘¨æœŸæ€§æ£€æµ‹ï¼ˆè‡ªç›¸å…³ï¼‰
                    autocorr = np.correlate(energies, energies, mode='full')
                    autocorr = autocorr[autocorr.size // 2:]
                    if len(autocorr) > 2:
                        stat_features[i, 5] = np.max(autocorr[1:3]) / autocorr[0]
        
        return stat_features
    
    def _extract_difficulty_features(self, features: np.ndarray, 
                                   difficulty_params: Dict[str, float]) -> np.ndarray:
        """æå–éš¾åº¦ç›¸å…³ç‰¹å¾"""
        N = len(features)
        diff_features = np.zeros((N, 5))
        
        note_density = difficulty_params.get('note_density', 0.5)
        note_threshold = difficulty_params.get('note_threshold', 0.5)
        
        for i in range(N):
            # 1. éš¾åº¦å‚æ•°ç›´æ¥ç‰¹å¾
            diff_features[i, 0] = note_density
            diff_features[i, 1] = note_threshold
            
            # 2. èƒ½é‡ä¸éš¾åº¦çš„äº¤äº’ç‰¹å¾
            energy = features[i, 0]
            diff_features[i, 2] = energy * note_density  # èƒ½é‡-å¯†åº¦äº¤äº’
            
            # 3. å¤æ‚åº¦è°ƒæ•´
            complexity = np.std(features[i, 4:9])  # MFCCæ ‡å‡†å·®ä½œä¸ºå¤æ‚åº¦
            diff_features[i, 3] = complexity * (1 + note_density)
            
            # 4. è‡ªé€‚åº”é˜ˆå€¼
            if i >= 10:
                recent_energy = features[i-10:i, 0]
                adaptive_threshold = np.percentile(recent_energy, 70) * note_threshold
                diff_features[i, 4] = adaptive_threshold
            else:
                diff_features[i, 4] = note_threshold
        
        return diff_features


class HybridTransformerGenerator(nn.Module):
    """æ··åˆTransformerç”Ÿæˆå™¨ï¼šå¤šå°ºåº¦ç‰¹å¾ + ä¸“å®¶èåˆ"""
    
    def __init__(self, enhanced_input_dim: int = 50, d_model: int = 256,
                 num_heads: int = 8, num_layers: int = 6, dropout: float = 0.1):
        super().__init__()
        
        self.enhanced_input_dim = enhanced_input_dim
        self.d_model = d_model
        
        # å¤šè·¯å¾„ç‰¹å¾å¤„ç†
        self.audio_projection = nn.Linear(15, d_model // 4)      # åŸå§‹éŸ³é¢‘ç‰¹å¾
        self.rf_projection = nn.Linear(6, d_model // 4)          # RFé¢„æµ‹ç‰¹å¾  
        self.temporal_projection = nn.Linear(10, d_model // 4)   # æ—¶åºç‰¹å¾
        self.context_projection = nn.Linear(enhanced_input_dim - 31, d_model // 4)  # å…¶ä»–ç‰¹å¾
        
        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model)
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))
        
        # å¤šå°ºåº¦æ³¨æ„åŠ›
        self.short_attention = self._make_transformer_layer(d_model, num_heads, dropout)  # çŸ­æœŸæ¨¡å¼
        self.long_attention = self._make_transformer_layer(d_model, num_heads, dropout)   # é•¿æœŸæ¨¡å¼
        
        # ä¸»Transformerå±‚
        self.transformer_layers = nn.ModuleList([
            self._make_transformer_layer(d_model, num_heads, dropout)
            for _ in range(num_layers)
        ])
        
        # ä¸“å®¶ç³»ç»Ÿèåˆ
        self.expert_gate = nn.Sequential(
            nn.Linear(d_model, 3),
            nn.Softmax(dim=-1)
        )
        
        # ä¸“å®¶ç½‘ç»œ
        self.rf_expert = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64)
        )
        
        self.transformer_expert = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64)
        )
        
        self.fusion_expert = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64)
        )
        
        # è¾“å‡ºå¤´
        self.note_placement_head = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 4),
            nn.Sigmoid()
        )
        
        self.event_type_head = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 3),
            nn.Softmax(dim=-1)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def _make_transformer_layer(self, d_model: int, num_heads: int, dropout: float):
        """åˆ›å»ºTransformerå±‚"""
        return nn.ModuleDict({
            'attention': nn.MultiheadAttention(d_model, num_heads, dropout, batch_first=True),
            'norm1': nn.LayerNorm(d_model),
            'ff': nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(d_model * 4, d_model)
            ),
            'norm2': nn.LayerNorm(d_model),
            'dropout': nn.Dropout(dropout)
        })
    
    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, enhanced_input_dim]
        """
        batch_size, seq_len, _ = x.size()
        
        # åˆ†ç¦»ä¸åŒç±»å‹çš„ç‰¹å¾
        audio_features = x[:, :, :15]           # åŸå§‹éŸ³é¢‘ç‰¹å¾
        rf_features = x[:, :, 15:21]            # RFé¢„æµ‹ç‰¹å¾
        temporal_features = x[:, :, 21:31]      # æ—¶åºç‰¹å¾
        context_features = x[:, :, 31:]         # å…¶ä»–ç‰¹å¾
        
        # å¤šè·¯å¾„ç‰¹å¾æŠ•å½±
        audio_emb = self.audio_projection(audio_features)
        rf_emb = self.rf_projection(rf_features)
        temporal_emb = self.temporal_projection(temporal_features)
        context_emb = self.context_projection(context_features)
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([audio_emb, rf_emb, temporal_emb, context_emb], dim=-1)
        fused_features = self.feature_fusion(combined_features)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = fused_features + self.pos_encoding[:seq_len].unsqueeze(0)
        x = self.dropout(x)
        
        # å¤šå°ºåº¦æ³¨æ„åŠ›
        # çŸ­æœŸæ³¨æ„åŠ›ï¼ˆå±€éƒ¨æ¨¡å¼ï¼‰
        short_attn, _ = self.short_attention['attention'](x, x, x)
        x_short = self.short_attention['norm1'](x + self.short_attention['dropout'](short_attn))
        ff_short = self.short_attention['ff'](x_short)
        x_short = self.short_attention['norm2'](x_short + self.short_attention['dropout'](ff_short))
        
        # é•¿æœŸæ³¨æ„åŠ›ï¼ˆå…¨å±€æ¨¡å¼ï¼‰
        long_attn, _ = self.long_attention['attention'](x, x, x)
        x_long = self.long_attention['norm1'](x + self.long_attention['dropout'](long_attn))
        ff_long = self.long_attention['ff'](x_long)
        x_long = self.long_attention['norm2'](x_long + self.long_attention['dropout'](ff_long))
        
        # å¤šå°ºåº¦èåˆ
        x = (x_short + x_long) / 2
        
        # ä¸»Transformerå±‚
        for layer in self.transformer_layers:
            attn_out, _ = layer['attention'](x, x, x)
            x = layer['norm1'](x + layer['dropout'](attn_out))
            ff_out = layer['ff'](x)
            x = layer['norm2'](x + layer['dropout'](ff_out))
        
        # ä½¿ç”¨æœ€åæ—¶é—´æ­¥çš„è¾“å‡º
        final_hidden = x[:, -1, :]  # [batch_size, d_model]
        
        # ä¸“å®¶ç³»ç»Ÿ
        gate_weights = self.expert_gate(final_hidden)  # [batch_size, 3]
        
        rf_output = self.rf_expert(final_hidden)
        transformer_output = self.transformer_expert(final_hidden)
        fusion_output = self.fusion_expert(final_hidden)
        
        # ä¸“å®¶èåˆ
        expert_outputs = torch.stack([rf_output, transformer_output, fusion_output], dim=2)  # [batch_size, 64, 3]
        final_output = torch.sum(expert_outputs * gate_weights.unsqueeze(1), dim=2)  # [batch_size, 64]
        
        # æœ€ç»ˆé¢„æµ‹
        note_probs = self.note_placement_head(final_output)
        event_probs = self.event_type_head(final_output)
        
        return note_probs, event_probs, gate_weights


class HybridBeatmapLearningSystem:
    """æ··åˆæ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼šRFç‰¹å¾å·¥ç¨‹ + Transformeråºåˆ—å»ºæ¨¡"""
    
    def __init__(self, sequence_length: int = 64, batch_size: int = 32,
                 learning_rate: float = 0.0005, device: str = None):
        self.sequence_length = sequence_length
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ æ··åˆç³»ç»Ÿä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = EnhancedFeatureExtractor()
        
        # æ¨¡å‹
        self.model = None
        self.optimizer = None
        self.scheduler = None
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'note_accuracy': [],
            'event_accuracy': [],
            'expert_weights': []
        }
    
    def prepare_hybrid_training(self, traindata_dir: str, training_data_path: str):
        """å‡†å¤‡æ··åˆè®­ç»ƒï¼šå…ˆè®­ç»ƒRFï¼Œå†å‡†å¤‡æ·±åº¦å­¦ä¹ æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡æ··åˆæ·±åº¦å­¦ä¹ è®­ç»ƒ...")
        
        # 1. è®­ç»ƒéšæœºæ£®æ—ç‰¹å¾æå–å™¨
        success = self.feature_extractor.train_rf_feature_extractors(training_data_path)
        if not success:
            return None, None
        
        # 2. åŠ è½½å’Œå¢å¼ºç‰¹å¾
        print("ğŸ”§ åŠ è½½å¹¶å¢å¼ºç‰¹å¾...")
        # è¿™é‡Œéœ€è¦é‡æ–°å®ç°æ•°æ®åŠ è½½ï¼Œä½¿ç”¨å¢å¼ºç‰¹å¾
        # ä¸ºç®€åŒ–æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        
        return self._create_mock_enhanced_data()
    
    def _create_mock_enhanced_data(self):
        """åˆ›å»ºæ¨¡æ‹Ÿå¢å¼ºæ•°æ®ç”¨äºæ¼”ç¤º"""
        print("ğŸ² åˆ›å»ºæ¨¡æ‹Ÿå¢å¼ºæ•°æ®...")
        
        # æ¨¡æ‹Ÿå¢å¼ºç‰¹å¾ï¼š50ç»´
        num_samples = 5000
        enhanced_dim = 50
        
        enhanced_features = np.random.randn(num_samples, enhanced_dim).astype(np.float32)
        beatmap_labels = np.zeros((num_samples, 7), dtype=np.float32)
        
        # æ¨¡æ‹ŸçœŸå®çš„éŸ³ç¬¦æ¨¡å¼
        for i in range(0, num_samples, 15):
            if np.random.random() > 0.6:
                column = np.random.randint(0, 4)
                beatmap_labels[i, column] = 1.0
                event_type = np.random.choice([0, 1, 2], p=[0.8, 0.15, 0.05])
                beatmap_labels[i, 4 + event_type] = 1.0
        
        return enhanced_features, beatmap_labels
    
    def create_hybrid_model(self, enhanced_input_dim: int = 50):
        """åˆ›å»ºæ··åˆæ¨¡å‹"""
        print(f"ğŸ—ï¸ åˆ›å»ºæ··åˆTransformeræ¨¡å‹ (è¾“å…¥ç»´åº¦: {enhanced_input_dim})")
        
        self.model = HybridTransformerGenerator(
            enhanced_input_dim=enhanced_input_dim,
            d_model=256,
            num_heads=8,
            num_layers=6,
            dropout=0.1
        ).to(self.device)
        
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=0.01
        )
        
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )
        
        print(f"ğŸ“Š æ··åˆæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")


def demo_hybrid_system():
    """æ¼”ç¤ºæ··åˆç³»ç»Ÿ"""
    print("ğŸ® æ··åˆæ·±åº¦å­¦ä¹ ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ··åˆç³»ç»Ÿ
    hybrid_system = HybridBeatmapLearningSystem(
        sequence_length=32,
        batch_size=16,
        learning_rate=0.001
    )
    
    # å‡†å¤‡è®­ç»ƒï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
    print("ğŸ”§ å‡†å¤‡æ··åˆè®­ç»ƒæ•°æ®...")
    enhanced_features, beatmap_labels = hybrid_system._create_mock_enhanced_data()
    
    # åˆ›å»ºæ··åˆæ¨¡å‹
    hybrid_system.create_hybrid_model(enhanced_input_dim=enhanced_features.shape[1])
    
    print("âœ… æ··åˆæ·±åº¦å­¦ä¹ ç³»ç»Ÿåˆ›å»ºæˆåŠŸï¼")
    print(f"ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿:")
    print(f"   â€¢ ç»“åˆéšæœºæ£®æ—çš„ç‰¹å¾å·¥ç¨‹æ™ºæ…§")
    print(f"   â€¢ åˆ©ç”¨Transformerçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›") 
    print(f"   â€¢ å¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶")
    print(f"   â€¢ ä¸“å®¶ç³»ç»Ÿèåˆé¢„æµ‹")
    print(f"   â€¢ å¢å¼ºç‰¹å¾ç»´åº¦: {enhanced_features.shape[1]}ç»´")
    
    return hybrid_system


if __name__ == "__main__":
    hybrid_system = demo_hybrid_system()
