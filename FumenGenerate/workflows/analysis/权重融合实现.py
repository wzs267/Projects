#!/usr/bin/env python3
"""
æƒé‡èåˆç‰ˆæ··åˆæ¨¡å‹å®ç°

å°†åŸæ¥çš„ç‰¹å¾æ‹¼æ¥æ”¹ä¸ºçœŸæ­£çš„æƒé‡èåˆï¼š
- è€å¸ˆå‚…(éšæœºæ£®æ—) æƒé‡: Î± 
- å­¦ç”Ÿ(ç¥ç»ç½‘ç»œ) æƒé‡: Î²
- æœ€ç»ˆé¢„æµ‹ = Î± Ã— RFé¢„æµ‹ + Î² Ã— NNé¢„æµ‹

æ”¯æŒåŠ¨æ€è°ƒæ•´æƒé‡æ¯”ä¾‹ï¼Œå®ç°çœŸæ­£çš„å¸ˆå¾’åä½œè°ƒä¼˜
"""

import torch
import torch.nn as nn
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class WeightedFusionHybridModel(nn.Module):
    """æƒé‡èåˆç‰ˆæ··åˆæ¨¡å‹"""
    
    def __init__(self, audio_features_dim=15, rf_features_dim=15, 
                 rf_weight=0.4, nn_weight=0.6):
        super(WeightedFusionHybridModel, self).__init__()
        
        # æƒé‡èåˆå‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
        self.rf_weight = nn.Parameter(torch.tensor(rf_weight, dtype=torch.float32))
        self.nn_weight = nn.Parameter(torch.tensor(nn_weight, dtype=torch.float32))
        
        print(f"ğŸ¤ åˆ›å»ºæƒé‡èåˆæ··åˆæ¨¡å‹:")
        print(f"   ğŸŒ² åˆå§‹RFæƒé‡: {rf_weight}")
        print(f"   ğŸ§  åˆå§‹NNæƒé‡: {nn_weight}")
        
        # éšæœºæ£®æ—åˆ†æ”¯ - æ¨¡æ‹ŸRFçš„å†³ç­–è¿‡ç¨‹
        self.rf_branch = nn.Sequential(
            nn.Linear(rf_features_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()  # è¾“å‡º0-1æ¦‚ç‡
        )
        
        # ç¥ç»ç½‘ç»œåˆ†æ”¯ - æ·±åº¦ç‰¹å¾å­¦ä¹ 
        self.nn_branch = nn.Sequential(
            nn.Linear(audio_features_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(16, 1),
            nn.Sigmoid()  # è¾“å‡º0-1æ¦‚ç‡
        )
        
    def forward(self, audio_features, rf_features, return_individual=False):
        """
        æƒé‡èåˆå‰å‘ä¼ æ’­
        
        Args:
            audio_features: éŸ³é¢‘ç‰¹å¾ [batch_size, audio_features_dim]
            rf_features: RFç‰¹å¾ [batch_size, rf_features_dim]
            return_individual: æ˜¯å¦è¿”å›å„åˆ†æ”¯çš„å•ç‹¬é¢„æµ‹
            
        Returns:
            å¦‚æœreturn_individual=True: (fused_prob, rf_prob, nn_prob, weights)
            å¦åˆ™: fused_prob
        """
        # ç¡®ä¿æƒé‡å½’ä¸€åŒ–
        total_weight = self.rf_weight + self.nn_weight
        normalized_rf_weight = self.rf_weight / total_weight
        normalized_nn_weight = self.nn_weight / total_weight
        
        # ä¸¤ä¸ªåˆ†æ”¯åˆ†åˆ«è¾“å‡ºæ¦‚ç‡
        rf_prob = self.rf_branch(rf_features)      # [batch_size, 1]
        nn_prob = self.nn_branch(audio_features)   # [batch_size, 1]
        
        # æƒé‡èåˆ - è¿™æ˜¯å…³é”®ï¼
        fused_prob = normalized_rf_weight * rf_prob + normalized_nn_weight * nn_prob
        
        if return_individual:
            return fused_prob, rf_prob, nn_prob, (normalized_rf_weight, normalized_nn_weight)
        else:
            return fused_prob
    
    def set_fusion_weights(self, rf_weight: float, nn_weight: float):
        """æ‰‹åŠ¨è®¾ç½®èåˆæƒé‡"""
        with torch.no_grad():
            self.rf_weight.data = torch.tensor(rf_weight, dtype=torch.float32)
            self.nn_weight.data = torch.tensor(nn_weight, dtype=torch.float32)
        
        print(f"ğŸ”§ æ‰‹åŠ¨è®¾ç½®èåˆæƒé‡:")
        print(f"   ğŸŒ² RFæƒé‡: {rf_weight:.3f}")
        print(f"   ğŸ§  NNæƒé‡: {nn_weight:.3f}")
    
    def get_current_weights(self):
        """è·å–å½“å‰çš„èåˆæƒé‡"""
        total = self.rf_weight + self.nn_weight
        rf_w = self.rf_weight / total
        nn_w = self.nn_weight / total
        return rf_w.item(), nn_w.item()
    
    def freeze_weights(self, freeze=True):
        """å†»ç»“æˆ–è§£å†»æƒé‡å‚æ•°"""
        self.rf_weight.requires_grad = not freeze
        self.nn_weight.requires_grad = not freeze
        
        status = "å†»ç»“" if freeze else "è§£å†»"
        print(f"ğŸ”’ æƒé‡å‚æ•°å·²{status}")

class WeightFusionTrainer:
    """æƒé‡èåˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, learning_rate=0.001):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.BCELoss()
        self.training_history = {
            'loss': [],
            'accuracy': [],
            'rf_weights': [],
            'nn_weights': [],
            'rf_accuracy': [],
            'nn_accuracy': [],
            'fusion_accuracy': []
        }
    
    def train_epoch(self, train_loader, verbose=True):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_samples = 0
        correct_predictions = 0
        
        for batch_idx, (audio_features, rf_features, labels) in enumerate(train_loader):
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            fused_prob, rf_prob, nn_prob, weights = self.model(
                audio_features, rf_features, return_individual=True
            )
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(fused_prob.squeeze(), labels.float())
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            predictions = (fused_prob.squeeze() > 0.5).float()
            correct_predictions += (predictions == labels.float()).sum().item()
            total_samples += labels.size(0)
            
            if verbose and batch_idx % 10 == 0:
                rf_w, nn_w = weights
                print(f"   æ‰¹æ¬¡ {batch_idx}: Loss={loss.item():.4f}, "
                      f"RFæƒé‡={rf_w:.3f}, NNæƒé‡={nn_w:.3f}")
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct_predictions / total_samples
        
        return avg_loss, accuracy
    
    def evaluate(self, test_loader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_samples = 0
        fusion_correct = 0
        rf_correct = 0
        nn_correct = 0
        
        all_fusion_probs = []
        all_rf_probs = []
        all_nn_probs = []
        all_labels = []
        
        with torch.no_grad():
            for audio_features, rf_features, labels in test_loader:
                fused_prob, rf_prob, nn_prob, weights = self.model(
                    audio_features, rf_features, return_individual=True
                )
                
                # é¢„æµ‹
                fusion_pred = (fused_prob.squeeze() > 0.5).float()
                rf_pred = (rf_prob.squeeze() > 0.5).float()
                nn_pred = (nn_prob.squeeze() > 0.5).float()
                
                # ç»Ÿè®¡æ­£ç¡®ç‡
                fusion_correct += (fusion_pred == labels.float()).sum().item()
                rf_correct += (rf_pred == labels.float()).sum().item()
                nn_correct += (nn_pred == labels.float()).sum().item()
                total_samples += labels.size(0)
                
                # ä¿å­˜æ¦‚ç‡ç”¨äºåˆ†æ
                all_fusion_probs.extend(fused_prob.squeeze().cpu().numpy())
                all_rf_probs.extend(rf_prob.squeeze().cpu().numpy())
                all_nn_probs.extend(nn_prob.squeeze().cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        fusion_acc = fusion_correct / total_samples
        rf_acc = rf_correct / total_samples
        nn_acc = nn_correct / total_samples
        
        return {
            'fusion_accuracy': fusion_acc,
            'rf_accuracy': rf_acc,
            'nn_accuracy': nn_acc,
            'fusion_probs': all_fusion_probs,
            'rf_probs': all_rf_probs,
            'nn_probs': all_nn_probs,
            'labels': all_labels,
            'current_weights': self.model.get_current_weights()
        }
    
    def train(self, train_loader, test_loader, num_epochs=50):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        print(f"ğŸš€ å¼€å§‹æƒé‡èåˆæ¨¡å‹è®­ç»ƒ ({num_epochs} epochs)")
        print("=" * 60)
        
        best_accuracy = 0
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader, verbose=(epoch % 10 == 0))
            
            # è¯„ä¼°
            eval_results = self.evaluate(test_loader)
            
            # è®°å½•å†å²
            self.training_history['loss'].append(train_loss)
            self.training_history['accuracy'].append(train_acc)
            self.training_history['rf_weights'].append(eval_results['current_weights'][0])
            self.training_history['nn_weights'].append(eval_results['current_weights'][1])
            self.training_history['rf_accuracy'].append(eval_results['rf_accuracy'])
            self.training_history['nn_accuracy'].append(eval_results['nn_accuracy'])
            self.training_history['fusion_accuracy'].append(eval_results['fusion_accuracy'])
            
            # æ‰“å°è¿›åº¦
            rf_w, nn_w = eval_results['current_weights']
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"   èåˆå‡†ç¡®ç‡: {eval_results['fusion_accuracy']:.4f}")
            print(f"   RFå‡†ç¡®ç‡: {eval_results['rf_accuracy']:.4f}")
            print(f"   NNå‡†ç¡®ç‡: {eval_results['nn_accuracy']:.4f}")
            print(f"   å½“å‰æƒé‡: RF={rf_w:.3f}, NN={nn_w:.3f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_results['fusion_accuracy'] > best_accuracy:
                best_accuracy = eval_results['fusion_accuracy']
                torch.save(self.model.state_dict(), 'best_weighted_fusion_model.pth')
                print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {best_accuracy:.4f})")
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
        return eval_results

def create_mock_dataset(num_samples=5000):
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†ç”¨äºæ¼”ç¤º"""
    print(f"ğŸ² åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›† ({num_samples} æ ·æœ¬)")
    
    # æ¨¡æ‹ŸéŸ³é¢‘ç‰¹å¾ (15ç»´)
    audio_features = np.random.randn(num_samples, 15).astype(np.float32)
    
    # æ¨¡æ‹ŸRFç‰¹å¾ (15ç»´ï¼Œä¸éŸ³é¢‘ç‰¹å¾ç›¸å…³ä½†æœ‰å™ªéŸ³)
    rf_features = audio_features + 0.1 * np.random.randn(num_samples, 15)
    rf_features = rf_features.astype(np.float32)
    
    # åˆ›å»ºæœ‰æ„ä¹‰çš„æ ‡ç­¾ï¼ˆåŸºäºç‰¹å¾çš„æŸç§ç»„åˆï¼‰
    # æ¨¡æ‹ŸéŸ³ç¬¦æ£€æµ‹ï¼šRMSèƒ½é‡é«˜ + é¢‘è°±è´¨å¿ƒåˆé€‚ = æœ‰éŸ³ç¬¦
    rms_energy = audio_features[:, 0]  # å‡è®¾ç¬¬ä¸€ç»´æ˜¯RMSèƒ½é‡
    spectral_centroid = audio_features[:, 1]  # å‡è®¾ç¬¬äºŒç»´æ˜¯é¢‘è°±è´¨å¿ƒ
    
    # åˆ›å»ºæ ‡ç­¾ï¼šèƒ½é‡é«˜ä¸”è´¨å¿ƒåœ¨åˆç†èŒƒå›´å†…
    labels = ((rms_energy > 0.5) & (np.abs(spectral_centroid) < 1.0)).astype(np.float32)
    
    # æ·»åŠ ä¸€äº›å™ªéŸ³è®©é—®é¢˜æ›´çœŸå®
    noise_indices = np.random.choice(num_samples, size=int(0.1 * num_samples), replace=False)
    labels[noise_indices] = 1 - labels[noise_indices]
    
    print(f"   âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ")
    print(f"   ğŸ“Š æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(labels):.3f}")
    
    return audio_features, rf_features, labels

def create_data_loader(audio_features, rf_features, labels, batch_size=32, shuffle=True):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(audio_features),
        torch.FloatTensor(rf_features), 
        torch.FloatTensor(labels)
    )
    
    return torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle
    )

def plot_training_results(trainer):
    """ç»˜åˆ¶è®­ç»ƒç»“æœ"""
    history = trainer.training_history
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('æƒé‡èåˆæ¨¡å‹è®­ç»ƒç»“æœ', fontsize=16)
    
    # 1. æŸå¤±å’Œå‡†ç¡®ç‡
    axes[0, 0].plot(history['loss'], label='è®­ç»ƒæŸå¤±', color='red')
    ax_twin = axes[0, 0].twinx()
    ax_twin.plot(history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('æŸå¤±', color='red')
    ax_twin.set_ylabel('å‡†ç¡®ç‡', color='blue')
    axes[0, 0].set_title('è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡')
    
    # 2. æƒé‡å˜åŒ–
    axes[0, 1].plot(history['rf_weights'], label='RFæƒé‡', marker='o')
    axes[0, 1].plot(history['nn_weights'], label='NNæƒé‡', marker='s')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('æƒé‡å€¼')
    axes[0, 1].set_title('èåˆæƒé‡åŠ¨æ€å˜åŒ–')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # 3. å„åˆ†æ”¯å‡†ç¡®ç‡å¯¹æ¯”
    axes[1, 0].plot(history['fusion_accuracy'], label='èåˆæ¨¡å‹', linewidth=2)
    axes[1, 0].plot(history['rf_accuracy'], label='RFåˆ†æ”¯', linestyle='--')
    axes[1, 0].plot(history['nn_accuracy'], label='NNåˆ†æ”¯', linestyle=':')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('å‡†ç¡®ç‡')
    axes[1, 0].set_title('å„æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # 4. æœ€ç»ˆæƒé‡åˆ†å¸ƒ
    final_rf_weight = history['rf_weights'][-1]
    final_nn_weight = history['nn_weights'][-1]
    
    weights = [final_rf_weight, final_nn_weight]
    labels_pie = [f'éšæœºæ£®æ—\n{final_rf_weight:.3f}', f'ç¥ç»ç½‘ç»œ\n{final_nn_weight:.3f}']
    colors = ['lightblue', 'lightcoral']
    
    axes[1, 1].pie(weights, labels=labels_pie, colors=colors, autopct='%1.1f%%', startangle=90)
    axes[1, 1].set_title('æœ€ç»ˆæƒé‡åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.savefig('weighted_fusion_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()

def weight_sensitivity_analysis(model, test_loader):
    """æƒé‡æ•æ„Ÿæ€§åˆ†æ"""
    print("\nğŸ”¬ æƒé‡æ•æ„Ÿæ€§åˆ†æ")
    print("=" * 40)
    
    # æµ‹è¯•ä¸åŒæƒé‡ç»„åˆ
    weight_combinations = [
        (0.1, 0.9), (0.2, 0.8), (0.3, 0.7), (0.4, 0.6), (0.5, 0.5),
        (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1)
    ]
    
    results = []
    
    for rf_w, nn_w in weight_combinations:
        model.set_fusion_weights(rf_w, nn_w)
        
        # è¯„ä¼°
        model.eval()
        total_correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for audio_features, rf_features, labels in test_loader:
                fused_prob = model(audio_features, rf_features)
                predictions = (fused_prob.squeeze() > 0.5).float()
                total_correct += (predictions == labels.float()).sum().item()
                total_samples += labels.size(0)
        
        accuracy = total_correct / total_samples
        results.append((rf_w, nn_w, accuracy))
        
        print(f"RFæƒé‡: {rf_w:.1f}, NNæƒé‡: {nn_w:.1f} â†’ å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # æ‰¾å‡ºæœ€ä½³æƒé‡ç»„åˆ
    best_result = max(results, key=lambda x: x[2])
    print(f"\nğŸ† æœ€ä½³æƒé‡ç»„åˆ:")
    print(f"   RFæƒé‡: {best_result[0]:.1f}")
    print(f"   NNæƒé‡: {best_result[1]:.1f}")
    print(f"   å‡†ç¡®ç‡: {best_result[2]:.4f}")
    
    return results, best_result

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ® æƒé‡èåˆæ··åˆæ¨¡å‹å®Œæ•´å®ç°")
    print("=" * 60)
    
    # 1. åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    audio_features, rf_features, labels = create_mock_dataset(5000)
    
    # 2. æ•°æ®åˆ†å‰²
    X_audio_train, X_audio_test, X_rf_train, X_rf_test, y_train, y_test = train_test_split(
        audio_features, rf_features, labels, test_size=0.2, random_state=42
    )
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_data_loader(X_audio_train, X_rf_train, y_train, batch_size=64)
    test_loader = create_data_loader(X_audio_test, X_rf_test, y_test, batch_size=64, shuffle=False)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   è®­ç»ƒé›†: {len(X_audio_train)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(X_audio_test)} æ ·æœ¬")
    
    # 4. åˆ›å»ºæƒé‡èåˆæ¨¡å‹
    print(f"\nğŸ—ï¸ åˆ›å»ºæƒé‡èåˆæ¨¡å‹...")
    model = WeightedFusionHybridModel(
        audio_features_dim=15,
        rf_features_dim=15,
        rf_weight=0.4,  # åˆå§‹è€å¸ˆå‚…æƒé‡
        nn_weight=0.6   # åˆå§‹å­¦ç”Ÿæƒé‡
    )
    
    # 5. åˆ›å»ºè®­ç»ƒå™¨
    trainer = WeightFusionTrainer(model, learning_rate=0.001)
    
    # 6. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    final_results = trainer.train(train_loader, test_loader, num_epochs=30)
    
    # 7. ç»˜åˆ¶è®­ç»ƒç»“æœ
    print(f"\nğŸ“Š ç»˜åˆ¶è®­ç»ƒç»“æœ...")
    plot_training_results(trainer)
    
    # 8. æƒé‡æ•æ„Ÿæ€§åˆ†æ
    sensitivity_results, best_weights = weight_sensitivity_analysis(model, test_loader)
    
    # 9. æ€»ç»“
    print(f"\nğŸ‰ æƒé‡èåˆå®ç°å®Œæˆï¼")
    print("=" * 40)
    print(f"ğŸ“ˆ æœ€ç»ˆç»“æœ:")
    print(f"   ğŸ¤ èåˆæ¨¡å‹å‡†ç¡®ç‡: {final_results['fusion_accuracy']:.4f}")
    print(f"   ğŸŒ² RFåˆ†æ”¯å‡†ç¡®ç‡: {final_results['rf_accuracy']:.4f}")
    print(f"   ğŸ§  NNåˆ†æ”¯å‡†ç¡®ç‡: {final_results['nn_accuracy']:.4f}")
    
    rf_w, nn_w = final_results['current_weights']
    print(f"   âš–ï¸ å­¦ä¹ åˆ°çš„æƒé‡: RF={rf_w:.3f}, NN={nn_w:.3f}")
    print(f"   ğŸ† æœ€ä¼˜æƒé‡ç»„åˆ: RF={best_weights[0]:.1f}, NN={best_weights[1]:.1f}")
    
    print(f"\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:")
    print(f"   â€¢ best_weighted_fusion_model.pth - æœ€ä½³æ¨¡å‹")
    print(f"   â€¢ weighted_fusion_training_results.png - è®­ç»ƒå›¾è¡¨")
    
    return model, trainer, final_results

if __name__ == "__main__":
    model, trainer, results = main()
