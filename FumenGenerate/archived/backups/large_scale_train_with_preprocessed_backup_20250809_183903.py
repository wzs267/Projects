#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºé¢„å¤„ç†æ•°æ®çš„å¤§è§„æ¨¡è®­ç»ƒè„šæœ¬
ä½¿ç”¨ä¸å°æ‰¹é‡è®­ç»ƒç›¸åŒçš„æˆåŠŸæ¨¡å¼
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torch.nn.functional as F
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def load_preprocessed_data():
    """åŠ è½½é¢„å¤„ç†çš„è®­ç»ƒæ•°æ®"""
    try:
        from models.beatmap_learning_system import BeatmapLearningSystem
        
        system = BeatmapLearningSystem()
        system.traindata_dir = "preprocessed_data"
        
        # åŠ è½½JSONæ•°æ®
        json_file = "preprocessed_data/all_4k_beatmaps.json"
        
        if not os.path.exists(json_file):
            print(f"âŒ æ‰¾ä¸åˆ°é¢„å¤„ç†æ•°æ®æ–‡ä»¶: {json_file}")
            return None, None
        
        with open(json_file, 'r', encoding='utf-8') as f:
            beatmaps_data = json.load(f)
        
        print(f"ğŸ“ åŠ è½½äº† {len(beatmaps_data)} ä¸ªé¢„å¤„ç†è°±é¢")
        
        # ä½¿ç”¨ç³»ç»Ÿçš„å¤„ç†æ–¹æ³•ï¼ˆä¸å°æ‰¹é‡è®­ç»ƒç›¸åŒï¼‰
        collected_data = []
        
        # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ‰€æœ‰è°±é¢
        with tqdm(beatmaps_data, desc="ğŸµ å¤„ç†è°±é¢", unit="è°±é¢") as pbar:
            for i, beatmap_data in enumerate(pbar):
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                title_short = beatmap_data['title'][:15] + "..." if len(beatmap_data['title']) > 15 else beatmap_data['title']
                pbar.set_postfix({
                    "å½“å‰": title_short,
                    "æˆåŠŸ": len(collected_data)
                })
                
                # æ„å»ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„
                audio_file = os.path.join("preprocessed_data/audio", beatmap_data['audio_file'])
                
                if not os.path.exists(audio_file):
                    continue
                
                # å­—æ®µæ˜ å°„ï¼šå°†é¢„å¤„ç†æ•°æ®çš„å­—æ®µåæ˜ å°„åˆ°BeatmapLearningSystemæœŸæœ›çš„å­—æ®µå
                mapped_beatmap_data = {
                    'song_title': beatmap_data.get('title', 'Unknown'),
                    'difficulty_version': beatmap_data.get('difficulty_name', 'Unknown'),
                    'notes': beatmap_data.get('notes', []),
                    'timing_points': beatmap_data.get('timing_points', []),
                    'note_count': beatmap_data.get('note_count', 0),
                    'note_density': beatmap_data.get('note_density', 0),
                    'long_notes_ratio': beatmap_data.get('long_notes_ratio', 0),
                    'avg_bpm': beatmap_data.get('avg_bpm', 120),
                    'duration': beatmap_data.get('duration', 0),
                    'initial_bpm': beatmap_data.get('initial_bpm', 120)
                }
                
                # ä½¿ç”¨ç³»ç»Ÿçš„å¤„ç†æ–¹æ³•
                mcz_name = beatmap_data['source_mcz']
                data = system.process_single_beatmap(mcz_name, mapped_beatmap_data, audio_file)
                
                if data:
                    collected_data.append(data)
            else:
                print(f"âš ï¸ å¤„ç†å¤±è´¥: {beatmap_data['title']}")
        
        print(f"\næ”¶é›†åˆ° {len(collected_data)} ä¸ªæœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬")
        
        if not collected_data:
            return None, None
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
        all_features = []
        all_labels = []
        
        for data in collected_data:
            # AlignedDataå¯¹è±¡ï¼Œä½¿ç”¨å±æ€§è®¿é—®è€Œä¸æ˜¯å­—å…¸è®¿é—®
            features = data.audio_features  # éŸ³é¢‘ç‰¹å¾çŸ©é˜µ
            events = data.beatmap_events     # è°±é¢äº‹ä»¶çŸ©é˜µ
            
            # æ„å»ºæ ‡ç­¾ï¼šå°†å¤šåˆ—çš„äº‹ä»¶çŸ©é˜µè½¬æ¢ä¸ºå•åˆ—çš„noteæ ‡ç­¾
            # å‡è®¾eventsçŸ©é˜µçš„æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªé”®é“
            # æ ‡ç­¾ä¸º1è¡¨ç¤ºè¯¥æ—¶é—´æ­¥æœ‰éŸ³ç¬¦ï¼Œ0è¡¨ç¤ºæ²¡æœ‰
            labels = np.any(events > 0, axis=1).astype(int)
            
            all_features.append(features)
            all_labels.append(labels)
        
        # æ‹¼æ¥æ‰€æœ‰æ•°æ®
        combined_features = np.vstack(all_features)
        combined_labels = np.hstack(all_labels)
        
        print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ: {combined_features.shape[0]:,} æ ·æœ¬")
        return combined_features, combined_labels
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# æ··åˆæ¨¡å‹å®šä¹‰ï¼ˆä¸working_train.pyç›¸åŒï¼‰
class HybridNeuralNetwork(nn.Module):
    """æ··åˆç¥ç»ç½‘ç»œï¼šç»“åˆéšæœºæ£®æ—ç‰¹å¾å’ŒéŸ³é¢‘ç‰¹å¾"""
    
    def __init__(self, audio_features_dim=15, rf_features_dim=15):
        super(HybridNeuralNetwork, self).__init__()
        
        # éŸ³é¢‘ç‰¹å¾åˆ†æ”¯
        self.audio_branch = nn.Sequential(
            nn.Linear(audio_features_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # éšæœºæ£®æ—ç‰¹å¾åˆ†æ”¯
        self.rf_branch = nn.Sequential(
            nn.Linear(rf_features_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(32 + 16, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, audio_features, rf_features):
        audio_out = self.audio_branch(audio_features)
        rf_out = self.rf_branch(rf_features)
        
        combined = torch.cat([audio_out, rf_out], dim=1)
        output = self.fusion(combined)
        
        return output

def train_large_scale_model(features, labels):
    """è®­ç»ƒå¤§è§„æ¨¡æ··åˆæ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹å¤§è§„æ¨¡æ··åˆæ¨¡å‹è®­ç»ƒ...")
    print(f"   ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(features):,}")
    print(f"   ğŸ¯ æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(labels):.3f}")
    
    # æ•°æ®åˆ’åˆ†
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=0.2, random_state=42
    )
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # é˜¶æ®µ1: è®­ç»ƒéšæœºæ£®æ—
    print(f"\nğŸŒ² é˜¶æ®µ1: è®­ç»ƒéšæœºæ£®æ—...")
    rf_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        random_state=42,
        n_jobs=-1
    )
    
    rf_model.fit(X_train_scaled, y_train)
    rf_accuracy = rf_model.score(X_test_scaled, y_test)
    print(f"   ğŸŒ² éšæœºæ£®æ—å‡†ç¡®ç‡: {rf_accuracy:.3f}")
    
    # è·å–éšæœºæ£®æ—é¢„æµ‹æ¦‚ç‡ä½œä¸ºç‰¹å¾
    rf_train_probs = rf_model.predict_proba(X_train_scaled)[:, 1].reshape(-1, 1)
    rf_test_probs = rf_model.predict_proba(X_test_scaled)[:, 1].reshape(-1, 1)
    
    # ç‰¹å¾é‡è¦æ€§å‰15ä¸ªç‰¹å¾
    feature_importance = rf_model.feature_importances_
    top_indices = np.argsort(feature_importance)[-15:]
    
    X_train_top = X_train_scaled[:, top_indices]
    X_test_top = X_test_scaled[:, top_indices]
    
    # æ„å»ºå¢å¼ºç‰¹å¾
    rf_enhanced_train = np.hstack([
        X_train_top,
        rf_train_probs
    ])
    rf_enhanced_test = np.hstack([
        X_test_top,
        rf_test_probs
    ])
    
    # é˜¶æ®µ2: è®­ç»ƒæ··åˆç¥ç»ç½‘ç»œ
    print(f"\nğŸ§  é˜¶æ®µ2: è®­ç»ƒæ··åˆç¥ç»ç½‘ç»œ...")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_audio_train = torch.FloatTensor(X_train_scaled)
    X_rf_train = torch.FloatTensor(rf_enhanced_train)
    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)
    
    X_audio_test = torch.FloatTensor(X_test_scaled)
    X_rf_test = torch.FloatTensor(rf_enhanced_test)
    y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)
    
    # åˆ›å»ºæ¨¡å‹
    model = HybridNeuralNetwork(
        audio_features_dim=X_train_scaled.shape[1],
        rf_features_dim=rf_enhanced_train.shape[1]
    )
    
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒå¾ªç¯
    epochs = 20
    batch_size = 512
    
    # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
    with tqdm(range(epochs), desc="ğŸ§  è®­ç»ƒç¥ç»ç½‘ç»œ", unit="epoch") as epoch_bar:
        for epoch in epoch_bar:
            model.train()
            total_loss = 0
            
            # æ‰¹é‡è®­ç»ƒ
            num_batches = (len(X_audio_train) + batch_size - 1) // batch_size
            for i in range(0, len(X_audio_train), batch_size):
                batch_audio = X_audio_train[i:i+batch_size]
                batch_rf = X_rf_train[i:i+batch_size]
                batch_y = y_train_tensor[i:i+batch_size]
                
                optimizer.zero_grad()
                outputs = model(batch_audio, batch_rf)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            # è®¡ç®—éªŒè¯å‡†ç¡®ç‡
            model.eval()
            with torch.no_grad():
                test_outputs = model(X_audio_test, X_rf_test)
                test_predictions = (test_outputs > 0.5).float()
                accuracy = (test_predictions == y_test_tensor).float().mean().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            epoch_bar.set_postfix({
                "æŸå¤±": f"{total_loss/num_batches:.4f}",
                "å‡†ç¡®ç‡": f"{accuracy:.3f}"
            })
    
    # æœ€ç»ˆè¯„ä¼°
    model.eval()
    with torch.no_grad():
        final_outputs = model(X_audio_test, X_rf_test)
        final_predictions = (final_outputs > 0.5).float()
        final_accuracy = (final_predictions == y_test_tensor).float().mean().item()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸŒ² éšæœºæ£®æ—å‡†ç¡®ç‡: {rf_accuracy:.3f}")
    print(f"ğŸ§  æ··åˆæ¨¡å‹å‡†ç¡®ç‡: {final_accuracy:.3f}")
    print(f"ğŸ“ˆ æå‡: {(final_accuracy - rf_accuracy):.3f}")
    
    # ä¿å­˜æ¨¡å‹
    torch.save({
        'model_state_dict': model.state_dict(),
        'scaler': scaler,
        'rf_model': rf_model,
        'top_indices': top_indices
    }, 'models/large_scale_hybrid_model.pth')
    
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° models/large_scale_hybrid_model.pth")
    
    return final_accuracy

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ® å¤§è§„æ¨¡é¢„å¤„ç†æ•°æ®è®­ç»ƒ")
    print("=" * 50)
    print("ğŸ“ ä½¿ç”¨ä¸å°æ‰¹é‡è®­ç»ƒç›¸åŒçš„æˆåŠŸæ¨¡å¼")
    print()
    
    # åŠ è½½é¢„å¤„ç†æ•°æ®
    print("ğŸ“¥ åŠ è½½é¢„å¤„ç†è®­ç»ƒæ•°æ®...")
    features, labels = load_preprocessed_data()
    
    if features is None or labels is None:
        print("âŒ æ— æ³•è·å–è®­ç»ƒæ•°æ®")
        return
    
    # è®­ç»ƒæ¨¡å‹
    accuracy = train_large_scale_model(features, labels)
    
    if accuracy > 0.90:
        print(f"\nğŸ† æ­å–œï¼è¾¾åˆ°90%ä»¥ä¸Šå‡†ç¡®ç‡ï¼")
    elif accuracy > 0.85:
        print(f"\nğŸ¯ å¾ˆå¥½ï¼è¾¾åˆ°85%ä»¥ä¸Šå‡†ç¡®ç‡ï¼")
    else:
        print(f"\nğŸ’¡ å‡†ç¡®ç‡ä¸º{accuracy:.1%}ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–")

if __name__ == "__main__":
    main()
