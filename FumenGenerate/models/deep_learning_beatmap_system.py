#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºæ·±åº¦å­¦ä¹ çš„éŸ³æ¸¸è°±é¢ç”Ÿæˆç³»ç»Ÿ

ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†(700+ MCZæ–‡ä»¶)è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒ
å®ç°åºåˆ—åˆ°åºåˆ—çš„éŸ³é¢‘-è°±é¢æ˜ å°„
"""

import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from typing import List, Dict, Any, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class BeatmapDataset(Dataset):
    """è°±é¢æ•°æ®é›†ç±»"""
    
    def __init__(self, audio_features: np.ndarray, beatmap_labels: np.ndarray, 
                 sequence_length: int = 64):
        """
        Args:
            audio_features: éŸ³é¢‘ç‰¹å¾åºåˆ— [N, feature_dim]
            beatmap_labels: è°±é¢æ ‡ç­¾ [N, 4+3] (4è½¨é“+3äº‹ä»¶ç±»å‹)
            sequence_length: åºåˆ—é•¿åº¦ï¼ˆæ—¶é—´æ­¥æ•°ï¼‰
        """
        self.sequence_length = sequence_length
        self.audio_features = torch.FloatTensor(audio_features)
        self.beatmap_labels = torch.FloatTensor(beatmap_labels)
        
        # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿåˆ›å»ºåºåˆ—
        self.valid_indices = list(range(sequence_length, len(audio_features)))
        
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        
        # æå–åºåˆ—
        start_idx = actual_idx - self.sequence_length
        end_idx = actual_idx
        
        audio_seq = self.audio_features[start_idx:end_idx]  # [seq_len, feature_dim]
        beatmap_target = self.beatmap_labels[actual_idx]    # [7] (4è½¨é“+3äº‹ä»¶ç±»å‹)
        
        return audio_seq, beatmap_target


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        
        # çº¿æ€§å˜æ¢
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åŠ æƒæ±‚å’Œ
        context = torch.matmul(attention_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        return self.W_o(context)


class TransformerBeatmapGenerator(nn.Module):
    """åŸºäºTransformerçš„è°±é¢ç”Ÿæˆå™¨"""
    
    def __init__(self, input_dim: int = 15, d_model: int = 256, 
                 num_heads: int = 8, num_layers: int = 6, 
                 dropout: float = 0.1):
        super().__init__()
        
        self.input_dim = input_dim
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))
        
        # Transformerå±‚
        self.transformer_layers = nn.ModuleList([
            nn.ModuleDict({
                'attention': MultiHeadAttention(d_model, num_heads, dropout),
                'norm1': nn.LayerNorm(d_model),
                'ff': nn.Sequential(
                    nn.Linear(d_model, d_model * 4),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(d_model * 4, d_model)
                ),
                'norm2': nn.LayerNorm(d_model),
                'dropout': nn.Dropout(dropout)
            })
            for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå¤´
        self.note_placement_head = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 4),  # 4ä¸ªè½¨é“çš„éŸ³ç¬¦æ”¾ç½®æ¦‚ç‡
            nn.Sigmoid()
        )
        
        self.event_type_head = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3),  # note, long_start, long_end
            nn.Softmax(dim=-1)
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, input_dim]
        Returns:
            note_probs: [batch_size, 4] - å„è½¨é“éŸ³ç¬¦æ¦‚ç‡
            event_probs: [batch_size, 3] - äº‹ä»¶ç±»å‹æ¦‚ç‡
        """
        batch_size, seq_len, _ = x.size()
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch_size, seq_len, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:seq_len].unsqueeze(0)
        x = self.dropout(x)
        
        # Transformerå±‚
        for layer in self.transformer_layers:
            # å¤šå¤´æ³¨æ„åŠ›
            attn_out = layer['attention'](x)
            x = layer['norm1'](x + layer['dropout'](attn_out))
            
            # å‰é¦ˆç½‘ç»œ
            ff_out = layer['ff'](x)
            x = layer['norm2'](x + layer['dropout'](ff_out))
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        final_hidden = x[:, -1, :]  # [batch_size, d_model]
        
        # è¾“å‡ºé¢„æµ‹
        note_probs = self.note_placement_head(final_hidden)     # [batch_size, 4]
        event_probs = self.event_type_head(final_hidden)        # [batch_size, 3]
        
        return note_probs, event_probs


class DeepBeatmapLearningSystem:
    """æ·±åº¦å­¦ä¹ è°±é¢ç”Ÿæˆç³»ç»Ÿ"""
    
    def __init__(self, sequence_length: int = 64, batch_size: int = 64, 
                 learning_rate: float = 0.001, device: str = None):
        """
        Args:
            sequence_length: è¾“å…¥åºåˆ—é•¿åº¦
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            device: è®¡ç®—è®¾å¤‡
        """
        self.sequence_length = sequence_length
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        # è®¾å¤‡é€‰æ‹©
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self.model = None
        self.optimizer = None
        self.scaler = StandardScaler()
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'note_accuracy': [],
            'event_accuracy': []
        }
    
    def create_model(self, input_dim: int = 15):
        """åˆ›å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        self.model = TransformerBeatmapGenerator(
            input_dim=input_dim,
            d_model=256,
            num_heads=8,
            num_layers=6,
            dropout=0.1
        ).to(self.device)
        
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=0.01
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def load_large_dataset(self, traindata_dir: str) -> Tuple[np.ndarray, np.ndarray]:
        """
        åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆ700+ MCZæ–‡ä»¶ï¼‰
        
        Args:
            traindata_dir: è®­ç»ƒæ•°æ®ç›®å½•
            
        Returns:
            (audio_features, beatmap_labels): éŸ³é¢‘ç‰¹å¾å’Œè°±é¢æ ‡ç­¾
        """
        print("ğŸ” æ‰«æå¤§è§„æ¨¡æ•°æ®é›†...")
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from core.mcz_parser import MCZParser
        from core.four_k_extractor import FourKBeatmapExtractor
        from core.audio_beatmap_analyzer import AudioBeatmapAnalyzer
        
        parser = MCZParser()
        extractor = FourKBeatmapExtractor()
        analyzer = AudioBeatmapAnalyzer(time_resolution=0.05)
        
        # æ‰«ææ‰€æœ‰MCZæ–‡ä»¶
        mcz_files = [f for f in os.listdir(traindata_dir) if f.endswith('.mcz')]
        print(f"ğŸ“‚ å‘ç° {len(mcz_files)} ä¸ªMCZæ–‡ä»¶")
        
        all_audio_features = []
        all_beatmap_labels = []
        processed_count = 0
        target_count = min(100, len(mcz_files))  # å…ˆå¤„ç†100ä¸ªæ–‡ä»¶è¿›è¡Œæµ‹è¯•
        
        for i, mcz_file in enumerate(mcz_files[:target_count]):
            try:
                mcz_path = os.path.join(traindata_dir, mcz_file)
                print(f"âš¡ å¤„ç† [{i+1}/{target_count}]: {mcz_file}")
                
                # è§£æMCZæ–‡ä»¶
                song_data = parser.parse_mcz_file(mcz_path)
                if not song_data:
                    continue
                
                # æå–4Kè°±é¢
                beatmaps_4k = extractor.extract_4k_beatmaps(song_data)
                if not beatmaps_4k:
                    continue
                
                # æå–éŸ³é¢‘æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                temp_audio_dir = "temp_audio_extraction"
                os.makedirs(temp_audio_dir, exist_ok=True)
                
                extracted_audio = parser.extract_audio_files(mcz_path, temp_audio_dir)
                if not extracted_audio:
                    continue
                
                # å¤„ç†æ¯ä¸ª4Kè°±é¢
                for beatmap in beatmaps_4k:
                    for audio_file in extracted_audio:
                        try:
                            # åˆ†æéŸ³é¢‘å’Œè°±é¢
                            aligned_data = analyzer.align_audio_and_beatmap(
                                audio_file, beatmap, {}
                            )
                            
                            if aligned_data and len(aligned_data.audio_features) > self.sequence_length:
                                # æ·»åŠ åˆ°è®­ç»ƒæ•°æ®
                                audio_features = aligned_data.audio_features
                                beatmap_events = aligned_data.beatmap_events
                                
                                all_audio_features.append(audio_features)
                                all_beatmap_labels.append(beatmap_events)
                                
                                processed_count += 1
                                break  # æ¯ä¸ªè°±é¢åªç”¨ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶
                                
                        except Exception as e:
                            print(f"   âš ï¸ å¤„ç†éŸ³é¢‘å¤±è´¥: {e}")
                            continue
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                import shutil
                if os.path.exists(temp_audio_dir):
                    shutil.rmtree(temp_audio_dir)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†MCZå¤±è´¥: {e}")
                continue
        
        print(f"âœ… æˆåŠŸå¤„ç† {processed_count} ä¸ªè°±é¢æ ·æœ¬")
        
        if processed_count == 0:
            raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        audio_features = np.vstack(all_audio_features)
        beatmap_labels = np.vstack(all_beatmap_labels)
        
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {audio_features.shape[0]:,} ä¸ªæ—¶é—´æ­¥")
        print(f"ğŸµ éŸ³é¢‘ç‰¹å¾ç»´åº¦: {audio_features.shape[1]}")
        print(f"ğŸ® è°±é¢æ ‡ç­¾ç»´åº¦: {beatmap_labels.shape[1]}")
        
        return audio_features, beatmap_labels
    
    def prepare_training_data(self, audio_features: np.ndarray, beatmap_labels: np.ndarray, 
                            train_ratio: float = 0.8) -> Tuple[DataLoader, DataLoader]:
        """
        å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        
        Args:
            audio_features: éŸ³é¢‘ç‰¹å¾
            beatmap_labels: è°±é¢æ ‡ç­¾
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            
        Returns:
            (train_loader, val_loader): è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        """
        print("ğŸ”§ å‡†å¤‡æ·±åº¦å­¦ä¹ è®­ç»ƒæ•°æ®...")
        
        # æ ‡å‡†åŒ–éŸ³é¢‘ç‰¹å¾
        audio_features_scaled = self.scaler.fit_transform(audio_features)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            audio_features_scaled, beatmap_labels, 
            test_size=1-train_ratio, random_state=42, shuffle=True
        )
        
        print(f"ğŸ“ˆ è®­ç»ƒé›†å¤§å°: {len(X_train):,}")
        print(f"ğŸ“‰ éªŒè¯é›†å¤§å°: {len(X_val):,}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = BeatmapDataset(X_train, y_train, self.sequence_length)
        val_dataset = BeatmapDataset(X_val, y_val, self.sequence_length)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, 
            shuffle=True, num_workers=0, pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, batch_size=self.batch_size, 
            shuffle=False, num_workers=0, pin_memory=True
        )
        
        print(f"ğŸ¯ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"ğŸ¯ éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        
        return train_loader, val_loader
    
    def compute_loss(self, note_pred: torch.Tensor, event_pred: torch.Tensor, 
                    beatmap_target: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è®¡ç®—æŸå¤±å‡½æ•°
        
        Args:
            note_pred: éŸ³ç¬¦é¢„æµ‹ [batch_size, 4]
            event_pred: äº‹ä»¶é¢„æµ‹ [batch_size, 3]
            beatmap_target: ç›®æ ‡æ ‡ç­¾ [batch_size, 7]
            
        Returns:
            (total_loss, loss_dict): æ€»æŸå¤±å’Œå„é¡¹æŸå¤±
        """
        # åˆ†ç¦»ç›®æ ‡æ ‡ç­¾
        note_target = beatmap_target[:, :4]      # 4ä¸ªè½¨é“
        event_target = beatmap_target[:, 4:]     # 3ä¸ªäº‹ä»¶ç±»å‹
        
        # éŸ³ç¬¦æ”¾ç½®æŸå¤±ï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰
        note_loss = F.binary_cross_entropy(note_pred, note_target, reduction='mean')
        
        # äº‹ä»¶ç±»å‹æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        event_target_indices = torch.argmax(event_target, dim=1)
        event_loss = F.cross_entropy(event_pred, event_target_indices, reduction='mean')
        
        # æ€»æŸå¤±ï¼ˆåŠ æƒç»„åˆï¼‰
        total_loss = 0.7 * note_loss + 0.3 * event_loss
        
        loss_dict = {
            'total': total_loss.item(),
            'note': note_loss.item(),
            'event': event_loss.item()
        }
        
        return total_loss, loss_dict
    
    def compute_accuracy(self, note_pred: torch.Tensor, event_pred: torch.Tensor, 
                        beatmap_target: torch.Tensor) -> Dict[str, float]:
        """è®¡ç®—å‡†ç¡®ç‡"""
        with torch.no_grad():
            # éŸ³ç¬¦æ”¾ç½®å‡†ç¡®ç‡ï¼ˆé˜ˆå€¼0.5ï¼‰
            note_target = beatmap_target[:, :4]
            note_pred_binary = (note_pred > 0.5).float()
            note_accuracy = (note_pred_binary == note_target).float().mean().item()
            
            # äº‹ä»¶ç±»å‹å‡†ç¡®ç‡
            event_target = beatmap_target[:, 4:]
            event_target_indices = torch.argmax(event_target, dim=1)
            event_pred_indices = torch.argmax(event_pred, dim=1)
            event_accuracy = (event_pred_indices == event_target_indices).float().mean().item()
            
            return {
                'note_accuracy': note_accuracy,
                'event_accuracy': event_accuracy
            }
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_note_accuracy = 0
        total_event_accuracy = 0
        num_batches = 0
        
        for batch_idx, (audio_seq, beatmap_target) in enumerate(train_loader):
            audio_seq = audio_seq.to(self.device)
            beatmap_target = beatmap_target.to(self.device)
            
            # å‰å‘ä¼ æ’­
            note_pred, event_pred = self.model(audio_seq)
            
            # è®¡ç®—æŸå¤±
            loss, loss_dict = self.compute_loss(note_pred, event_pred, beatmap_target)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy_dict = self.compute_accuracy(note_pred, event_pred, beatmap_target)
            
            # ç´¯è®¡ç»Ÿè®¡
            total_loss += loss_dict['total']
            total_note_accuracy += accuracy_dict['note_accuracy']
            total_event_accuracy += accuracy_dict['event_accuracy']
            num_batches += 1
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 50 == 0:
                print(f"   æ‰¹æ¬¡ [{batch_idx:4d}/{len(train_loader)}] "
                      f"æŸå¤±: {loss_dict['total']:.4f} "
                      f"éŸ³ç¬¦å‡†ç¡®ç‡: {accuracy_dict['note_accuracy']:.3f} "
                      f"äº‹ä»¶å‡†ç¡®ç‡: {accuracy_dict['event_accuracy']:.3f}")
        
        return {
            'loss': total_loss / num_batches,
            'note_accuracy': total_note_accuracy / num_batches,
            'event_accuracy': total_event_accuracy / num_batches
        }
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0
        total_note_accuracy = 0
        total_event_accuracy = 0
        num_batches = 0
        
        with torch.no_grad():
            for audio_seq, beatmap_target in val_loader:
                audio_seq = audio_seq.to(self.device)
                beatmap_target = beatmap_target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                note_pred, event_pred = self.model(audio_seq)
                
                # è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡
                loss, loss_dict = self.compute_loss(note_pred, event_pred, beatmap_target)
                accuracy_dict = self.compute_accuracy(note_pred, event_pred, beatmap_target)
                
                # ç´¯è®¡ç»Ÿè®¡
                total_loss += loss_dict['total']
                total_note_accuracy += accuracy_dict['note_accuracy']
                total_event_accuracy += accuracy_dict['event_accuracy']
                num_batches += 1
        
        return {
            'loss': total_loss / num_batches,
            'note_accuracy': total_note_accuracy / num_batches,
            'event_accuracy': total_event_accuracy / num_batches
        }
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, 
              num_epochs: int = 50, save_path: str = None):
        """
        è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        print(f"ğŸš€ å¼€å§‹æ·±åº¦å­¦ä¹ è®­ç»ƒ ({num_epochs} è½®)")
        
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = 10
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“Š Epoch [{epoch+1}/{num_epochs}]")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_metrics['loss'])
            
            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_metrics['loss'])
            self.training_history['val_loss'].append(val_metrics['loss'])
            self.training_history['note_accuracy'].append(val_metrics['note_accuracy'])
            self.training_history['event_accuracy'].append(val_metrics['event_accuracy'])
            
            # æ‰“å°ç»“æœ
            print(f"ğŸ”¥ è®­ç»ƒ - æŸå¤±: {train_metrics['loss']:.4f}, "
                  f"éŸ³ç¬¦å‡†ç¡®ç‡: {train_metrics['note_accuracy']:.3f}, "
                  f"äº‹ä»¶å‡†ç¡®ç‡: {train_metrics['event_accuracy']:.3f}")
            print(f"âœ… éªŒè¯ - æŸå¤±: {val_metrics['loss']:.4f}, "
                  f"éŸ³ç¬¦å‡†ç¡®ç‡: {val_metrics['note_accuracy']:.3f}, "
                  f"äº‹ä»¶å‡†ç¡®ç‡: {val_metrics['event_accuracy']:.3f}")
            
            # æ—©åœæ£€æŸ¥
            if val_metrics['loss'] < best_val_loss:
                best_val_loss = val_metrics['loss']
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if save_path:
                    torch.save({
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'scaler': self.scaler,
                        'epoch': epoch,
                        'val_loss': val_metrics['loss']
                    }, save_path)
                    print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {save_path}")
            else:
                patience_counter += 1
                
            if patience_counter >= max_patience:
                print(f"â° æ—©åœè§¦å‘ (patience={max_patience})")
                break
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.training_history['train_loss'], label='è®­ç»ƒæŸå¤±')
        axes[0, 0].plot(self.training_history['val_loss'], label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('æŸå¤±å‡½æ•°å˜åŒ–')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # éŸ³ç¬¦å‡†ç¡®ç‡
        axes[0, 1].plot(self.training_history['note_accuracy'], label='éŸ³ç¬¦å‡†ç¡®ç‡')
        axes[0, 1].set_title('éŸ³ç¬¦æ”¾ç½®å‡†ç¡®ç‡')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # äº‹ä»¶å‡†ç¡®ç‡
        axes[1, 0].plot(self.training_history['event_accuracy'], label='äº‹ä»¶å‡†ç¡®ç‡', color='orange')
        axes[1, 0].set_title('äº‹ä»¶ç±»å‹å‡†ç¡®ç‡')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # ç»¼åˆæŒ‡æ ‡
        axes[1, 1].plot(self.training_history['val_loss'], label='éªŒè¯æŸå¤±', alpha=0.7)
        ax2 = axes[1, 1].twinx()
        ax2.plot(self.training_history['note_accuracy'], label='éŸ³ç¬¦å‡†ç¡®ç‡', color='green', alpha=0.7)
        ax2.plot(self.training_history['event_accuracy'], label='äº‹ä»¶å‡†ç¡®ç‡', color='orange', alpha=0.7)
        
        axes[1, 1].set_title('ç»¼åˆè®­ç»ƒæŒ‡æ ‡')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss')
        ax2.set_ylabel('Accuracy')
        axes[1, 1].legend(loc='upper left')
        ax2.legend(loc='upper right')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig('deep_learning_training_history.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜ä¸º 'deep_learning_training_history.png'")


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºæ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹"""
    print("ğŸ® æ·±åº¦å­¦ä¹ éŸ³æ¸¸è°±é¢ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = DeepBeatmapLearningSystem(
        sequence_length=64,
        batch_size=32,
        learning_rate=0.001
    )
    
    # åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†
    try:
        audio_features, beatmap_labels = system.load_large_dataset('trainData')
        
        # åˆ›å»ºæ¨¡å‹
        system.create_model(input_dim=audio_features.shape[1])
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_loader, val_loader = system.prepare_training_data(
            audio_features, beatmap_labels, train_ratio=0.8
        )
        
        # å¼€å§‹è®­ç»ƒ
        system.train(
            train_loader, val_loader, 
            num_epochs=50,
            save_path='best_deep_beatmap_model.pth'
        )
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        system.plot_training_history()
        
        print("\nğŸ‰ æ·±åº¦å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
        print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º 'best_deep_beatmap_model.pth'")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
