#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥ä½œåŒºæ•´ç†å™¨
è¯†åˆ«é‡è¦æ–‡ä»¶ï¼Œæ¸…ç†é‡å¤æ–‡æ¡£ï¼Œæ•´ç†é¡¹ç›®ç»“æ„
"""

import os
import shutil
import json
from pathlib import Path
from typing import Dict, List, Set
import hashlib

class WorkspaceOrganizer:
    def __init__(self, workspace_path: str):
        self.workspace_path = Path(workspace_path)
        self.important_files = set()
        self.duplicate_files = {}
        self.backup_files = []
        self.documentation_files = []
        self.analysis_report = {}
        
    def analyze_workspace(self):
        """åˆ†æå·¥ä½œåŒºç»“æ„"""
        print("ğŸ” åˆ†æå·¥ä½œåŒºç»“æ„...")
        
        # é‡è¦çš„æ ¸å¿ƒæ–‡ä»¶æ¨¡å¼
        core_patterns = {
            # æ ¸å¿ƒè®­ç»ƒè„šæœ¬
            'enhanced_weighted_fusion_training_3_7.py': 'æœ€æ–°å¢å¼ºæƒé‡èåˆè®­ç»ƒè„šæœ¬',
            'large_scale_real_training.py': 'å¤§è§„æ¨¡çœŸå®æ•°æ®è®­ç»ƒ',
            'weighted_fusion_large_scale_training_2_8.py': 'æƒé‡èåˆå¤§è§„æ¨¡è®­ç»ƒ',
            
            # æ ¸å¿ƒæ¨¡å‹å’Œç»„ä»¶
            'models/weighted_fusion_model.py': 'æƒé‡èåˆæ¨¡å‹å®šä¹‰',
            'models/deep_learning_beatmap_system.py': 'æ·±åº¦å­¦ä¹ ç³»ç»Ÿ',
            'models/hybrid_beatmap_system.py': 'æ··åˆæ¨¡å‹ç³»ç»Ÿ',
            
            # æ ¸å¿ƒè§£æå™¨
            'core/mcz_parser.py': 'MCZæ–‡ä»¶è§£æå™¨',
            'core/four_k_extractor.py': '4Kè°±é¢æå–å™¨',
            'core/audio_beatmap_analyzer.py': 'éŸ³é¢‘è°±é¢åˆ†æå™¨',
            
            # ä¸»ç¨‹åºå’Œç”Ÿæˆå™¨
            'main.py': 'ä¸»ç¨‹åºå…¥å£',
            'deep_beatmap_generator.py': 'æ·±åº¦å­¦ä¹ è°±é¢ç”Ÿæˆå™¨',
            'final_demo.py': 'æœ€ç»ˆæ¼”ç¤ºç¨‹åº',
            
            # é‡è¦é…ç½®å’Œæ–‡æ¡£
            'README.md': 'é¡¹ç›®è¯´æ˜æ–‡æ¡£',
        }
        
        # å·²è®­ç»ƒçš„é‡è¦æ¨¡å‹
        important_models = {
            'enhanced_weighted_fusion_model_3_7.pth': 'å¢å¼ºæƒé‡èåˆæ¨¡å‹ (3:7)',
            'best_weighted_fusion_model.pth': 'æœ€ä½³æƒé‡èåˆæ¨¡å‹',
        }
        
        # å¤‡ä»½æ–‡ä»¶æ¨¡å¼
        backup_patterns = [
            '_backup_',
            '.bak',
            '_old',
            '_copy'
        ]
        
        # é‡å¤æ–‡æ¡£æ¨¡å¼
        duplicate_doc_patterns = [
            'å·¥ä½œåŸç†è¯¦è§£.md',
            'å¯è§†åŒ–æ¼”ç¤º.py',
            'åŸç†ç®€åŒ–æ¼”ç¤º.py'
        ]
        
        all_files = []
        for root, dirs, files in os.walk(self.workspace_path):
            for file in files:
                file_path = Path(root) / file
                relative_path = file_path.relative_to(self.workspace_path)
                all_files.append(relative_path)
        
        # åˆ†ç±»æ–‡ä»¶
        for file_path in all_files:
            file_str = str(file_path)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¸å¿ƒæ–‡ä»¶
            if file_str in core_patterns:
                self.important_files.add((file_path, core_patterns[file_str]))
            elif file_str in important_models:
                self.important_files.add((file_path, important_models[file_str]))
            
            # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
            if any(pattern in file_str for pattern in backup_patterns):
                self.backup_files.append(file_path)
            
            # æ£€æŸ¥é‡å¤æ–‡æ¡£
            if any(pattern in file_str for pattern in duplicate_doc_patterns):
                self.documentation_files.append(file_path)
        
        # æŸ¥æ‰¾é‡å¤æ–‡ä»¶ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
        self._find_duplicates(all_files)
        
        print(f"âœ… åˆ†æå®Œæˆ:")
        print(f"   ğŸ“ æ€»æ–‡ä»¶æ•°: {len(all_files)}")
        print(f"   â­ é‡è¦æ–‡ä»¶: {len(self.important_files)}")
        print(f"   ğŸ“„ å¤‡ä»½æ–‡ä»¶: {len(self.backup_files)}")
        print(f"   ğŸ“ é‡å¤æ–‡æ¡£: {len(self.documentation_files)}")
        print(f"   ğŸ”„ é‡å¤æ–‡ä»¶ç»„: {len(self.duplicate_files)}")
    
    def _find_duplicates(self, all_files: List[Path]):
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        file_hashes = {}
        
        for file_path in all_files:
            full_path = self.workspace_path / file_path
            if full_path.is_file() and full_path.suffix in ['.py', '.md']:
                try:
                    content_hash = self._get_file_hash(full_path)
                    if content_hash in file_hashes:
                        # å‘ç°é‡å¤æ–‡ä»¶
                        if content_hash not in self.duplicate_files:
                            self.duplicate_files[content_hash] = []
                        self.duplicate_files[content_hash].append(file_path)
                    else:
                        file_hashes[content_hash] = file_path
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹å“ˆå¸Œ"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            content = f.read()
            hasher.update(content)
        return hasher.hexdigest()
    
    def create_cleanup_plan(self):
        """åˆ›å»ºæ¸…ç†è®¡åˆ’"""
        print("\nğŸ“‹ åˆ›å»ºæ¸…ç†è®¡åˆ’...")
        
        cleanup_plan = {
            'keep_files': [],
            'backup_to_archive': [],
            'duplicate_docs_to_remove': [],
            'old_training_scripts_to_archive': [],
            'generated_data_to_clean': []
        }
        
        # ä¿ç•™é‡è¦æ–‡ä»¶
        for file_path, description in self.important_files:
            cleanup_plan['keep_files'].append({
                'path': str(file_path),
                'description': description
            })
        
        # å¤‡ä»½æ–‡ä»¶å½’æ¡£
        for file_path in self.backup_files:
            cleanup_plan['backup_to_archive'].append(str(file_path))
        
        # é‡å¤æ–‡æ¡£æ¸…ç†
        doc_groups = self._group_similar_docs()
        for group_name, files in doc_groups.items():
            if len(files) > 1:
                # ä¿ç•™æœ€æ–°çš„ï¼Œå…¶ä»–çš„æ ‡è®°åˆ é™¤
                sorted_files = sorted(files, key=lambda x: os.path.getmtime(self.workspace_path / x), reverse=True)
                for file_to_remove in sorted_files[1:]:
                    cleanup_plan['duplicate_docs_to_remove'].append(str(file_to_remove))
        
        # æ—§è®­ç»ƒè„šæœ¬å½’æ¡£
        old_training_patterns = [
            'quick_weighted_fusion_training_2_8.py',
            'working_train.py',
            'quick_hybrid_train.py',
            'large_scale_train_with_preprocessed_backup_*.py'
        ]
        
        all_files = list(self.workspace_path.glob('*.py'))
        for file_path in all_files:
            relative_path = file_path.relative_to(self.workspace_path)
            if any(pattern.replace('*', '') in str(relative_path) for pattern in old_training_patterns):
                if relative_path not in [f[0] for f in self.important_files]:
                    cleanup_plan['old_training_scripts_to_archive'].append(str(relative_path))
        
        # ç”Ÿæˆæ•°æ®æ¸…ç†
        generated_patterns = [
            'temp_mcz_analysis/',
            'extracted_audio/',
            '__pycache__/',
            '*.png',
            '*.csv',
            'test_*.json'
        ]
        
        for pattern in generated_patterns:
            matches = list(self.workspace_path.glob(pattern))
            for match in matches:
                relative_path = match.relative_to(self.workspace_path)
                cleanup_plan['generated_data_to_clean'].append(str(relative_path))
        
        self.cleanup_plan = cleanup_plan
        return cleanup_plan
    
    def _group_similar_docs(self) -> Dict[str, List[Path]]:
        """å°†ç›¸ä¼¼çš„æ–‡æ¡£åˆ†ç»„"""
        groups = {
            'neural_network_docs': [],
            'random_forest_docs': [],
            'fusion_docs': [],
            'principle_docs': []
        }
        
        for file_path in self.documentation_files:
            file_str = str(file_path)
            if 'ç¥ç»ç½‘ç»œ' in file_str:
                groups['neural_network_docs'].append(file_path)
            elif 'éšæœºæ£®æ—' in file_str:
                groups['random_forest_docs'].append(file_path)
            elif 'æƒé‡èåˆ' in file_str or 'èåˆ' in file_str:
                groups['fusion_docs'].append(file_path)
            elif 'åŸç†' in file_str or 'è¯¦è§£' in file_str:
                groups['principle_docs'].append(file_path)
        
        return groups
    
    def execute_cleanup(self, dry_run: bool = True):
        """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
        if not hasattr(self, 'cleanup_plan'):
            print("âŒ è¯·å…ˆè°ƒç”¨ create_cleanup_plan()")
            return
        
        print(f"\nğŸš€ {'æ¨¡æ‹Ÿ' if dry_run else 'æ‰§è¡Œ'}æ¸…ç†æ“ä½œ...")
        
        # åˆ›å»ºå½’æ¡£ç›®å½•
        archive_dir = self.workspace_path / 'archived'
        if not dry_run:
            archive_dir.mkdir(exist_ok=True)
            (archive_dir / 'backups').mkdir(exist_ok=True)
            (archive_dir / 'old_scripts').mkdir(exist_ok=True)
            (archive_dir / 'generated_data').mkdir(exist_ok=True)
        
        actions = []
        
        # å½’æ¡£å¤‡ä»½æ–‡ä»¶
        for backup_file in self.cleanup_plan['backup_to_archive']:
            src = self.workspace_path / backup_file
            dst = archive_dir / 'backups' / backup_file
            action = f"å½’æ¡£å¤‡ä»½: {backup_file} -> archived/backups/"
            actions.append(action)
            if not dry_run:
                dst.parent.mkdir(parents=True, exist_ok=True)
                shutil.move(str(src), str(dst))
        
        # åˆ é™¤é‡å¤æ–‡æ¡£
        for doc_file in self.cleanup_plan['duplicate_docs_to_remove']:
            action = f"åˆ é™¤é‡å¤æ–‡æ¡£: {doc_file}"
            actions.append(action)
            if not dry_run:
                (self.workspace_path / doc_file).unlink()
        
        # å½’æ¡£æ—§è®­ç»ƒè„šæœ¬
        for old_script in self.cleanup_plan['old_training_scripts_to_archive']:
            src = self.workspace_path / old_script
            dst = archive_dir / 'old_scripts' / old_script
            action = f"å½’æ¡£æ—§è„šæœ¬: {old_script} -> archived/old_scripts/"
            actions.append(action)
            if not dry_run:
                dst.parent.mkdir(parents=True, exist_ok=True)
                shutil.move(str(src), str(dst))
        
        # æ¸…ç†ç”Ÿæˆæ•°æ®
        for generated_item in self.cleanup_plan['generated_data_to_clean']:
            path = self.workspace_path / generated_item
            if path.exists():
                if path.is_dir():
                    action = f"å½’æ¡£ç›®å½•: {generated_item} -> archived/generated_data/"
                    if not dry_run:
                        dst = archive_dir / 'generated_data' / generated_item
                        dst.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(path), str(dst))
                else:
                    action = f"åˆ é™¤ç”Ÿæˆæ–‡ä»¶: {generated_item}"
                    if not dry_run:
                        path.unlink()
                actions.append(action)
        
        print(f"ğŸ“Š æ¸…ç†æ“ä½œæ€»ç»“:")
        for action in actions:
            print(f"   â€¢ {action}")
        
        if dry_run:
            print(f"\nğŸ’¡ è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œæ²¡æœ‰å®é™…ä¿®æ”¹æ–‡ä»¶")
            print(f"   è¦æ‰§è¡Œå®é™…æ¸…ç†ï¼Œè¯·è°ƒç”¨ execute_cleanup(dry_run=False)")
    
    def create_project_structure_doc(self):
        """åˆ›å»ºé¡¹ç›®ç»“æ„æ–‡æ¡£"""
        print("\nğŸ“– åˆ›å»ºé¡¹ç›®ç»“æ„æ–‡æ¡£...")
        
        doc_content = """# FumenGenerate é¡¹ç›®ç»“æ„

## ğŸ“ æ ¸å¿ƒæ–‡ä»¶

### ğŸš€ ä¸»è¦è®­ç»ƒè„šæœ¬
"""
        
        # æŒ‰ç±»åˆ«ç»„ç»‡é‡è¦æ–‡ä»¶
        categories = {
            'è®­ç»ƒè„šæœ¬': [],
            'æ ¸å¿ƒæ¨¡å‹': [],
            'è§£æå™¨': [],
            'ç”Ÿæˆå™¨': [],
            'é…ç½®æ–‡æ¡£': []
        }
        
        for file_path, description in self.important_files:
            file_str = str(file_path)
            if 'training' in file_str:
                categories['è®­ç»ƒè„šæœ¬'].append((file_path, description))
            elif file_str.startswith('models/'):
                categories['æ ¸å¿ƒæ¨¡å‹'].append((file_path, description))
            elif file_str.startswith('core/'):
                categories['è§£æå™¨'].append((file_path, description))
            elif 'generator' in file_str or 'demo' in file_str:
                categories['ç”Ÿæˆå™¨'].append((file_path, description))
            else:
                categories['é…ç½®æ–‡æ¡£'].append((file_path, description))
        
        for category, files in categories.items():
            if files:
                doc_content += f"\n### {category}\n"
                for file_path, description in files:
                    doc_content += f"- `{file_path}` - {description}\n"
        
        doc_content += """

## ğŸ—‚ï¸ ç›®å½•ç»“æ„

- `core/` - æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
  - `mcz_parser.py` - MCZæ–‡ä»¶è§£æ
  - `four_k_extractor.py` - 4Kè°±é¢æå–
  - `audio_beatmap_analyzer.py` - éŸ³é¢‘åˆ†æ
  
- `models/` - æœºå™¨å­¦ä¹ æ¨¡å‹
  - `weighted_fusion_model.py` - æƒé‡èåˆæ¨¡å‹
  - `deep_learning_beatmap_system.py` - æ·±åº¦å­¦ä¹ ç³»ç»Ÿ
  - `hybrid_beatmap_system.py` - æ··åˆæ¨¡å‹
  
- `trainData/` - è®­ç»ƒæ•°æ® (MCZæ–‡ä»¶)
- `preprocessed_data/` - é¢„å¤„ç†æ•°æ®
- `generated_beatmaps/` - ç”Ÿæˆçš„è°±é¢
- `archived/` - å½’æ¡£æ–‡ä»¶

## ğŸ¯ æ¨èä½¿ç”¨æµç¨‹

1. **è®­ç»ƒæ¨¡å‹**: ä½¿ç”¨ `enhanced_weighted_fusion_training_3_7.py`
2. **ç”Ÿæˆè°±é¢**: ä½¿ç”¨ `deep_beatmap_generator.py`
3. **æ¼”ç¤ºæ•ˆæœ**: ä½¿ç”¨ `final_demo.py`

## ğŸ“ ç‰ˆæœ¬è¯´æ˜

- **enhanced_weighted_fusion_training_3_7.py**: æœ€æ–°ç‰ˆæœ¬ï¼Œ3:7æƒé‡æ¯”ä¾‹ï¼Œå®Œæ•´ç®—æ³•æ¶æ„
- **æƒé‡èåˆ**: RFåˆ†æ”¯30% + NNåˆ†æ”¯70%ï¼Œè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æƒé‡
- **æ¨¡å‹æ¶æ„**: d_model=256, heads=8, layers=6ï¼Œä¸å®Œæ•´ç‰ˆæœ¬å¯¹é½

"""
        
        doc_path = self.workspace_path / 'PROJECT_STRUCTURE.md'
        with open(doc_path, 'w', encoding='utf-8') as f:
            f.write(doc_content)
        
        print(f"âœ… é¡¹ç›®ç»“æ„æ–‡æ¡£å·²åˆ›å»º: {doc_path}")
    
    def generate_report(self):
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        report = {
            'workspace_path': str(self.workspace_path),
            'analysis_date': '2025-08-09',
            'important_files': [
                {'path': str(path), 'description': desc} 
                for path, desc in self.important_files
            ],
            'cleanup_plan': self.cleanup_plan if hasattr(self, 'cleanup_plan') else {},
            'recommendations': [
                "ä¿ç•™enhanced_weighted_fusion_training_3_7.pyä½œä¸ºä¸»è®­ç»ƒè„šæœ¬",
                "å½’æ¡£æ‰€æœ‰å¤‡ä»½æ–‡ä»¶åˆ°archived/ç›®å½•",
                "åˆ é™¤é‡å¤çš„æ–‡æ¡£æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬",
                "æ¸…ç†ç”Ÿæˆçš„ä¸´æ—¶æ•°æ®å’Œç¼“å­˜",
                "å®šæœŸå¤‡ä»½é‡è¦çš„è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶"
            ]
        }
        
        report_path = self.workspace_path / 'workspace_cleanup_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æ¸…ç†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report

def main():
    """ä¸»å‡½æ•°"""
    workspace_path = r"D:\Projects\FumenGenerate"
    
    print("ğŸ”§ FumenGenerate å·¥ä½œåŒºæ•´ç†å™¨")
    print("=" * 50)
    
    organizer = WorkspaceOrganizer(workspace_path)
    
    # åˆ†æå·¥ä½œåŒº
    organizer.analyze_workspace()
    
    # åˆ›å»ºæ¸…ç†è®¡åˆ’
    organizer.create_cleanup_plan()
    
    # æ¨¡æ‹Ÿæ¸…ç†ï¼ˆå…ˆä¸å®é™…æ‰§è¡Œï¼‰
    organizer.execute_cleanup(dry_run=True)
    
    # åˆ›å»ºé¡¹ç›®ç»“æ„æ–‡æ¡£
    organizer.create_project_structure_doc()
    
    # ç”ŸæˆæŠ¥å‘Š
    organizer.generate_report()
    
    print("\nğŸ‰ å·¥ä½œåŒºåˆ†æå®Œæˆï¼")
    print("ğŸ“‹ æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶äº†è§£è¯¦æƒ…:")
    print("   â€¢ PROJECT_STRUCTURE.md - é¡¹ç›®ç»“æ„è¯´æ˜")
    print("   â€¢ workspace_cleanup_report.json - è¯¦ç»†æ¸…ç†æŠ¥å‘Š")
    print("\nğŸ’¡ è¦æ‰§è¡Œå®é™…æ¸…ç†ï¼Œè¯·è¿è¡Œ:")
    print("   organizer.execute_cleanup(dry_run=False)")

if __name__ == "__main__":
    main()
