#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
4Kè°±é¢å­¦ä¹ ç¯å¢ƒæ€»ç»“

å±•ç¤ºå·²æ„å»ºçš„å®Œæ•´4Kè°±é¢åˆ†æå’Œæ•°æ®é¢„å¤„ç†ç¯å¢ƒã€‚
"""

import os
import json
import pandas as pd
import numpy as np
import pickle
from data_processor import FourKDataProcessor

def show_environment_summary():
    """å±•ç¤ºç¯å¢ƒæ€»ç»“"""
    print("=" * 60)
    print("4Kè°±é¢æ™ºèƒ½ç”Ÿæˆç³»ç»Ÿ - æ•°æ®åˆ†æç¯å¢ƒ")
    print("=" * 60)
    
    print("\nğŸ“ é¡¹ç›®ç»“æ„:")
    files = [
        "mcz_parser.py - MCZæ–‡ä»¶è§£æå™¨ï¼ˆåŸºç¡€è§£æåŠŸèƒ½ï¼‰",
        "four_k_extractor.py - 4Kè°±é¢æå–å™¨ï¼ˆä¸“é—¨æå–4Kè°±é¢ï¼‰", 
        "data_processor.py - æ•°æ®é¢„å¤„ç†å™¨ï¼ˆç‰¹å¾å·¥ç¨‹å’Œåºåˆ—åŒ–ï¼‰",
        "batch_analyzer.py - æ‰¹é‡åˆ†æå™¨ï¼ˆç»Ÿè®¡åˆ†æï¼‰",
        "test_4k_extractor.py - æµ‹è¯•è„šæœ¬",
        "trainData/ - è®­ç»ƒæ•°æ®ç›®å½•ï¼ˆåŒ…å«MCZæ–‡ä»¶ï¼‰"
    ]
    
    for file_desc in files:
        print(f"  âœ“ {file_desc}")
    
    print("\nğŸ“Š æ•°æ®æ ¼å¼ç†è§£:")
    print("  âœ“ MCZæ–‡ä»¶ = ZIPå‹ç¼©åŒ…ï¼ŒåŒ…å«:")
    print("    - éŸ³é¢‘æ–‡ä»¶ (.ogg)")
    print("    - å›¾ç‰‡æ–‡ä»¶ (.jpg)")
    print("    - MCè°±é¢æ–‡ä»¶ (.mc - JSONæ ¼å¼)")
    print("    - TJAè°±é¢æ–‡ä»¶ (.tja - å¤ªé¼“è¾¾äººæ ¼å¼)")
    
    print("\nğŸ® 4Kè°±é¢ç‰¹å¾:")
    print("  âœ“ æ¸¸æˆæ¨¡å¼: 0ï¼ˆå››è½¨é“ä¸‹è½å¼ï¼‰")
    print("  âœ“ ä½¿ç”¨åˆ—: 0, 1, 2, 3ï¼ˆå››ä¸ªè½¨é“ï¼‰")
    print("  âœ“ éš¾åº¦æ ‡è¯†: ç‰ˆæœ¬ååŒ…å«'4K'")
    print("  âœ“ éŸ³ç¬¦ç±»å‹: æ™®é€šéŸ³ç¬¦(1.0) + é•¿æŒ‰éŸ³ç¬¦(2.0)")
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
    print("\nğŸ“„ å·²ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶:")
    generated_files = [
        ("test_4k_beatmaps.json", "4Kè°±é¢åŸå§‹æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰"),
        ("test_4k_training_data.csv", "4Kè®­ç»ƒæ•°æ®é›†ï¼ˆCSVæ ¼å¼ï¼‰"),
        ("processed_4k_sequences.pkl", "é¢„å¤„ç†åçš„åºåˆ—æ•°æ®"),
        ("processed_4k_features.csv", "æå–çš„ç‰¹å¾æ•°æ®")
    ]
    
    for filename, description in generated_files:
        if os.path.exists(filename):
            size = os.path.getsize(filename)
            print(f"  âœ“ {filename} - {description} ({size:,} bytes)")
        else:
            print(f"  âœ— {filename} - {description} (æœªç”Ÿæˆ)")

def analyze_processed_data():
    """åˆ†æå¤„ç†åçš„æ•°æ®"""
    print("\n" + "=" * 40)
    print("æ•°æ®åˆ†æç»“æœ")
    print("=" * 40)
    
    # åˆ†æCSVæ•°æ®
    if os.path.exists("test_4k_training_data.csv"):
        df = pd.read_csv("test_4k_training_data.csv")
        print(f"\nğŸ“ˆ è®­ç»ƒæ•°æ®é›†ç»Ÿè®¡:")
        print(f"  â€¢ æ€»è°±é¢æ•°: {len(df)}")
        print(f"  â€¢ ç‹¬ç‰¹æ­Œæ›²æ•°: {df['song_title'].nunique()}")
        print(f"  â€¢ ç‹¬ç‰¹è‰ºæœ¯å®¶æ•°: {df['artist'].nunique()}")
        
        print(f"\nğŸµ éŸ³ç¬¦ç»Ÿè®¡:")
        print(f"  â€¢ å¹³å‡éŸ³ç¬¦æ•°: {df['note_count'].mean():.1f}")
        print(f"  â€¢ éŸ³ç¬¦æ•°èŒƒå›´: {df['note_count'].min()} - {df['note_count'].max()}")
        print(f"  â€¢ å¹³å‡éŸ³ç¬¦å¯†åº¦: {df['note_density'].mean():.2f} éŸ³ç¬¦/èŠ‚æ‹")
        
        print(f"\nğŸ¼ BPMç»Ÿè®¡:")
        print(f"  â€¢ å¹³å‡BPM: {df['initial_bpm'].mean():.1f}")
        print(f"  â€¢ BPMèŒƒå›´: {df['initial_bpm'].min():.1f} - {df['initial_bpm'].max():.1f}")
        
        print(f"\nğŸ¯ éš¾åº¦åˆ†å¸ƒ:")
        diff_counts = df['difficulty_version'].value_counts()
        for diff, count in diff_counts.head(5).items():
            print(f"  â€¢ {diff}: {count}")
    
    # åˆ†æåºåˆ—æ•°æ®
    if os.path.exists("processed_4k_sequences.pkl"):
        print(f"\nğŸ”§ é¢„å¤„ç†æ•°æ®:")
        try:
            processor = FourKDataProcessor()
            sequences = processor.load_processed_data("processed_4k_sequences.pkl")
        except Exception as e:
            print(f"  âœ— æ— æ³•åŠ è½½åºåˆ—æ•°æ®: {e}")
            sequences = []
        
        print(f"  â€¢ åºåˆ—æ•°é‡: {len(sequences)}")
        if sequences:
            sample = sequences[0]
            print(f"  â€¢ åºåˆ—å½¢çŠ¶: {sample.note_grid.shape}")
            print(f"  â€¢ æ—¶é—´åˆ†è¾¨ç‡: æ¯èŠ‚æ‹16æ­¥")
            print(f"  â€¢ æœ€å¤§åºåˆ—é•¿åº¦: 2000æ­¥")
            
            # è®¡ç®—ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            total_notes = sum(np.sum(seq.note_grid > 0) for seq in sequences)
            total_steps = sum(seq.note_grid.shape[0] for seq in sequences)
            print(f"  â€¢ æ€»éŸ³ç¬¦æ•°: {total_notes:,}")
            print(f"  â€¢ æ€»æ—¶é—´æ­¥æ•°: {total_steps:,}")
            print(f"  â€¢ å¹³å‡å¯†åº¦: {total_notes/total_steps:.4f}")

def show_next_steps():
    """æ˜¾ç¤ºä¸‹ä¸€æ­¥å»ºè®®"""
    print("\n" + "=" * 40)
    print("ä¸‹ä¸€æ­¥å¼€å‘å»ºè®®")
    print("=" * 40)
    
    steps = [
        "ğŸš€ æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘:",
        "   â€¢ ä½¿ç”¨LSTM/GRUå¤„ç†åºåˆ—æ•°æ®",
        "   â€¢ ä½¿ç”¨Transformerå¤„ç†éŸ³ç¬¦åºåˆ—",
        "   â€¢ è€ƒè™‘VAEç”Ÿæˆå˜åˆ†è‡ªç¼–ç å™¨",
        "",
        "ğŸµ éŸ³é¢‘ç‰¹å¾æå–:",
        "   â€¢ æå–éŸ³é¢‘çš„æ¢…å°”é¢‘è°±å›¾",
        "   â€¢ èŠ‚æ‹æ£€æµ‹å’ŒèŠ‚å¥åˆ†æ",
        "   â€¢ éŸ³è°ƒå’Œå’Œå¼¦åˆ†æ",
        "",
        "ğŸ“Š æ•°æ®å¢å¼º:",
        "   â€¢ å¤„ç†æ›´å¤šMCZæ–‡ä»¶ï¼ˆå½“å‰åªæœ‰15ä¸ªæ ·æœ¬ï¼‰",
        "   â€¢ æ•°æ®å¹³è¡¡ï¼ˆä¸åŒéš¾åº¦çš„æ ·æœ¬æ•°é‡ï¼‰",
        "   â€¢ äº¤å‰éªŒè¯æ•°æ®åˆ†å‰²",
        "",
        "ğŸ”„ æ¨¡å‹è®­ç»ƒ:",
        "   â€¢ éŸ³ä¹ç‰¹å¾ â†’ è°±é¢ç”Ÿæˆ",
        "   â€¢ æ¡ä»¶ç”Ÿæˆï¼ˆæŒ‡å®šéš¾åº¦ã€é£æ ¼ï¼‰",
        "   â€¢ åºåˆ—åˆ°åºåˆ—å­¦ä¹ "
    ]
    
    for step in steps:
        print(step)

def create_sample_usage():
    """åˆ›å»ºä½¿ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 40) 
    print("ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 40)
    
    print("\nğŸ’¡ å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç¯å¢ƒ:")
    
    usage_examples = [
        "# 1. æå–4Kè°±é¢",
        "from four_k_extractor import FourKBeatmapExtractor",
        "extractor = FourKBeatmapExtractor()",
        "beatmaps = extractor.extract_from_directory('trainData')",
        "",
        "# 2. æ•°æ®é¢„å¤„ç†", 
        "from data_processor import FourKDataProcessor",
        "processor = FourKDataProcessor(time_resolution=16)",
        "sequences = processor.process_dataset('four_k_beatmaps.json')",
        "",
        "# 3. è·å–è®­ç»ƒæ•°æ®",
        "note_grids, timing_grids, features = processor.create_training_arrays(sequences)",
        "# note_grids.shape: (n_samples, max_length, 4)",
        "# timing_grids.shape: (n_samples, max_length, 1)", 
        "# features.shape: (n_samples, n_features)",
        "",
        "# 4. ç°åœ¨å¯ä»¥ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼"
    ]
    
    for line in usage_examples:
        print(line)

def main():
    """ä¸»å‡½æ•°"""
    show_environment_summary()
    analyze_processed_data()
    show_next_steps()
    create_sample_usage()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ 4Kè°±é¢å­¦ä¹ ç¯å¢ƒæ„å»ºå®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒéŸ³ä¹åˆ°è°±é¢çš„ç”Ÿæˆæ¨¡å‹äº†ã€‚")
    print("=" * 60)

if __name__ == "__main__":
    main()
