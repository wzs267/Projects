"""
éŸ³æ¸¸ä¸“ç”¨çš„64æ­¥åºåˆ—Transformerè®¾è®¡
================================

åŸºäºéŸ³æ¸¸çš„ç‰¹æ®Šéœ€æ±‚ï¼Œå……åˆ†åˆ©ç”¨64ä¸ªæ—¶é—´æ­¥çš„å†å²ä¿¡æ¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class RhythmGameTransformer(nn.Module):
    """éŸ³æ¸¸ä¸“ç”¨çš„åºåˆ—Transformer"""
    
    def __init__(self, input_dim=15, d_model=256, num_heads=8, num_layers=6, 
                 dropout=0.1, max_seq_len=64):
        super().__init__()
        
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # è¾“å…¥æŠ•å½±ï¼šå°†15ç»´ç‰¹å¾æ˜ å°„åˆ°256ç»´
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç ï¼šè®©æ¨¡å‹ç†è§£æ—¶é—´é¡ºåº
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # éŸ³æ¸¸ä¸“ç”¨çš„å¤šå±‚æ¬¡æ³¨æ„åŠ›
        self.transformer_layers = nn.ModuleList([
            RhythmTransformerLayer(d_model, num_heads, dropout)
            for _ in range(num_layers)
        ])
        
        # æ—¶åºä¿¡æ¯å‹ç¼©ï¼šä»64æ­¥â†’1æ­¥å†³ç­–
        self.temporal_compression = TemporalCompressionLayer(d_model)
        
        # è¾“å‡ºå±‚
        self.output_notes = nn.Linear(d_model, 4)   # 4è½¨é“éŸ³ç¬¦
        self.output_events = nn.Linear(d_model, 3)  # 3ç§äº‹ä»¶ç±»å‹
        
        self._init_weights()
    
    def _init_weights(self):
        """Xavieråˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
    
    def forward(self, x):
        """
        Args:
            x: [batch, 64, 15] - 64ä¸ªæ—¶é—´æ­¥çš„éŸ³é¢‘ç‰¹å¾
        Returns:
            note_output: [batch, 4] - 4è½¨é“éŸ³ç¬¦æ¦‚ç‡
            event_output: [batch, 3] - 3ç§äº‹ä»¶æ¦‚ç‡
        """
        batch_size, seq_len, _ = x.shape
        
        # 1. è¾“å…¥æŠ•å½±ï¼šæ¯ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾ 15â†’256
        x = self.input_projection(x)  # [batch, 64, 256]
        
        # 2. ä½ç½®ç¼–ç ï¼šæ·»åŠ æ—¶é—´ä½ç½®ä¿¡æ¯
        x = self.pos_encoding(x)      # [batch, 64, 256]
        
        # 3. å¤šå±‚Transformerå¤„ç†
        for layer in self.transformer_layers:
            x = layer(x)              # [batch, 64, 256]
        
        # 4. æ—¶åºå‹ç¼©ï¼š64æ­¥â†’1ä¸ªå†³ç­–
        decision_features = self.temporal_compression(x)  # [batch, 256]
        
        # 5. è¾“å‡ºé¢„æµ‹
        note_output = torch.sigmoid(self.output_notes(decision_features))
        event_output = torch.sigmoid(self.output_events(decision_features))
        
        return note_output, event_output

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç ï¼šè®©æ¨¡å‹çŸ¥é“æ¯ä¸ªæ—¶é—´æ­¥çš„ä½ç½®"""
    
    def __init__(self, d_model, max_len=64):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, d_model]
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]
        """
        seq_len = x.size(1)
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pe[:seq_len, :].transpose(0, 1)
        return x

class RhythmTransformerLayer(nn.Module):
    """éŸ³æ¸¸ä¸“ç”¨çš„Transformerå±‚"""
    
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.self_attention = nn.MultiheadAttention(
            d_model, num_heads, dropout=dropout, batch_first=True
        )
        
        # å‰é¦ˆç½‘ç»œ
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )
        
        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]
        """
        # 1. è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_out, attn_weights = self.self_attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_out))
        
        # 2. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        ff_out = self.feed_forward(x)
        x = self.norm2(x + ff_out)
        
        return x

class TemporalCompressionLayer(nn.Module):
    """æ—¶åºå‹ç¼©å±‚ï¼šå°†64æ­¥å†å²å‹ç¼©ä¸ºå•ä¸ªå†³ç­–ç‰¹å¾"""
    
    def __init__(self, d_model):
        super().__init__()
        
        # å¤šç§å‹ç¼©ç­–ç•¥
        self.current_projection = nn.Linear(d_model, d_model // 2)
        self.history_compression = nn.Sequential(
            nn.Conv1d(d_model, d_model // 2, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
    
    def forward(self, x):
        """
        Args:
            x: [batch, 64, d_model]
        Returns:
            compressed: [batch, d_model]
        """
        batch_size, seq_len, d_model = x.shape
        
        # 1. å½“å‰æ—¶åˆ»ç‰¹å¾ï¼ˆæœ€åä¸€æ­¥ï¼‰
        current_features = self.current_projection(x[:, -1, :])  # [batch, d_model//2]
        
        # 2. å†å²ä¿¡æ¯å‹ç¼©
        x_transposed = x.transpose(1, 2)  # [batch, d_model, 64]
        history_features = self.history_compression(x_transposed)  # [batch, d_model//2, 1]
        history_features = history_features.squeeze(-1)  # [batch, d_model//2]
        
        # 3. èåˆå½“å‰+å†å²
        combined = torch.cat([current_features, history_features], dim=1)  # [batch, d_model]
        compressed = self.fusion(combined)
        
        return compressed

class ImprovedWeightedFusionTransformer(nn.Module):
    """æ”¹è¿›çš„æƒé‡èåˆTransformer - çœŸæ­£åˆ©ç”¨64æ­¥åºåˆ—"""
    
    def __init__(self, input_dim=15, d_model=256, num_heads=8, num_layers=6, 
                 dropout=0.1, rf_weight=0.3, nn_weight=0.7, learnable_weights=True):
        super().__init__()
        
        # æƒé‡å‚æ•°
        if learnable_weights:
            self.rf_weight = nn.Parameter(torch.tensor(rf_weight, dtype=torch.float32))
            self.nn_weight = nn.Parameter(torch.tensor(nn_weight, dtype=torch.float32))
        else:
            self.register_buffer('rf_weight', torch.tensor(rf_weight, dtype=torch.float32))
            self.register_buffer('nn_weight', torch.tensor(nn_weight, dtype=torch.float32))
        
        # RFåˆ†æ”¯ï¼šå¢å¼ºçš„éšæœºæ£®æ—æ¨¡æ‹Ÿ
        self.rf_branch = EnhancedRFBranch(input_dim, d_model, dropout)
        
        # NNåˆ†æ”¯ï¼šå®Œæ•´çš„åºåˆ—Transformer
        self.nn_branch = RhythmGameTransformer(input_dim, d_model, num_heads, num_layers, dropout)
        
    def forward(self, x):
        """
        Args:
            x: [batch, 64, 15] - çœŸå®çš„64æ­¥åºåˆ—
        """
        # æƒé‡å½’ä¸€åŒ–
        total_weight = self.rf_weight + self.nn_weight
        normalized_rf = self.rf_weight / (total_weight + 1e-8)
        normalized_nn = self.nn_weight / (total_weight + 1e-8)
        
        # RFåˆ†æ”¯ï¼šç®€å•åºåˆ—å¤„ç†
        rf_notes, rf_events = self.rf_branch(x)
        
        # NNåˆ†æ”¯ï¼šå¤æ‚åºåˆ—å¤„ç†
        nn_notes, nn_events = self.nn_branch(x)
        
        # æƒé‡èåˆ
        fused_notes = normalized_rf * rf_notes + normalized_nn * nn_notes
        fused_events = normalized_rf * rf_events + normalized_nn * nn_events
        
        return fused_notes, fused_events
    
    def get_weights(self):
        """è·å–å½“å‰æƒé‡"""
        total = self.rf_weight + self.nn_weight
        return {
            'rf_weight': (self.rf_weight / total).item(),
            'nn_weight': (self.nn_weight / total).item()
        }

class EnhancedRFBranch(nn.Module):
    """å¢å¼ºçš„RFåˆ†æ”¯ - æ¨¡æ‹ŸçœŸå®éšæœºæ£®æ—çš„å¤æ‚å†³ç­–é€»è¾‘"""
    
    def __init__(self, input_dim, d_model, dropout=0.1, num_trees=32, tree_depth=5):
        super().__init__()
        
        self.input_dim = input_dim
        self.d_model = d_model
        self.num_trees = num_trees
        
        # ç‰¹å¾å­é›†é€‰æ‹©å™¨ï¼ˆæ¨¡æ‹Ÿéšæœºæ£®æ—çš„ç‰¹å¾éšæœºé‡‡æ ·ï¼‰
        self.feature_selectors = nn.ModuleList([
            FeatureSelector(input_dim, tree_idx) for tree_idx in range(num_trees)
        ])
        
        # å¤šæ£µå†³ç­–æ ‘ï¼ˆä¸åŒæ·±åº¦å’Œç»“æ„ï¼‰
        self.decision_trees = nn.ModuleList([
            DecisionTreeNetwork(input_dim, tree_depth, dropout, tree_idx, 
                              feature_ratio=0.7) 
            for tree_idx in range(num_trees)
        ])
        
        # æ ‘çš„æƒé‡ï¼ˆæ¨¡æ‹Ÿä¸åŒæ ‘çš„é‡è¦æ€§ï¼‰
        self.tree_weights = nn.Parameter(torch.ones(num_trees) / num_trees)
        
        # æ—¶åºç‰¹å¾æå–ï¼ˆRFä¹Ÿéœ€è¦ç†è§£æ—¶åºï¼‰
        self.temporal_features = TemporalFeatureExtractor(input_dim, 64)
        
        # é›†æˆå­¦ä¹ èåˆå±‚
        self.ensemble_fusion = nn.Sequential(
            nn.Linear(num_trees * 16 + 32, d_model),  # æ ‘è¾“å‡º + æ—¶åºç‰¹å¾
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model // 2),
            nn.ReLU()
        )
        
        # è¾“å‡ºå±‚
        self.output_notes = nn.Sequential(
            nn.Linear(d_model // 2, 32),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),  # RFä½¿ç”¨è¾ƒå°‘dropout
            nn.Linear(32, 4),
            nn.Sigmoid()
        )
        
        self.output_events = nn.Sequential(
            nn.Linear(d_model // 2, 32),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(32, 3),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        """
        Args:
            x: [batch, 64, input_dim] - 64æ­¥åºåˆ—
        Returns:
            notes: [batch, 4] - éŸ³ç¬¦è¾“å‡º
            events: [batch, 3] - äº‹ä»¶è¾“å‡º
        """
        batch_size = x.size(0)
        
        # 1. æå–æ—¶åºç‰¹å¾
        temporal_features = self.temporal_features(x)  # [batch, 32]
        
        # 2. å¤šæ£µå†³ç­–æ ‘å¹¶è¡Œå¤„ç†
        tree_outputs = []
        for tree_idx, (selector, tree) in enumerate(zip(self.feature_selectors, self.decision_trees)):
            # ç‰¹å¾é€‰æ‹©
            selected_features = selector(x)  # [batch, 64, selected_dim]
            
            # å†³ç­–æ ‘å¤„ç†
            tree_output = tree(selected_features)  # [batch, 16]
            tree_outputs.append(tree_output)
        
        # 3. é›†æˆæ‰€æœ‰æ ‘çš„è¾“å‡º
        tree_features = torch.cat(tree_outputs, dim=1)  # [batch, num_trees * 16]
        
        # 4. åº”ç”¨æ ‘æƒé‡ï¼ˆæ¨¡æ‹ŸæŠ•ç¥¨æœºåˆ¶ï¼‰
        tree_weights = F.softmax(self.tree_weights, dim=0)  # å½’ä¸€åŒ–æƒé‡
        weighted_trees = []
        for i in range(self.num_trees):
            start_idx = i * 16
            end_idx = (i + 1) * 16
            weighted_tree = tree_weights[i] * tree_features[:, start_idx:end_idx]
            weighted_trees.append(weighted_tree)
        
        weighted_tree_features = torch.cat(weighted_trees, dim=1)  # [batch, num_trees * 16]
        
        # 5. èåˆæ ‘ç‰¹å¾å’Œæ—¶åºç‰¹å¾
        combined_features = torch.cat([weighted_tree_features, temporal_features], dim=1)
        fused_features = self.ensemble_fusion(combined_features)  # [batch, d_model // 2]
        
        # 6. è¾“å‡ºé¢„æµ‹
        notes = self.output_notes(fused_features)
        events = self.output_events(fused_features)
        
        return notes, events

class FeatureSelector(nn.Module):
    """ç‰¹å¾é€‰æ‹©å™¨ - æ¨¡æ‹Ÿéšæœºæ£®æ—çš„ç‰¹å¾éšæœºé‡‡æ ·"""
    
    def __init__(self, input_dim, tree_idx, feature_ratio=0.7):
        super().__init__()
        
        # æ ¹æ®æ ‘ç´¢å¼•ç”Ÿæˆç¡®å®šæ€§çš„ç‰¹å¾é€‰æ‹©
        torch.manual_seed(tree_idx * 42)  # ç¡®ä¿æ¯æ£µæ ‘æœ‰ä¸åŒä½†å›ºå®šçš„ç‰¹å¾é€‰æ‹©
        
        num_features = max(int(input_dim * feature_ratio), 1)
        self.selected_features = torch.randperm(input_dim)[:num_features].sort()[0]
        self.register_buffer('feature_indices', self.selected_features)
        
        # æ¢å¤éšæœºç§å­
        torch.manual_seed(torch.initial_seed())
    
    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, input_dim] or [batch, input_dim]
        Returns:
            selected_x: é€‰ä¸­çš„ç‰¹å¾å­é›†
        """
        if x.dim() == 3:
            return x[:, :, self.feature_indices]
        else:
            return x[:, self.feature_indices]

class DecisionTreeNetwork(nn.Module):
    """ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿçš„å†³ç­–æ ‘"""
    
    def __init__(self, input_dim, depth=5, dropout=0.1, tree_idx=0, feature_ratio=0.7):
        super().__init__()
        
        self.depth = depth
        self.tree_idx = tree_idx
        
        # è®¡ç®—å®é™…ä½¿ç”¨çš„ç‰¹å¾æ•°é‡
        self.num_features = max(int(input_dim * feature_ratio), 1)
        
        # æ ¹æ®æ ‘ç´¢å¼•åˆ›å»ºä¸åŒçš„æ¶æ„
        width_factor = 1.0 + 0.1 * (tree_idx % 5)  # ä¸åŒæ ‘æœ‰ä¸åŒå®½åº¦
        hidden_dim = int(64 * width_factor)
        
        # å†³ç­–è·¯å¾„ç½‘ç»œï¼ˆæ¨¡æ‹Ÿå†³ç­–æ ‘çš„åˆ†æ”¯é€»è¾‘ï¼‰
        layers = []
        current_dim = self.num_features  # ä½¿ç”¨å®é™…ç‰¹å¾æ•°é‡
        
        for i in range(depth):
            next_dim = hidden_dim // (2 ** i) if i < depth - 1 else 16
            next_dim = max(next_dim, 8)  # æœ€å°ç»´åº¦
            
            layers.extend([
                nn.Linear(current_dim, next_dim),
                nn.ReLU() if i % 2 == 0 else nn.LeakyReLU(0.1),  # äº¤æ›¿æ¿€æ´»å‡½æ•°
                nn.Dropout(dropout * 0.5) if i < depth - 1 else nn.Identity()
            ])
            current_dim = next_dim
        
        self.decision_path = nn.Sequential(*layers)
        
        # å¶å­èŠ‚ç‚¹ç‰¹å¾
        self.leaf_features = nn.Linear(16, 16)
    
    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, selected_features] or [batch, selected_features]
        Returns:
            tree_output: [batch, 16] - å•æ£µæ ‘çš„è¾“å‡ºç‰¹å¾
        """
        # å¦‚æœæ˜¯åºåˆ—è¾“å…¥ï¼Œå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼ˆRFå…³æ³¨å½“å‰æ—¶åˆ»ï¼‰
        if x.dim() == 3:
            x = x[:, -1, :]  # [batch, selected_features]
        
        # å†³ç­–è·¯å¾„å¤„ç†
        features = self.decision_path(x)  # [batch, 16]
        
        # å¶å­èŠ‚ç‚¹å¤„ç†
        output = self.leaf_features(features)  # [batch, 16]
        
        return output

class TemporalFeatureExtractor(nn.Module):
    """æ—¶åºç‰¹å¾æå–å™¨ - RFåˆ†æ”¯çš„è½»é‡çº§æ—¶åºç†è§£"""
    
    def __init__(self, input_dim, seq_len):
        super().__init__()
        
        # å¤šå°ºåº¦å·ç§¯ï¼ˆä¸åŒæ—¶é—´çª—å£ï¼‰
        self.conv_short = nn.Conv1d(input_dim, 8, kernel_size=3, padding=1)    # çŸ­æœŸ
        self.conv_medium = nn.Conv1d(input_dim, 8, kernel_size=7, padding=3)   # ä¸­æœŸ
        self.conv_long = nn.Conv1d(input_dim, 8, kernel_size=15, padding=7)    # é•¿æœŸ
        
        # æ± åŒ–å±‚
        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(24, 32),  # 3ä¸ªå°ºåº¦ Ã— 8ç»´ = 24ç»´
            nn.ReLU(),
            nn.Linear(32, 32)
        )
    
    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, input_dim]
        Returns:
            temporal_features: [batch, 32]
        """
        # è½¬æ¢ä¸ºå·ç§¯æ ¼å¼
        x = x.transpose(1, 2)  # [batch, input_dim, seq_len]
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        short_features = self.adaptive_pool(F.relu(self.conv_short(x))).squeeze(-1)   # [batch, 8]
        medium_features = self.adaptive_pool(F.relu(self.conv_medium(x))).squeeze(-1) # [batch, 8]
        long_features = self.adaptive_pool(F.relu(self.conv_long(x))).squeeze(-1)     # [batch, 8]
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([short_features, medium_features, long_features], dim=1)  # [batch, 24]
        temporal_features = self.fusion(combined)  # [batch, 32]
        
        return temporal_features

def main():
    """æµ‹è¯•æ–°æ¶æ„"""
    print("ğŸµ æµ‹è¯•æ”¹è¿›çš„64æ­¥åºåˆ—Transformer")
    
    # åˆ›å»ºæ¨¡å‹
    model = ImprovedWeightedFusionTransformer(
        input_dim=15,
        d_model=256, 
        num_heads=8,
        num_layers=6
    )
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 32
    seq_len = 64
    input_dim = 15
    
    x = torch.randn(batch_size, seq_len, input_dim)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    notes, events = model(x)
    print(f"éŸ³ç¬¦è¾“å‡º: {notes.shape}")
    print(f"äº‹ä»¶è¾“å‡º: {events.shape}")
    
    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°æ•°: {total_params:,}")
    
    # æƒé‡ä¿¡æ¯
    weights = model.get_weights()
    print(f"æƒé‡æ¯”ä¾‹: RF={weights['rf_weight']:.3f}, NN={weights['nn_weight']:.3f}")

if __name__ == "__main__":
    main()
